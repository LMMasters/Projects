{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ef1793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, Dropout\n",
    "from tensorflow.keras.losses import mse, mae\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.models import load_model\n",
    "from keras.backend import clear_session\n",
    "\n",
    "import optuna\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5332ccaf",
   "metadata": {},
   "source": [
    "## Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0091a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting cartesian to spherical\n",
    "def cart2sph(x, y, z):\n",
    "    hxy = np.hypot(x, y)\n",
    "    r = np.hypot(hxy, z)\n",
    "    theta = np.arctan2(hxy, z)\n",
    "    phi = np.arctan2(y, x)\n",
    "    return r, theta, phi\n",
    "\n",
    "#Converting spherical to cartesian\n",
    "def sph2cart(r, theta, phi):\n",
    "    rsin_theta = r * np.sin(theta)\n",
    "    x = rsin_theta * np.cos(phi)\n",
    "    y = rsin_theta * np.sin(phi)\n",
    "    z = r * np.cos(theta)\n",
    "    return x, y, z        \n",
    "\n",
    "#Calculating theta from x,y,z\n",
    "def calcTheta(x,y,z):\n",
    "    return np.arctan(np.sqrt(x**2+y**2)/z)\n",
    "\n",
    "#Calculating phi from x,y,z\n",
    "def calcPhi(x,y,z):\n",
    "    if x > 0:\n",
    "        return np.arctan(y/x)\n",
    "    elif x < 0 and y >= 0:\n",
    "        return np.arctan(y/x) + np.pi\n",
    "    elif x < 0 and y < 0:\n",
    "        return np.arctan(y/x) - np.pi\n",
    "    else:\n",
    "        print(\"Error in calculating phi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90bfdb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All possible particles\n",
    "#particles = ['u','d','c','s','t','b','e','mu','tau','w']\n",
    "\n",
    "#The set that we are going to look at\n",
    "particles = ['u', 't']\n",
    "\n",
    "#Getting a mass from the mass range of a given particle\n",
    "def mass(x):\n",
    "    return {\n",
    "        'u': rng.uniform(low=1.8*1e-3, high=2.8*1e-3),\n",
    "        'd': rng.uniform(low=4.2*1e-3, high=5.1*1e-3),\n",
    "        'c': rng.uniform(low=1.25, high=1.38),\n",
    "        's': rng.uniform(low=92*1e-3, high=104*1e-3),\n",
    "        't': rng.uniform(low=172.5, high=173.7),\n",
    "        'b': rng.uniform(low=4.15, high=4.22),\n",
    "        'e': 0.511*1e-3,\n",
    "        'mu': 105.7*1e-3,\n",
    "        'tau': 1.776,\n",
    "        'w': 80.385*1e-3,\n",
    "    }[x]\n",
    "\n",
    "#Creating empty lists\n",
    "E1_list = []\n",
    "E2_list = []\n",
    "px1_list = []\n",
    "px2_list = []\n",
    "py1_list = []\n",
    "py2_list = []\n",
    "pz1_list = []\n",
    "pz2_list = []\n",
    "M_list = []\n",
    "theta1_list = []\n",
    "theta2_list = []\n",
    "phi1_list = []\n",
    "phi2_list = []\n",
    "\n",
    "#Looping 100.000 times\n",
    "for i in range(100000):\n",
    "    #Choosing mass\n",
    "    if i < 50000: \n",
    "        particle = 'u'\n",
    "    else:\n",
    "        particle = 't'\n",
    "        \n",
    "    M = mass(particle)\n",
    "    E = 200\n",
    "    E1 = E*rng.normal(1,0.1) #Smearing\n",
    "    E2 = E*rng.normal(1,0.1) #Smearing\n",
    "    P = np.sqrt(E**2 - M**2) #Calculating |P|\n",
    "    \n",
    "    #Generating angles\n",
    "    theta1 = np.pi*rng.random() #Uniformly distributed\n",
    "    phi1 = 2*np.pi*rng.random()   #Uniformly distributed\n",
    "    theta2 = np.pi - theta1\n",
    "    phi2 = phi1 + np.pi \n",
    "    \n",
    "    #Smearing\n",
    "    #theta1 = theta1*rng.normal(1,0.05) \n",
    "    #theta2 = theta1*rng.normal(1,0.05) \n",
    "    #phi1 = phi1*rng.normal(1,0.05) \n",
    "    #phi2 = phi2*rng.normal(1,0.05) \n",
    "   \n",
    "    #Calculating cartesian coordinate values of p\n",
    "    px1, py1, pz1 = sph2cart(P, theta1, phi1)\n",
    "    px2, py2, pz2 = sph2cart(P, theta2, phi2)\n",
    "    \n",
    "    #Appending to lists\n",
    "    E1_list.append(E1)\n",
    "    E2_list.append(E2)\n",
    "    px1_list.append(px1)\n",
    "    px2_list.append(px2)\n",
    "    py1_list.append(py1)\n",
    "    py2_list.append(py2)\n",
    "    pz1_list.append(pz1)\n",
    "    pz2_list.append(pz2)\n",
    "    M_list.append(M)\n",
    "    theta1_list.append(theta1)\n",
    "    theta2_list.append(theta2)\n",
    "    phi1_list.append(phi1)\n",
    "    phi2_list.append(phi2)\n",
    "\n",
    "#Converting to numpy arrays\n",
    "E1_list = np.array(E1_list)\n",
    "E2_list = np.array(E2_list)\n",
    "px1_list = np.array(px1_list)\n",
    "px2_list = np.array(px2_list)\n",
    "py1_list = np.array(py1_list)\n",
    "py2_list = np.array(py2_list)\n",
    "pz1_list = np.array(pz1_list)\n",
    "pz2_list = np.array(pz2_list)\n",
    "M_list = np.array(M_list)    \n",
    "    \n",
    "#Shuffling\n",
    "perm = np.random.permutation(100000)\n",
    "E1_list = E1_list[perm]\n",
    "E2_list = E2_list[perm]\n",
    "px1_list = px1_list[perm]\n",
    "px2_list = px2_list[perm]\n",
    "py1_list = py1_list[perm]\n",
    "py2_list = py2_list[perm]\n",
    "pz1_list = pz1_list[perm]\n",
    "pz2_list = pz2_list[perm]\n",
    "M_list = M_list[perm]\n",
    "    \n",
    "#Creating .csv file with dataset\n",
    "#header = ['ID', 'E1', 'E2', 'p_x1', 'p_x2', 'p_y1', 'p_y2', 'p_z1', 'p_z2', 'M']\n",
    "\n",
    "#with open('MC_data.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "#    writer = csv.writer(f)\n",
    "#    writer.writerow(header)\n",
    "    \n",
    "#    for i in range(100000):\n",
    "#        writer.writerow([i, E1_list[i], E2_list[i], px1_list[i], px2_list[i], py1_list[i], py2_list[i], pz1_list[i], pz2_list[i], M_list[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f53be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEaCAYAAAAR0SDgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYOUlEQVR4nO3df7RdZZ3f8fdHosioRH4EGhMwUFIVcNQhg3RcWjUq8UeFtjDGUYk2nVQGXXbV1ZaMLkenzYzY6aBoYQYHm4AjkFJHUpVRGqWOlgHDyADhx5CCkJgUoiAGFTTx2z/Oc52Tw7n3nnu5v5L7fq111tn7u/ez9/Pk5p7P2Xufe3aqCkmSnjLdHZAkzQwGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0EaVZJXJtnWNb85ySsnaNtvS/LVrvlKctxEbLtt79Ekx07U9rR/MxA0YyT5bpKfJTm8p35ze6FcNE1d20tVnVBV1420TpJFrc9zRtnWn1fV6yaiX0muS/Kverb/zKq6ZyK2r/2fgaCZ5l7grUMzSV4IHDR93Zk8o4WFNNUMBM00lwFndc2vAC7tXiHJG5N8J8mPkmxN8uGuZU9P8tkkP0jywyTfTnJkW/bOJPck2ZXk3iRv69eBJAclWZvk4SS3A7/es/y7SV7Tpk9Osqn15YEkf9xW+0Z7/mE7bfOP2/6/leT8JA8BH261b/Z04Q2tn99P8p+TPKXt68NJPtvVj18ehSRZA7wc+FTb36faOr88BZVkbpJLk+xMcl+SD3Zt+51Jvpnkj9q4703y+hF/UtrvGAiaaf4aODjJC5IcALwF+GzPOj+mExrPBt4InJ3k9LZsBTAXOAo4DHg38NMkzwAuAF5fVc8CfgO4eZg+/B7wD9vj1LbN4XwC+ERVHdzWX9/qr2jPz26nba5v8y8F7gGOANYMs81/BiwBfg04DfiXI+wfgKr6APBXwHva/t7TZ7VP0vm3ORb4J3T+Dd/VtfylwF3A4cDHgEuSZLR9a/9hIGgmGjpKeC1wJ/C97oVVdV1V3VpVv6iqW4DL6bzAAfycThAcV1V7quqmqvpRW/YL4MQkB1XVjqraPMz+fxNYU1UPVdVWOkEynJ8DxyU5vKoeraq/HmVs26vqk1W1u6p+Osw657V93w98nK5TaOPVFa6rq2pXVX0X+C/AO7pWu6+qPl1Ve4B1wHzgyCe7b+07DATNRJcBvwW8k57TRQBJXprk6+3UxyN0jgIO72r7FeCKJNuTfCzJU6vqx3ReEN8N7EjypSTPH2b/zwG2ds3fN0JfVwL/CLiznZ560yhj2zrK8t517mv9ebIOB57G3mO5D1jQNf//hiaq6idt8pkTsG/tIwwEzThVdR+di8tvAD7fZ5XPARuAo6pqLvAnQFrbn1fVR6rqeDqnhd5EuyZRVV+pqtfSeed7J/DpYbqwg84ppyFHj9DXu6vqrXROAZ0HXNVOTw33NcKDfL1w7763t+kfA7/StewfjGHb36dzNPPcnm1/r//qmo0MBM1UK4FXt3f2vZ4FPFRVjyU5mc7RBABJXpXkhe0UyY/ovAjuSXJkkje3F+vHgUeBPcPsez2wOskhSRYC7x2uk0nenmReVf0C+GEr7wF20jlFNZ6/Afh3bd9HAe8Drmz1m4FXJDk6yVxgdU+7B4bbXzsNtB5Yk+RZSZ4L/FueeH1Gs5iBoBmpqv5vVW0aZvHvAL+fZBfwIf7+Qi503jVfRScM7gD+N50XvacA76fzbvshOtccfmeY7X+EzumUe4Gv0jkNNZxlwOYkj9K5wLy8qh5rp1zWAN9qn3Y6ZZQhd7sauIlOAHwJuASgqq6lEw63tOVf7Gn3CeCM9imhftc93kvnKOMe4Jt0jrQ+M4Z+aT8Xb5AjSQKPECRJjYEgSQIMBElSYyBIkgADQZLU7LPftnj44YfXokWLprsbkrRPuemmm75fVfP6LdtnA2HRokVs2jTcx9QlSf0kGfarWDxlJEkCDARJUmMgSJIAA0GS1BgIkiRgwEBo95C9NcnNSTa12qFJrk1yd3s+pGv91Um2JLkryald9ZPadrYkuWDo9nxJDkxyZavfkGTRBI9TkjSKsRwhvKqqXlxVS9r8ucDGqloMbGzzJDkeWA6cQOergS9s300PcBGwCljcHstafSXwcFUdB5xP50YjkqQp9GROGZ1G576rtOfTu+pXVNXjVXUvsAU4Ocl84OCqur4637l9aU+boW1dBSz15t6SNLUG/cO0Ar6apIA/raqLgSOragdAVe1IckRbdwHQfaPxba328zbdWx9qs7Vta3e7T+5hdG7790tJVtE5wuDoo4e9q+GoFp37pTGt/92PvnHc+5KkkYz19Qgm7zVp0EB4WVVtby/61ya5c4R1+72zrxHqI7XZu9AJoosBlixZ4p19JGkCDXTKqKq2t+cHgb8ATgYeaKeBaM8PttW3sfdNwhfSuW3htjbdW9+rTZI5wFw6tzmUJE2RUQMhyTOSPGtoGngdcBuwAVjRVltB5z6wtPry9smhY+hcPL6xnV7aleSUdn3grJ42Q9s6A/haeW9PSZpSg5wyOhL4i3aNdw7wuar6yyTfBtYnWQncD5wJUFWbk6wHbgd2A+dU1Z62rbOBtcBBwDXtAZ2biF+WZAudI4PlEzA2SdIYjBoIVXUP8KI+9R8AS4dpswZY06e+CTixT/0xWqBIkqaHf6ksSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEnNwIGQ5IAk30nyxTZ/aJJrk9zdng/pWnd1ki1J7kpyalf9pCS3tmUXJEmrH5jkyla/IcmiCRyjJGkAYzlCeB9wR9f8ucDGqloMbGzzJDkeWA6cACwDLkxyQGtzEbAKWNwey1p9JfBwVR0HnA+cN67RSJLGbaBASLIQeCPwZ13l04B1bXodcHpX/Yqqeryq7gW2ACcnmQ8cXFXXV1UBl/a0GdrWVcDSoaMHSdLUGPQI4ePAvwd+0VU7sqp2ALTnI1p9AbC1a71trbagTffW92pTVbuBR4DDejuRZFWSTUk27dy5c8CuS5IGMWogJHkT8GBV3TTgNvu9s68R6iO12btQdXFVLamqJfPmzRuwO5KkQcwZYJ2XAW9O8gbg6cDBST4LPJBkflXtaKeDHmzrbwOO6mq/ENje6gv71LvbbEsyB5gLPDTOMUmSxmHUI4SqWl1VC6tqEZ2LxV+rqrcDG4AVbbUVwNVtegOwvH1y6Bg6F49vbKeVdiU5pV0fOKunzdC2zmj7eMIRgiRp8gxyhDCcjwLrk6wE7gfOBKiqzUnWA7cDu4FzqmpPa3M2sBY4CLimPQAuAS5LsoXOkcHyJ9EvSdI4jCkQquo64Lo2/QNg6TDrrQHW9KlvAk7sU3+MFiiSpOnhXypLkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVIzaiAkeXqSG5P8bZLNST7S6ocmuTbJ3e35kK42q5NsSXJXklO76iclubUtuyBJWv3AJFe2+g1JFk3CWCVJIxjkCOFx4NVV9SLgxcCyJKcA5wIbq2oxsLHNk+R4YDlwArAMuDDJAW1bFwGrgMXtsazVVwIPV9VxwPnAeU9+aJKksRg1EKrj0Tb71PYo4DRgXauvA05v06cBV1TV41V1L7AFODnJfODgqrq+qgq4tKfN0LauApYOHT1IkqbGQNcQkhyQ5GbgQeDaqroBOLKqdgC05yPa6guArV3Nt7XagjbdW9+rTVXtBh4BDhvHeCRJ4zRQIFTVnqp6MbCQzrv9E0dYvd87+xqhPlKbvTecrEqyKcmmnTt3jtJrSdJYjOlTRlX1Q+A6Ouf+H2ingWjPD7bVtgFHdTVbCGxv9YV96nu1STIHmAs81Gf/F1fVkqpaMm/evLF0XZI0ikE+ZTQvybPb9EHAa4A7gQ3AirbaCuDqNr0BWN4+OXQMnYvHN7bTSruSnNKuD5zV02ZoW2cAX2vXGSRJU2TOAOvMB9a1Two9BVhfVV9Mcj2wPslK4H7gTICq2pxkPXA7sBs4p6r2tG2dDawFDgKuaQ+AS4DLkmyhc2SwfCIGJ0ka3KiBUFW3AC/pU/8BsHSYNmuANX3qm4AnXH+oqsdogSJJmh7+pbIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJGCAQkhyV5OtJ7kiyOcn7Wv3QJNcmubs9H9LVZnWSLUnuSnJqV/2kJLe2ZRckSasfmOTKVr8hyaJJGKskaQSDHCHsBt5fVS8ATgHOSXI8cC6wsaoWAxvbPG3ZcuAEYBlwYZID2rYuAlYBi9tjWauvBB6uquOA84HzJmBskqQxGDUQqmpHVf1Nm94F3AEsAE4D1rXV1gGnt+nTgCuq6vGquhfYApycZD5wcFVdX1UFXNrTZmhbVwFLh44eJElTY0zXENqpnJcANwBHVtUO6IQGcERbbQGwtavZtlZb0KZ763u1qardwCPAYX32vyrJpiSbdu7cOZauS5JGMXAgJHkm8D+Af1NVPxpp1T61GqE+Upu9C1UXV9WSqloyb9680bosSRqDgQIhyVPphMGfV9XnW/mBdhqI9vxgq28DjupqvhDY3uoL+9T3apNkDjAXeGisg5Ekjd8gnzIKcAlwR1X9cdeiDcCKNr0CuLqrvrx9cugYOhePb2ynlXYlOaVt86yeNkPbOgP4WrvOIEmaInMGWOdlwDuAW5Pc3Gq/C3wUWJ9kJXA/cCZAVW1Osh64nc4nlM6pqj2t3dnAWuAg4Jr2gE7gXJZkC50jg+VPbliSpLEaNRCq6pv0P8cPsHSYNmuANX3qm4AT+9QfowWKJGl6+JfKkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUjBoIST6T5MEkt3XVDk1ybZK72/MhXctWJ9mS5K4kp3bVT0pya1t2QZK0+oFJrmz1G5IsmuAxSpIGMMgRwlpgWU/tXGBjVS0GNrZ5khwPLAdOaG0uTHJAa3MRsApY3B5D21wJPFxVxwHnA+eNdzCSpPEbNRCq6hvAQz3l04B1bXodcHpX/Yqqeryq7gW2ACcnmQ8cXFXXV1UBl/a0GdrWVcDSoaMHSdLUGe81hCOragdAez6i1RcAW7vW29ZqC9p0b32vNlW1G3gEOGyc/ZIkjdNEX1Tu986+RqiP1OaJG09WJdmUZNPOnTvH2UVJUj/jDYQH2mkg2vODrb4NOKprvYXA9lZf2Ke+V5skc4C5PPEUFQBVdXFVLamqJfPmzRtn1yVJ/Yw3EDYAK9r0CuDqrvry9smhY+hcPL6xnVbaleSUdn3grJ42Q9s6A/hau84gSZpCc0ZbIcnlwCuBw5NsA34P+CiwPslK4H7gTICq2pxkPXA7sBs4p6r2tE2dTecTSwcB17QHwCXAZUm20DkyWD4hI5MkjcmogVBVbx1m0dJh1l8DrOlT3wSc2Kf+GC1QJEnTx79UliQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEzKBCSLEtyV5ItSc6d7v5I0mwzIwIhyQHAfwVeDxwPvDXJ8dPbK0maXWZEIAAnA1uq6p6q+hlwBXDaNPdJkmaVOdPdgWYBsLVrfhvw0t6VkqwCVrXZR5PcNQH7ngs8MtIKOW+gdYdb1q/eWxtpvnv6cOD7I/V1QKOOeQzrDjK+frX9bcz96o55do65tzbhY+56TRqpn/3MBZ477NKqmvYHcCbwZ13z7wA+OUX7vngi1h1uWb96b22k+Z7pTTNpzIOObzaMeaxjdMz775hHGeeMHvNMOWW0DTiqa34hsH2K9v0/J2jd4Zb1q/fWRpofS/8GNVFjHm75bBxzv7pjnp1j7q3tM2NOS41plWQO8HfAUuB7wLeB36qqzdPasRkmyaaqWjLd/ZhKjnl2cMwzw4y4hlBVu5O8B/gKcADwGcOgr4unuwPTwDHPDo55BpgRRwiSpOk3U64hSJKmmYEgSQIMBElSYyDsJ5KcnuTTSa5O8rrp7s9USHJskkuSXDXdfZksSZ6RZF372b5tuvszVWbDz7bXTPgdNhBmgCSfSfJgktt66gN/4V9VfaGqfht4J/CWSezuhJigMd9TVSsnt6cTb4xj/+fAVe1n++Yp7+wEGsu499Wfba8xjnnaf4cNhJlhLbCsuzDcF/4leWGSL/Y8juhq+sHWbqZby8SNeV+zlgHHTuePNIe+1mXPFPZxMqxl8HHvL9Yy9jFP2+/wjPg7hNmuqr6RZFFP+Zdf+AeQ5ArgtKr6Q+BNvdtIEuCjwDVV9TeT3OUnbSLGvK8ay9jp/BX/QuBm9vE3cGMc9+1T3L1JMZYxJ7mDaf4d3qf/g+3n+n3h34IR1n8v8BrgjCTvnsyOTaIxjTnJYUn+BHhJktWT3blJNtzYPw/8iyQXMTlfezDd+o57P/vZ9hruZz3tv8MeIcxc6VMb9q8Iq+oC4ILJ686UGOuYfwDsq+HXq+/Yq+rHwLumujNTaLhx708/217DjXnaf4c9Qpi5pvML/6bLbBzzkNk69tk47hk7ZgNh5vo2sDjJMUmeBiwHNkxznybbbBzzkNk69tk47hk7ZgNhBkhyOXA98Lwk25KsrKrdwNAX/t0BrN+fvvBvNo55yGwd+2wc9742Zr/cTpIEeIQgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQNCskqSSXdc3PSbIzyRcncZ8fT/KKrv39QZK7k9zcHh8Ypf3aJP+6p3Z6ki8neVqSbyTx+8g0YQwEzRY/Bk5MclCbfy3wvcnaWZJDgVOq6hut9J+A5wAvrKoXAy8HnjrKZi6n87UG3ZYDl1fVz4CN7AM3Q9K+w0DQbHIN8MY2/VY6L7gAJDk5yf9J8p32/LxWPyHJje0d/S1JFqdzW8svJfnbJLcl6feifAbwl20bvwL8NvDeqnoMoKp2VdWHu/b/9q79/Gm7icr/Ap6fZH7Xdl4DfKE1+wIwa26rqclnIGg2uQJYnuTpwK8CN3QtuxN4RVW9BPgQ8Aet/m7gE+1d/RI631S5DNheVS+qqhNpL/w9Xgbc1KaPA+6vql39OpXkBXTe6b+s7WcP8Laq2kPnfgi/2VZ9M/D1ru3cBvz64MOXRmYgaNaoqluARXSODr7cs3gu8N/Tufft+cAJrX498LtJ/gPw3Kr6KXAr8Jok5yV5eVU90md384Gd/fqR5F3tSGBrkqOApcBJwLeT3Nzmj22rd582Wk7XUU0LjJ8ledag/wbSSAwEzTYbgD+i64W1+Y903n2fCPxT4OkAVfU5Ou/Mfwp8Jcmrq+rv6LyA3wr8YZIP9dnPT4e2AWwBjh564a6q/9aOBB4BDqBzw5R1VfXi9nhe1+mkbwHzk7wI+A2eGGQHAo+N/Z9BeiIDQbPNZ4Dfr6pbe+pz+fuLzO8cKiY5Frin3c1qA/CrSZ4D/KSqPksnXH6tz37uoHOqiKr6CXAJ8Kl2umroRutPa+tupHPbxCPaskOTPLe1LWA9sA748tA1iLbeYcDOqvr5eP4hpF4GgmaVqtpWVZ/os+hjdN7tf4vOu/YhbwFua6dyng9cCrwQuLHVPkDnE0S9vgS8smv+A8COtq3vAH9F50V+e1XdDnwQ+GqSW4Br6ZxyGnI58CI610C6vYonHjFI4+b9EKRJkuSbwJuq6oeTtP3PA6ur6q7J2L5mH48QpMnzfuDoydhwu/XiFwwDTSSPECRJgEcIkqTGQJAkAQaCJKkxECRJgIEgSWr+P4VA4jdNAWvIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting histograms\n",
    "def plot_loghist(x, bins):\n",
    "    hist, bins = np.histogram(x, bins=bins)\n",
    "    logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
    "    plt.hist(x, bins=logbins)\n",
    "    plt.xlabel(\"Mass (GeV)\")\n",
    "    plt.title(\"Mass distribution\")\n",
    "    plt.xscale('log')\n",
    "    plt.show()\n",
    "\n",
    "#Mass distribution\n",
    "plot_loghist(M_list, 25)\n",
    "\n",
    "#Energy distribution\n",
    "plt.hist([E1_list, E2_list], label=['E1', 'E2'], bins=25, color=['orange','b'])\n",
    "plt.title(\"Energy distribution\")\n",
    "plt.xlabel(\"Energy (GeV)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Theta distribution\n",
    "P1, theta1, phi1 = cart2sph(px1_list,py1_list,pz1_list)\n",
    "P2, theta2, phi2 = cart2sph(px2_list,py2_list,pz2_list)\n",
    "\n",
    "plt.hist(theta1_list, bins=100, label='theta1', alpha=0.5, color='b')\n",
    "plt.hist(theta2_list, bins=100, label='theta2', alpha=0.5, color='y')\n",
    "plt.title(\"Theta distribution\")\n",
    "plt.xlabel(\"Theta\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Phi distribution\n",
    "plt.hist(phi1_list, bins=100, label='phi1', alpha=0.5, color='b')\n",
    "plt.hist(phi2_list, bins=100, label='phi2', alpha=0.5, color='y')\n",
    "plt.title(\"Phi distribution\")\n",
    "plt.xlabel(\"Phi\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Px distribution\n",
    "plt.hist(px1_list, bins=100, label='px_1', alpha=0.5, color='b')\n",
    "plt.hist(px2_list, bins=100, label='px_2', alpha=0.5, color='y')\n",
    "plt.title(\"Px distribution\")\n",
    "plt.xlabel(\"Px\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Py distribution\n",
    "plt.hist(py1_list, bins=100, label='py_1', alpha=0.5, color='b')\n",
    "plt.hist(py2_list, bins=100, label='py_2', alpha=0.5, color='y')\n",
    "plt.title(\"Py distribution\")\n",
    "plt.xlabel(\"Py\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Pz distribution\n",
    "plt.hist(pz1_list, bins=100, label='pz_1', alpha=0.5, color='b')\n",
    "plt.hist(pz2_list, bins=100, label='pz_2', alpha=0.5, color='y')\n",
    "plt.title(\"Pz distribution\")\n",
    "plt.xlabel(\"Pz\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Relativistic dispersion relation\n",
    "px1_list = np.array(px1_list)\n",
    "py1_list = np.array(py1_list)\n",
    "pz1_list = np.array(pz1_list)\n",
    "px2_list = np.array(px2_list)\n",
    "py2_list = np.array(py2_list)\n",
    "pz2_list = np.array(pz2_list)\n",
    "E1_list = np.array(E1_list)\n",
    "E2_list = np.array(E2_list)\n",
    "\n",
    "P1_list = np.sqrt(px1_list**2 + py1_list**2 + pz1_list**2)\n",
    "P2_list = np.sqrt(px2_list**2 + py2_list**2 + pz2_list**2)\n",
    "\n",
    "plt.hist(E1_list**2 - P1_list**2, bins=100, label='Particle 1', alpha=0.5, color='b')\n",
    "plt.hist(E2_list**2 - P2_list**2, bins=100, label='Particle 2', alpha=0.5, color='y')\n",
    "plt.title(\"Relativistic Dispersion Relation\")\n",
    "plt.ylabel(\"M^2\")\n",
    "plt.xlabel(\"E^2 - P^2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6092d8",
   "metadata": {},
   "source": [
    "## Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec691746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[291.2167811518605, 283.89847772642014, 199.99496309740064, 199.9777312774691, 199.99369143654667, 199.99019853997123, 199.9999998802648, 199.9999998969173]\n",
      "[112.60953952741993, 114.3396506683545, -199.97988556875922, -199.98280981341838, -199.98504924121696, -199.97611272386885, -199.99999996716505, -199.9999999656865]\n"
     ]
    }
   ],
   "source": [
    "#Extracting data from datafile\n",
    "data = np.genfromtxt('MC_data.csv', delimiter=',') #Reading datafile\n",
    "data = np.delete(data, 0, 0) #Removing header\n",
    "data = np.delete(data, 0, 1) #Removing ID column\n",
    "mass = data[:,-1] #Making mass a separate array\n",
    "data = np.delete(data, -1, 1)\n",
    "\n",
    "\n",
    "maxima = []\n",
    "minima = []\n",
    "\n",
    "#Normalizing\n",
    "for i in range(8):\n",
    "    imax = max(data[:,i])\n",
    "    imin = min(data[:,i])\n",
    "    data[:,i] = (data[:,i] - imin)/(imax - imin)\n",
    "    maxima.append(imax)\n",
    "    minima.append(imin)\n",
    "    \n",
    "X_train = data\n",
    "y_train = mass\n",
    "\n",
    "print(maxima)\n",
    "print(minima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92d72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling class\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328d0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAE class\n",
    "class VAE(keras.Model):\n",
    "    #Initializing variables\n",
    "    def __init__(self, encoder, decoder, input_shape, beta, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.beta = beta\n",
    "        self.build((None,) + input_shape)\n",
    "    \n",
    "    #Custom call function\n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)[2]\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    #Custom metrics\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    #Custom train step\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data) #Calculating z, mean and variance\n",
    "            reconstruction = self.decoder(z) #Reconstructing data using decoder\n",
    "            reconstruction_loss = tf.reduce_mean( #Calculating reconstruction loss\n",
    "                tf.reduce_sum(\n",
    "                    mse(data, reconstruction)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)) #Calculating kl-loss\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + self.beta * kl_loss #Calculating total loss\n",
    "            \n",
    "        #Updating values\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    #Custom test step\n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data) #Calculating z, mean and variance\n",
    "        reconstruction = self.decoder(z) #Reconstructing data using decoder\n",
    "        reconstruction_loss = tf.reduce_mean( #Calculating reconstruction loss\n",
    "            tf.reduce_sum(\n",
    "                mse(data, reconstruction)\n",
    "            )\n",
    "        )\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)) #Calculating kl-loss\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss #Calculating total loss\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df17286",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9d6386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 08:22:46,884]\u001b[0m A new study created in memory with name: no-name-d2ee4e16-e042-44db-b763-618631618b64\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 7ms/step - loss: 19.1318 - reconstruction_loss: 15.4030 - kl_loss: 0.9402 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 12.5385 - reconstruction_loss: 9.1931 - kl_loss: 2.2392 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 10.5010 - reconstruction_loss: 7.9537 - kl_loss: 2.5616 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 10.4364 - reconstruction_loss: 7.8605 - kl_loss: 2.5829 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 10.3754 - reconstruction_loss: 7.6374 - kl_loss: 2.6356 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 10.0968 - reconstruction_loss: 7.2001 - kl_loss: 2.7752 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 9.7641 - reconstruction_loss: 6.8124 - kl_loss: 2.9026 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 9.5614 - reconstruction_loss: 6.5231 - kl_loss: 2.9817 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 9.3705 - reconstruction_loss: 6.2876 - kl_loss: 3.0380 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 9.2674 - reconstruction_loss: 6.1098 - kl_loss: 3.0890 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 9.1294 - reconstruction_loss: 5.9567 - kl_loss: 3.1138 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 9.0318 - reconstruction_loss: 5.8434 - kl_loss: 3.1340 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 8.9115 - reconstruction_loss: 5.7307 - kl_loss: 3.1564 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 8.8585 - reconstruction_loss: 5.6274 - kl_loss: 3.1742 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 8.7409 - reconstruction_loss: 5.5214 - kl_loss: 3.2096 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 8.7018 - reconstruction_loss: 5.4359 - kl_loss: 3.2267 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 8.6619 - reconstruction_loss: 5.3418 - kl_loss: 3.2516 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 8.5476 - reconstruction_loss: 5.1932 - kl_loss: 3.3005 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 8.4507 - reconstruction_loss: 5.0502 - kl_loss: 3.3413 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 8.3122 - reconstruction_loss: 4.8962 - kl_loss: 3.3824 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 8.2536 - reconstruction_loss: 4.7851 - kl_loss: 3.4158 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 8.1953 - reconstruction_loss: 4.7096 - kl_loss: 3.4325 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 8.0841 - reconstruction_loss: 4.6186 - kl_loss: 3.4531 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 8.0510 - reconstruction_loss: 4.5294 - kl_loss: 3.4859 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.9658 - reconstruction_loss: 4.4521 - kl_loss: 3.4925 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.9572 - reconstruction_loss: 4.3906 - kl_loss: 3.5281 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.8959 - reconstruction_loss: 4.3176 - kl_loss: 3.5450 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.8335 - reconstruction_loss: 4.2371 - kl_loss: 3.5628 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.7880 - reconstruction_loss: 4.1929 - kl_loss: 3.5784 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.7877 - reconstruction_loss: 4.1456 - kl_loss: 3.5903 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.7071 - reconstruction_loss: 4.0886 - kl_loss: 3.6052 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.7049 - reconstruction_loss: 4.0718 - kl_loss: 3.6144 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.6592 - reconstruction_loss: 4.0183 - kl_loss: 3.6215 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.6336 - reconstruction_loss: 3.9882 - kl_loss: 3.6313 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.6043 - reconstruction_loss: 3.9443 - kl_loss: 3.6450 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.5769 - reconstruction_loss: 3.8959 - kl_loss: 3.6502 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.5468 - reconstruction_loss: 3.8724 - kl_loss: 3.6633 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.5402 - reconstruction_loss: 3.8521 - kl_loss: 3.6742 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.5276 - reconstruction_loss: 3.8019 - kl_loss: 3.7000 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.4963 - reconstruction_loss: 3.7695 - kl_loss: 3.7132 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.4725 - reconstruction_loss: 3.7377 - kl_loss: 3.7251 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.4452 - reconstruction_loss: 3.7167 - kl_loss: 3.7231 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.4444 - reconstruction_loss: 3.7015 - kl_loss: 3.7286 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.4265 - reconstruction_loss: 3.6909 - kl_loss: 3.7276 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.4240 - reconstruction_loss: 3.6704 - kl_loss: 3.7347 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3885 - reconstruction_loss: 3.6416 - kl_loss: 3.7411 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3816 - reconstruction_loss: 3.6220 - kl_loss: 3.7515 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3769 - reconstruction_loss: 3.6157 - kl_loss: 3.7516 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3428 - reconstruction_loss: 3.6046 - kl_loss: 3.7533 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3792 - reconstruction_loss: 3.5971 - kl_loss: 3.7541 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3860 - reconstruction_loss: 3.5952 - kl_loss: 3.7539 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3445 - reconstruction_loss: 3.5607 - kl_loss: 3.7659 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3270 - reconstruction_loss: 3.5531 - kl_loss: 3.7649 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3069 - reconstruction_loss: 3.5403 - kl_loss: 3.7659 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3104 - reconstruction_loss: 3.5270 - kl_loss: 3.7745 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3021 - reconstruction_loss: 3.5245 - kl_loss: 3.7753 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3100 - reconstruction_loss: 3.5296 - kl_loss: 3.7660 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.2996 - reconstruction_loss: 3.5171 - kl_loss: 3.7687 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3059 - reconstruction_loss: 3.5144 - kl_loss: 3.7700 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3160 - reconstruction_loss: 3.5153 - kl_loss: 3.7773 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.2798 - reconstruction_loss: 3.4986 - kl_loss: 3.7686 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.2654 - reconstruction_loss: 3.4990 - kl_loss: 3.7692 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.2757 - reconstruction_loss: 3.5133 - kl_loss: 3.7812 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 7.3242 - reconstruction_loss: 3.5257 - kl_loss: 3.7795\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.3239 - reconstruction_loss: 3.5130 - kl_loss: 3.7793 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.2830 - reconstruction_loss: 3.4681 - kl_loss: 3.7889 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.2361 - reconstruction_loss: 3.4393 - kl_loss: 3.7765 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.2288 - reconstruction_loss: 3.4285 - kl_loss: 3.7828 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.2188 - reconstruction_loss: 3.4305 - kl_loss: 3.7778 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.2415 - reconstruction_loss: 3.4278 - kl_loss: 3.7837 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.2062 - reconstruction_loss: 3.4280 - kl_loss: 3.7879 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 7.2396 - reconstruction_loss: 3.4418 - kl_loss: 3.7889\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.2392 - reconstruction_loss: 3.4320 - kl_loss: 3.7882 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 7.2016 - reconstruction_loss: 3.4147 - kl_loss: 3.7853 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1991 - reconstruction_loss: 3.4004 - kl_loss: 3.7869 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1812 - reconstruction_loss: 3.3927 - kl_loss: 3.7809 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1877 - reconstruction_loss: 3.3843 - kl_loss: 3.7953 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.2074 - reconstruction_loss: 3.3873 - kl_loss: 3.7944 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 7.1923 - reconstruction_loss: 3.3987 - kl_loss: 3.7963\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1922 - reconstruction_loss: 3.3874 - kl_loss: 3.7961 - lr: 2.5000e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.2044 - reconstruction_loss: 3.3900 - kl_loss: 3.7932 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1917 - reconstruction_loss: 3.3860 - kl_loss: 3.7907 - lr: 1.2500e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1744 - reconstruction_loss: 3.3779 - kl_loss: 3.7870 - lr: 1.2500e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1615 - reconstruction_loss: 3.3784 - kl_loss: 3.7867 - lr: 1.2500e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1760 - reconstruction_loss: 3.3757 - kl_loss: 3.7948 - lr: 1.2500e-05\n",
      "Epoch 83/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 7.1966 - reconstruction_loss: 3.3972 - kl_loss: 3.7956\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1963 - reconstruction_loss: 3.3822 - kl_loss: 3.7955 - lr: 1.2500e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1783 - reconstruction_loss: 3.3711 - kl_loss: 3.7957 - lr: 6.2500e-06\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1601 - reconstruction_loss: 3.3617 - kl_loss: 3.7914 - lr: 6.2500e-06\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1740 - reconstruction_loss: 3.3755 - kl_loss: 3.7870 - lr: 6.2500e-06\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1561 - reconstruction_loss: 3.3687 - kl_loss: 3.7957 - lr: 6.2500e-06\n",
      "Epoch 88/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 7.1829 - reconstruction_loss: 3.3676 - kl_loss: 3.7994\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1826 - reconstruction_loss: 3.3553 - kl_loss: 3.7985 - lr: 6.2500e-06\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1448 - reconstruction_loss: 3.3618 - kl_loss: 3.7924 - lr: 3.1250e-06\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1850 - reconstruction_loss: 3.3652 - kl_loss: 3.7956 - lr: 3.1250e-06\n",
      "Epoch 91/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 7.1619 - reconstruction_loss: 3.3697 - kl_loss: 3.7993\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1620 - reconstruction_loss: 3.3596 - kl_loss: 3.7991 - lr: 3.1250e-06\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1728 - reconstruction_loss: 3.3566 - kl_loss: 3.7933 - lr: 1.5625e-06\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1491 - reconstruction_loss: 3.3552 - kl_loss: 3.7932 - lr: 1.5625e-06\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1761 - reconstruction_loss: 3.3522 - kl_loss: 3.7931 - lr: 1.5625e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1691 - reconstruction_loss: 3.3639 - kl_loss: 3.7932 - lr: 1.5625e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1697 - reconstruction_loss: 3.3583 - kl_loss: 3.7927 - lr: 1.5625e-06\n",
      "Epoch 97/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 7.1649 - reconstruction_loss: 3.3666 - kl_loss: 3.7942\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1648 - reconstruction_loss: 3.3550 - kl_loss: 3.7945 - lr: 1.5625e-06\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1461 - reconstruction_loss: 3.3532 - kl_loss: 3.7931 - lr: 1.0000e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1723 - reconstruction_loss: 3.3610 - kl_loss: 3.7891 - lr: 1.0000e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1710 - reconstruction_loss: 3.3644 - kl_loss: 3.7912 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1645 - reconstruction_loss: 3.3636 - kl_loss: 3.7930 - lr: 1.0000e-06\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1586 - reconstruction_loss: 3.3597 - kl_loss: 3.7924 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1726 - reconstruction_loss: 3.3599 - kl_loss: 3.7953 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.1591 - reconstruction_loss: 3.3583 - kl_loss: 3.7950 - lr: 1.0000e-06\n",
      "Epoch 104: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 08:25:16,831]\u001b[0m Trial 0 finished with value: 0.006579539926431266 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 60, 'encoder_units_l2': 54, 'encoder_units_l3': 513, 'encoder_units_l4': 492, 'decoder_layers': 7, 'decoder_units_l1': 42, 'decoder_units_l2': 41, 'decoder_units_l3': 85, 'decoder_units_l4': 96, 'decoder_units_l5': 1019, 'decoder_units_l6': 163, 'decoder_units_l7': 197, 'beta': 1, 'lr': 0.0001, 'batch_size': 512}. Best is trial 0 with value: 0.006579539926431266.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 7.1364 - reconstruction_loss: 5.8099 - kl_loss: 2.2001 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 4.0659 - reconstruction_loss: 2.9888 - kl_loss: 3.9289 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.1876 - reconstruction_loss: 2.1878 - kl_loss: 4.4930 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.8563 - reconstruction_loss: 1.8939 - kl_loss: 4.7700 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.8439 - reconstruction_loss: 1.8415 - kl_loss: 4.8320 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.7411 - reconstruction_loss: 1.7967 - kl_loss: 4.9572 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.7764 - reconstruction_loss: 1.7246 - kl_loss: 4.8695 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.6569 - reconstruction_loss: 1.6336 - kl_loss: 4.9216 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5473 - reconstruction_loss: 1.5151 - kl_loss: 4.8712 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4756 - reconstruction_loss: 1.5646 - kl_loss: 4.9444 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.4118 - reconstruction_loss: 2.2590 - kl_loss: 4.8291 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 3.0517 - reconstruction_loss: 1.8847 - kl_loss: 5.1143\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.0495 - reconstruction_loss: 1.8801 - kl_loss: 5.1145 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5649 - reconstruction_loss: 1.4702 - kl_loss: 5.1311 - lr: 5.0000e-04\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4359 - reconstruction_loss: 1.4369 - kl_loss: 5.0061 - lr: 5.0000e-04\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4154 - reconstruction_loss: 1.4115 - kl_loss: 4.9717 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4001 - reconstruction_loss: 1.4300 - kl_loss: 5.0110 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3983 - reconstruction_loss: 1.3920 - kl_loss: 4.9591 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3352 - reconstruction_loss: 1.3699 - kl_loss: 4.9403 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3590 - reconstruction_loss: 1.3632 - kl_loss: 4.9646 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4344 - reconstruction_loss: 1.3953 - kl_loss: 5.0207 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3254 - reconstruction_loss: 1.3355 - kl_loss: 4.9214 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3071 - reconstruction_loss: 1.3315 - kl_loss: 4.8862 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3496 - reconstruction_loss: 1.3581 - kl_loss: 4.9157 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3202 - reconstruction_loss: 1.3385 - kl_loss: 4.8862 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.3219 - reconstruction_loss: 1.3357 - kl_loss: 4.9002\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3218 - reconstruction_loss: 1.3336 - kl_loss: 4.9001 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2700 - reconstruction_loss: 1.3016 - kl_loss: 4.8777 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2711 - reconstruction_loss: 1.2903 - kl_loss: 4.8412 - lr: 2.5000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2397 - reconstruction_loss: 1.2804 - kl_loss: 4.7968 - lr: 2.5000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2317 - reconstruction_loss: 1.2761 - kl_loss: 4.7792 - lr: 2.5000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2413 - reconstruction_loss: 1.2824 - kl_loss: 4.7692 - lr: 2.5000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2389 - reconstruction_loss: 1.2859 - kl_loss: 4.7716 - lr: 2.5000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2314 - reconstruction_loss: 1.2736 - kl_loss: 4.7552 - lr: 2.5000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2260 - reconstruction_loss: 1.2796 - kl_loss: 4.7467 - lr: 2.5000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2247 - reconstruction_loss: 1.2724 - kl_loss: 4.7451 - lr: 2.5000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2342 - reconstruction_loss: 1.2775 - kl_loss: 4.7395 - lr: 2.5000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2193 - reconstruction_loss: 1.2760 - kl_loss: 4.7430 - lr: 2.5000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2180 - reconstruction_loss: 1.2672 - kl_loss: 4.7397 - lr: 2.5000e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2221 - reconstruction_loss: 1.2718 - kl_loss: 4.7378 - lr: 2.5000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2237 - reconstruction_loss: 1.2718 - kl_loss: 4.7357 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2377 - reconstruction_loss: 1.3079 - kl_loss: 4.7823\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2378 - reconstruction_loss: 1.3067 - kl_loss: 4.7829 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2657 - reconstruction_loss: 1.3033 - kl_loss: 4.7902 - lr: 1.2500e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2413 - reconstruction_loss: 1.2749 - kl_loss: 4.7638 - lr: 1.2500e-04\n",
      "Epoch 43/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2267 - reconstruction_loss: 1.2758 - kl_loss: 4.7627\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2267 - reconstruction_loss: 1.2743 - kl_loss: 4.7629 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2196 - reconstruction_loss: 1.2689 - kl_loss: 4.7467 - lr: 6.2500e-05\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2178 - reconstruction_loss: 1.2630 - kl_loss: 4.7522 - lr: 6.2500e-05\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2184 - reconstruction_loss: 1.2672 - kl_loss: 4.7533 - lr: 6.2500e-05\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2284 - reconstruction_loss: 1.2733 - kl_loss: 4.7558 - lr: 6.2500e-05\n",
      "Epoch 48/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2250 - reconstruction_loss: 1.2721 - kl_loss: 4.7555\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2250 - reconstruction_loss: 1.2707 - kl_loss: 4.7554 - lr: 6.2500e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2263 - reconstruction_loss: 1.2702 - kl_loss: 4.7594 - lr: 3.1250e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2202 - reconstruction_loss: 1.2674 - kl_loss: 4.7626 - lr: 3.1250e-05\n",
      "Epoch 51/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2201 - reconstruction_loss: 1.2729 - kl_loss: 4.7586\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2201 - reconstruction_loss: 1.2711 - kl_loss: 4.7580 - lr: 3.1250e-05\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2233 - reconstruction_loss: 1.2700 - kl_loss: 4.7703 - lr: 1.5625e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2182 - reconstruction_loss: 1.2691 - kl_loss: 4.7683 - lr: 1.5625e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2295 - reconstruction_loss: 1.2701 - kl_loss: 4.7619\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2295 - reconstruction_loss: 1.2701 - kl_loss: 4.7619 - lr: 1.5625e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2234 - reconstruction_loss: 1.2647 - kl_loss: 4.7598 - lr: 7.8125e-06\n",
      "Epoch 55: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 08:28:05,084]\u001b[0m Trial 1 finished with value: 0.004936496311177702 and parameters: {'encoder_layers': 5, 'encoder_units_l1': 278, 'encoder_units_l2': 68, 'encoder_units_l3': 27, 'encoder_units_l4': 644, 'encoder_units_l5': 123, 'decoder_layers': 8, 'decoder_units_l1': 141, 'decoder_units_l2': 524, 'decoder_units_l3': 62, 'decoder_units_l4': 373, 'decoder_units_l5': 44, 'decoder_units_l6': 187, 'decoder_units_l7': 70, 'decoder_units_l8': 45, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 1 with value: 0.004936496311177702.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 2.5290 - reconstruction_loss: 2.0496 - kl_loss: 0.4060 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4129 - reconstruction_loss: 1.9614 - kl_loss: 0.4565 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4180 - reconstruction_loss: 1.9578 - kl_loss: 0.4617 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4108 - reconstruction_loss: 1.9539 - kl_loss: 0.4585 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4204 - reconstruction_loss: 1.9571 - kl_loss: 0.4602 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4128 - reconstruction_loss: 1.9558 - kl_loss: 0.4614 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 2.4088 - reconstruction_loss: 1.9550 - kl_loss: 0.4604\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4088 - reconstruction_loss: 1.9544 - kl_loss: 0.4604 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4230 - reconstruction_loss: 1.9505 - kl_loss: 0.4643 - lr: 5.0000e-05\n",
      "Epoch 9/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4100 - reconstruction_loss: 1.9505 - kl_loss: 0.4595 - lr: 5.0000e-05\n",
      "Epoch 10/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4208 - reconstruction_loss: 1.9505 - kl_loss: 0.4603 - lr: 5.0000e-05\n",
      "Epoch 11/300\n",
      "1563/1563 [==============================] - 5s 4ms/step - loss: 2.4147 - reconstruction_loss: 1.9495 - kl_loss: 0.4668 - lr: 5.0000e-05\n",
      "Epoch 12/300\n",
      "1555/1563 [============================>.] - ETA: 0s - loss: 2.4121 - reconstruction_loss: 1.9513 - kl_loss: 0.4603\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4121 - reconstruction_loss: 1.9513 - kl_loss: 0.4605 - lr: 5.0000e-05\n",
      "Epoch 13/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4091 - reconstruction_loss: 1.9479 - kl_loss: 0.4601 - lr: 2.5000e-05\n",
      "Epoch 14/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4039 - reconstruction_loss: 1.9463 - kl_loss: 0.4626 - lr: 2.5000e-05\n",
      "Epoch 15/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4162 - reconstruction_loss: 1.9488 - kl_loss: 0.4625 - lr: 2.5000e-05\n",
      "Epoch 16/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 2.4099 - reconstruction_loss: 1.9469 - kl_loss: 0.4624\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4099 - reconstruction_loss: 1.9464 - kl_loss: 0.4624 - lr: 2.5000e-05\n",
      "Epoch 17/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4117 - reconstruction_loss: 1.9471 - kl_loss: 0.4615 - lr: 1.2500e-05\n",
      "Epoch 18/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4177 - reconstruction_loss: 1.9489 - kl_loss: 0.4644 - lr: 1.2500e-05\n",
      "Epoch 19/300\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 2.4103 - reconstruction_loss: 1.9473 - kl_loss: 0.4638\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4103 - reconstruction_loss: 1.9466 - kl_loss: 0.4638 - lr: 1.2500e-05\n",
      "Epoch 20/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4007 - reconstruction_loss: 1.9443 - kl_loss: 0.4612 - lr: 6.2500e-06\n",
      "Epoch 21/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4065 - reconstruction_loss: 1.9483 - kl_loss: 0.4591 - lr: 6.2500e-06\n",
      "Epoch 22/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4146 - reconstruction_loss: 1.9481 - kl_loss: 0.4644 - lr: 6.2500e-06\n",
      "Epoch 23/300\n",
      "1550/1563 [============================>.] - ETA: 0s - loss: 2.4082 - reconstruction_loss: 1.9457 - kl_loss: 0.4618\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4082 - reconstruction_loss: 1.9448 - kl_loss: 0.4619 - lr: 6.2500e-06\n",
      "Epoch 24/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4122 - reconstruction_loss: 1.9454 - kl_loss: 0.4625 - lr: 3.1250e-06\n",
      "Epoch 25/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4096 - reconstruction_loss: 1.9458 - kl_loss: 0.4634 - lr: 3.1250e-06\n",
      "Epoch 26/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 2.4018 - reconstruction_loss: 1.9466 - kl_loss: 0.4594\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4018 - reconstruction_loss: 1.9460 - kl_loss: 0.4594 - lr: 3.1250e-06\n",
      "Epoch 27/300\n",
      "1563/1563 [==============================] - 5s 4ms/step - loss: 2.4086 - reconstruction_loss: 1.9470 - kl_loss: 0.4631 - lr: 1.5625e-06\n",
      "Epoch 28/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4113 - reconstruction_loss: 1.9457 - kl_loss: 0.4615 - lr: 1.5625e-06\n",
      "Epoch 29/300\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 2.4136 - reconstruction_loss: 1.9464 - kl_loss: 0.4638\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4135 - reconstruction_loss: 1.9457 - kl_loss: 0.4637 - lr: 1.5625e-06\n",
      "Epoch 30/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4071 - reconstruction_loss: 1.9466 - kl_loss: 0.4634 - lr: 1.0000e-06\n",
      "Epoch 31/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4055 - reconstruction_loss: 1.9449 - kl_loss: 0.4637 - lr: 1.0000e-06\n",
      "Epoch 32/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4140 - reconstruction_loss: 1.9456 - kl_loss: 0.4622 - lr: 1.0000e-06\n",
      "Epoch 33/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4112 - reconstruction_loss: 1.9481 - kl_loss: 0.4598 - lr: 1.0000e-06\n",
      "Epoch 34/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4049 - reconstruction_loss: 1.9441 - kl_loss: 0.4613 - lr: 1.0000e-06\n",
      "Epoch 35/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4043 - reconstruction_loss: 1.9456 - kl_loss: 0.4621 - lr: 1.0000e-06\n",
      "Epoch 36/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4036 - reconstruction_loss: 1.9452 - kl_loss: 0.4633 - lr: 1.0000e-06\n",
      "Epoch 36: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 08:31:18,054]\u001b[0m Trial 2 finished with value: 0.030475393185873525 and parameters: {'encoder_layers': 6, 'encoder_units_l1': 68, 'encoder_units_l2': 17, 'encoder_units_l3': 55, 'encoder_units_l4': 172, 'encoder_units_l5': 105, 'encoder_units_l6': 17, 'decoder_layers': 8, 'decoder_units_l1': 65, 'decoder_units_l2': 555, 'decoder_units_l3': 514, 'decoder_units_l4': 22, 'decoder_units_l5': 382, 'decoder_units_l6': 704, 'decoder_units_l7': 937, 'decoder_units_l8': 209, 'beta': 1, 'lr': 0.0001, 'batch_size': 64}. Best is trial 1 with value: 0.004936496311177702.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 3.7034 - reconstruction_loss: 2.4653 - kl_loss: 2.3400 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.4754 - reconstruction_loss: 1.8847 - kl_loss: 2.8514 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.2481 - reconstruction_loss: 1.4937 - kl_loss: 3.2177 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.9608 - reconstruction_loss: 1.2605 - kl_loss: 3.3993 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.9042 - reconstruction_loss: 1.2024 - kl_loss: 3.4444 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.8828 - reconstruction_loss: 1.1732 - kl_loss: 3.4835 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.8547 - reconstruction_loss: 1.1436 - kl_loss: 3.5179 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.8399 - reconstruction_loss: 1.1250 - kl_loss: 3.5442 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.8207 - reconstruction_loss: 1.1064 - kl_loss: 3.5621 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.8077 - reconstruction_loss: 1.0895 - kl_loss: 3.5869 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.7953 - reconstruction_loss: 1.0747 - kl_loss: 3.6103 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.7933 - reconstruction_loss: 1.0585 - kl_loss: 3.6420 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.7756 - reconstruction_loss: 1.0414 - kl_loss: 3.6655 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.7608 - reconstruction_loss: 1.0198 - kl_loss: 3.7063 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.7559 - reconstruction_loss: 1.0014 - kl_loss: 3.7491 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.7483 - reconstruction_loss: 0.9805 - kl_loss: 3.8057 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.7241 - reconstruction_loss: 0.9503 - kl_loss: 3.8534 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.7108 - reconstruction_loss: 0.9275 - kl_loss: 3.8965 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6922 - reconstruction_loss: 0.9024 - kl_loss: 3.9313 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6803 - reconstruction_loss: 0.8879 - kl_loss: 3.9609 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6837 - reconstruction_loss: 0.8834 - kl_loss: 3.9876 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6659 - reconstruction_loss: 0.8612 - kl_loss: 3.9993 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6549 - reconstruction_loss: 0.8533 - kl_loss: 4.0221 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6527 - reconstruction_loss: 0.8478 - kl_loss: 4.0241 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6495 - reconstruction_loss: 0.8441 - kl_loss: 4.0280 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6493 - reconstruction_loss: 0.8385 - kl_loss: 4.0324 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6491 - reconstruction_loss: 0.8334 - kl_loss: 4.0551 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6338 - reconstruction_loss: 0.8254 - kl_loss: 4.0595 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6452 - reconstruction_loss: 0.8295 - kl_loss: 4.0535 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6338 - reconstruction_loss: 0.8207 - kl_loss: 4.0751 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6297 - reconstruction_loss: 0.8121 - kl_loss: 4.0710 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6336 - reconstruction_loss: 0.8170 - kl_loss: 4.0775 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6278 - reconstruction_loss: 0.8057 - kl_loss: 4.0882 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6227 - reconstruction_loss: 0.8065 - kl_loss: 4.0678 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6183 - reconstruction_loss: 0.8056 - kl_loss: 4.0831 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6416 - reconstruction_loss: 0.8138 - kl_loss: 4.0811 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6265 - reconstruction_loss: 0.7990 - kl_loss: 4.0901 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6163 - reconstruction_loss: 0.8024 - kl_loss: 4.0958 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6108 - reconstruction_loss: 0.7876 - kl_loss: 4.0959 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6195 - reconstruction_loss: 0.8027 - kl_loss: 4.0775 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6127 - reconstruction_loss: 0.7937 - kl_loss: 4.0813 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "771/782 [============================>.] - ETA: 0s - loss: 1.6103 - reconstruction_loss: 0.7962 - kl_loss: 4.0846\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6103 - reconstruction_loss: 0.7955 - kl_loss: 4.0853 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6020 - reconstruction_loss: 0.7766 - kl_loss: 4.1022 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5922 - reconstruction_loss: 0.7744 - kl_loss: 4.1063 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5950 - reconstruction_loss: 0.7712 - kl_loss: 4.0980 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5890 - reconstruction_loss: 0.7708 - kl_loss: 4.0877 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5888 - reconstruction_loss: 0.7715 - kl_loss: 4.0815 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5833 - reconstruction_loss: 0.7656 - kl_loss: 4.0952 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5830 - reconstruction_loss: 0.7635 - kl_loss: 4.0989 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5886 - reconstruction_loss: 0.7709 - kl_loss: 4.0872 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5863 - reconstruction_loss: 0.7652 - kl_loss: 4.0918 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5860 - reconstruction_loss: 0.7627 - kl_loss: 4.0910 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5836 - reconstruction_loss: 0.7631 - kl_loss: 4.0835 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5812 - reconstruction_loss: 0.7623 - kl_loss: 4.0944 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5796 - reconstruction_loss: 0.7584 - kl_loss: 4.0981 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5842 - reconstruction_loss: 0.7588 - kl_loss: 4.1090 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5836 - reconstruction_loss: 0.7668 - kl_loss: 4.0989 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "777/782 [============================>.] - ETA: 0s - loss: 1.5767 - reconstruction_loss: 0.7629 - kl_loss: 4.0861\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5767 - reconstruction_loss: 0.7622 - kl_loss: 4.0863 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5730 - reconstruction_loss: 0.7541 - kl_loss: 4.0970 - lr: 2.5000e-05\n",
      "Epoch 60/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5674 - reconstruction_loss: 0.7480 - kl_loss: 4.1017 - lr: 2.5000e-05\n",
      "Epoch 61/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5711 - reconstruction_loss: 0.7505 - kl_loss: 4.0996 - lr: 2.5000e-05\n",
      "Epoch 62/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5652 - reconstruction_loss: 0.7484 - kl_loss: 4.0952 - lr: 2.5000e-05\n",
      "Epoch 63/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5706 - reconstruction_loss: 0.7457 - kl_loss: 4.1040 - lr: 2.5000e-05\n",
      "Epoch 64/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5708 - reconstruction_loss: 0.7489 - kl_loss: 4.0998 - lr: 2.5000e-05\n",
      "Epoch 65/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5674 - reconstruction_loss: 0.7484 - kl_loss: 4.0967 - lr: 2.5000e-05\n",
      "Epoch 66/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5691 - reconstruction_loss: 0.7462 - kl_loss: 4.1006 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5620 - reconstruction_loss: 0.7473 - kl_loss: 4.0943 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5651 - reconstruction_loss: 0.7467 - kl_loss: 4.1052 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5719 - reconstruction_loss: 0.7511 - kl_loss: 4.0932 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5678 - reconstruction_loss: 0.7473 - kl_loss: 4.0900 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5674 - reconstruction_loss: 0.7486 - kl_loss: 4.0871 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5714 - reconstruction_loss: 0.7493 - kl_loss: 4.1004 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.5668 - reconstruction_loss: 0.7490 - kl_loss: 4.0862\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5668 - reconstruction_loss: 0.7483 - kl_loss: 4.0860 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5617 - reconstruction_loss: 0.7421 - kl_loss: 4.0924 - lr: 1.2500e-05\n",
      "Epoch 75/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5561 - reconstruction_loss: 0.7427 - kl_loss: 4.0735 - lr: 1.2500e-05\n",
      "Epoch 76/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5567 - reconstruction_loss: 0.7412 - kl_loss: 4.0801 - lr: 1.2500e-05\n",
      "Epoch 77/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5622 - reconstruction_loss: 0.7420 - kl_loss: 4.0947 - lr: 1.2500e-05\n",
      "Epoch 78/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5578 - reconstruction_loss: 0.7413 - kl_loss: 4.0895 - lr: 1.2500e-05\n",
      "Epoch 79/300\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.5605 - reconstruction_loss: 0.7428 - kl_loss: 4.0799\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5605 - reconstruction_loss: 0.7420 - kl_loss: 4.0799 - lr: 1.2500e-05\n",
      "Epoch 80/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5553 - reconstruction_loss: 0.7386 - kl_loss: 4.0880 - lr: 6.2500e-06\n",
      "Epoch 81/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5548 - reconstruction_loss: 0.7398 - kl_loss: 4.0867 - lr: 6.2500e-06\n",
      "Epoch 82/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5619 - reconstruction_loss: 0.7407 - kl_loss: 4.0767 - lr: 6.2500e-06\n",
      "Epoch 83/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5540 - reconstruction_loss: 0.7389 - kl_loss: 4.0796 - lr: 6.2500e-06\n",
      "Epoch 84/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5575 - reconstruction_loss: 0.7399 - kl_loss: 4.0806 - lr: 6.2500e-06\n",
      "Epoch 85/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5516 - reconstruction_loss: 0.7390 - kl_loss: 4.0791 - lr: 6.2500e-06\n",
      "Epoch 86/300\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.5579 - reconstruction_loss: 0.7413 - kl_loss: 4.0841\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5579 - reconstruction_loss: 0.7406 - kl_loss: 4.0842 - lr: 6.2500e-06\n",
      "Epoch 87/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5551 - reconstruction_loss: 0.7345 - kl_loss: 4.0951 - lr: 3.1250e-06\n",
      "Epoch 88/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5545 - reconstruction_loss: 0.7381 - kl_loss: 4.0815 - lr: 3.1250e-06\n",
      "Epoch 89/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5531 - reconstruction_loss: 0.7365 - kl_loss: 4.0855 - lr: 3.1250e-06\n",
      "Epoch 90/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5533 - reconstruction_loss: 0.7359 - kl_loss: 4.0865 - lr: 3.1250e-06\n",
      "Epoch 91/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5535 - reconstruction_loss: 0.7368 - kl_loss: 4.0852 - lr: 3.1250e-06\n",
      "Epoch 92/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5560 - reconstruction_loss: 0.7357 - kl_loss: 4.0897 - lr: 3.1250e-06\n",
      "Epoch 93/300\n",
      "777/782 [============================>.] - ETA: 0s - loss: 1.5589 - reconstruction_loss: 0.7363 - kl_loss: 4.0952\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5588 - reconstruction_loss: 0.7355 - kl_loss: 4.0952 - lr: 3.1250e-06\n",
      "Epoch 94/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5607 - reconstruction_loss: 0.7360 - kl_loss: 4.0941 - lr: 1.5625e-06\n",
      "Epoch 95/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5572 - reconstruction_loss: 0.7366 - kl_loss: 4.0897 - lr: 1.5625e-06\n",
      "Epoch 96/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5538 - reconstruction_loss: 0.7348 - kl_loss: 4.0901 - lr: 1.5625e-06\n",
      "Epoch 97/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5547 - reconstruction_loss: 0.7365 - kl_loss: 4.0859 - lr: 1.5625e-06\n",
      "Epoch 98/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5538 - reconstruction_loss: 0.7372 - kl_loss: 4.0914 - lr: 1.5625e-06\n",
      "Epoch 99/300\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.5546 - reconstruction_loss: 0.7357 - kl_loss: 4.0930\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5545 - reconstruction_loss: 0.7349 - kl_loss: 4.0926 - lr: 1.5625e-06\n",
      "Epoch 100/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5523 - reconstruction_loss: 0.7339 - kl_loss: 4.0901 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5559 - reconstruction_loss: 0.7345 - kl_loss: 4.0879 - lr: 1.0000e-06\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5521 - reconstruction_loss: 0.7351 - kl_loss: 4.0910 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5514 - reconstruction_loss: 0.7346 - kl_loss: 4.0874 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5500 - reconstruction_loss: 0.7349 - kl_loss: 4.0832 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5522 - reconstruction_loss: 0.7350 - kl_loss: 4.0832 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5540 - reconstruction_loss: 0.7345 - kl_loss: 4.0854 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5609 - reconstruction_loss: 0.7343 - kl_loss: 4.0963 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5506 - reconstruction_loss: 0.7331 - kl_loss: 4.0951 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5495 - reconstruction_loss: 0.7343 - kl_loss: 4.0857 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5501 - reconstruction_loss: 0.7334 - kl_loss: 4.0851 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5497 - reconstruction_loss: 0.7345 - kl_loss: 4.0853 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5489 - reconstruction_loss: 0.7333 - kl_loss: 4.0882 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5525 - reconstruction_loss: 0.7335 - kl_loss: 4.0868 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5502 - reconstruction_loss: 0.7345 - kl_loss: 4.0874 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5557 - reconstruction_loss: 0.7348 - kl_loss: 4.0917 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5510 - reconstruction_loss: 0.7334 - kl_loss: 4.0905 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5500 - reconstruction_loss: 0.7319 - kl_loss: 4.0860 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5549 - reconstruction_loss: 0.7355 - kl_loss: 4.0873 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5517 - reconstruction_loss: 0.7327 - kl_loss: 4.0942 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5527 - reconstruction_loss: 0.7322 - kl_loss: 4.0862 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5491 - reconstruction_loss: 0.7330 - kl_loss: 4.0868 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5522 - reconstruction_loss: 0.7334 - kl_loss: 4.0913 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5525 - reconstruction_loss: 0.7336 - kl_loss: 4.0880 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5505 - reconstruction_loss: 0.7335 - kl_loss: 4.0855 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5529 - reconstruction_loss: 0.7334 - kl_loss: 4.0899 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5460 - reconstruction_loss: 0.7327 - kl_loss: 4.0928 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5495 - reconstruction_loss: 0.7317 - kl_loss: 4.0870 - lr: 1.0000e-06\n",
      "Epoch 127: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 08:37:54,121]\u001b[0m Trial 3 finished with value: 0.005736962354173272 and parameters: {'encoder_layers': 6, 'encoder_units_l1': 58, 'encoder_units_l2': 463, 'encoder_units_l3': 210, 'encoder_units_l4': 59, 'encoder_units_l5': 39, 'encoder_units_l6': 42, 'decoder_layers': 6, 'decoder_units_l1': 125, 'decoder_units_l2': 241, 'decoder_units_l3': 409, 'decoder_units_l4': 131, 'decoder_units_l5': 833, 'decoder_units_l6': 25, 'beta': 0.2, 'lr': 0.0001, 'batch_size': 128}. Best is trial 1 with value: 0.004936496311177702.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 12ms/step - loss: 19.0204 - reconstruction_loss: 15.0816 - kl_loss: 1.1577 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 13.8638 - reconstruction_loss: 12.2797 - kl_loss: 1.4535 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 13.4467 - reconstruction_loss: 10.6085 - kl_loss: 1.8815 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 10.5660 - reconstruction_loss: 7.9646 - kl_loss: 2.5632 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 10.4594 - reconstruction_loss: 7.6651 - kl_loss: 2.6203 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 9.7130 - reconstruction_loss: 6.5044 - kl_loss: 2.9002 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 8.7350 - reconstruction_loss: 5.4831 - kl_loss: 3.1112 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 8.3729 - reconstruction_loss: 5.1532 - kl_loss: 3.1667 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.2693 - reconstruction_loss: 5.0328 - kl_loss: 3.1849 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 8.1813 - reconstruction_loss: 4.9258 - kl_loss: 3.2131 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 8.0882 - reconstruction_loss: 4.8582 - kl_loss: 3.2310 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 8.0679 - reconstruction_loss: 4.7819 - kl_loss: 3.2492 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 8.0242 - reconstruction_loss: 4.7558 - kl_loss: 3.2521 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.0210 - reconstruction_loss: 4.7189 - kl_loss: 3.2746 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.9990 - reconstruction_loss: 4.6954 - kl_loss: 3.2823 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.9581 - reconstruction_loss: 4.6309 - kl_loss: 3.3010 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.9328 - reconstruction_loss: 4.6084 - kl_loss: 3.3059 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.9592 - reconstruction_loss: 4.6284 - kl_loss: 3.3177 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.9179 - reconstruction_loss: 4.5706 - kl_loss: 3.3139 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.8709 - reconstruction_loss: 4.5183 - kl_loss: 3.3381 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.8825 - reconstruction_loss: 4.5230 - kl_loss: 3.3296 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.8419 - reconstruction_loss: 4.4921 - kl_loss: 3.3458 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.8424 - reconstruction_loss: 4.4650 - kl_loss: 3.3505 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.8410 - reconstruction_loss: 4.4609 - kl_loss: 3.3664 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.8070 - reconstruction_loss: 4.4219 - kl_loss: 3.3781 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.8026 - reconstruction_loss: 4.4126 - kl_loss: 3.3846 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.8370 - reconstruction_loss: 4.4153 - kl_loss: 3.3941 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.7740 - reconstruction_loss: 4.3730 - kl_loss: 3.4079 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.7547 - reconstruction_loss: 4.3142 - kl_loss: 3.4178 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.7530 - reconstruction_loss: 4.3189 - kl_loss: 3.4241 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.7339 - reconstruction_loss: 4.2997 - kl_loss: 3.4350 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 7.8044 - reconstruction_loss: 4.2959 - kl_loss: 3.4530\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.8034 - reconstruction_loss: 4.2798 - kl_loss: 3.4530 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.6747 - reconstruction_loss: 4.1479 - kl_loss: 3.4797 - lr: 5.0000e-05\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.6120 - reconstruction_loss: 4.1063 - kl_loss: 3.4909 - lr: 5.0000e-05\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.6127 - reconstruction_loss: 4.0998 - kl_loss: 3.5132 - lr: 5.0000e-05\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.5767 - reconstruction_loss: 4.0616 - kl_loss: 3.5120 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.5727 - reconstruction_loss: 4.0279 - kl_loss: 3.5305 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.5578 - reconstruction_loss: 4.0059 - kl_loss: 3.5330 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.5435 - reconstruction_loss: 3.9860 - kl_loss: 3.5441 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.5402 - reconstruction_loss: 3.9727 - kl_loss: 3.5539 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.5281 - reconstruction_loss: 3.9603 - kl_loss: 3.5590 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.5368 - reconstruction_loss: 3.9504 - kl_loss: 3.5619 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.5049 - reconstruction_loss: 3.9287 - kl_loss: 3.5700 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.4838 - reconstruction_loss: 3.8868 - kl_loss: 3.5832 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.4988 - reconstruction_loss: 3.8767 - kl_loss: 3.6033 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.5183 - reconstruction_loss: 3.8785 - kl_loss: 3.6097 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4806 - reconstruction_loss: 3.8475 - kl_loss: 3.6089 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4472 - reconstruction_loss: 3.8001 - kl_loss: 3.6302 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4696 - reconstruction_loss: 3.7938 - kl_loss: 3.6420 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4180 - reconstruction_loss: 3.7569 - kl_loss: 3.6525 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4285 - reconstruction_loss: 3.7396 - kl_loss: 3.6658 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4160 - reconstruction_loss: 3.7374 - kl_loss: 3.6692 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.4089 - reconstruction_loss: 3.7162 - kl_loss: 3.6752 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.3721 - reconstruction_loss: 3.6896 - kl_loss: 3.6804 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.4031 - reconstruction_loss: 3.6908 - kl_loss: 3.6839 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.3663 - reconstruction_loss: 3.6753 - kl_loss: 3.6818 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.3914 - reconstruction_loss: 3.6659 - kl_loss: 3.6979 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.3581 - reconstruction_loss: 3.6571 - kl_loss: 3.7064 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.3855 - reconstruction_loss: 3.6333 - kl_loss: 3.7159 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.3988 - reconstruction_loss: 3.6393 - kl_loss: 3.7182 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.3627 - reconstruction_loss: 3.6174 - kl_loss: 3.7274 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.3395 - reconstruction_loss: 3.5911 - kl_loss: 3.7216 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.3233 - reconstruction_loss: 3.5816 - kl_loss: 3.7311 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.3499 - reconstruction_loss: 3.6039 - kl_loss: 3.7268 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 7.3239 - reconstruction_loss: 3.5842 - kl_loss: 3.7454\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.3238 - reconstruction_loss: 3.5716 - kl_loss: 3.7458 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.2710 - reconstruction_loss: 3.5215 - kl_loss: 3.7446 - lr: 2.5000e-05\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.2656 - reconstruction_loss: 3.5008 - kl_loss: 3.7615 - lr: 2.5000e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.2866 - reconstruction_loss: 3.5102 - kl_loss: 3.7526 - lr: 2.5000e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.2711 - reconstruction_loss: 3.5097 - kl_loss: 3.7561 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.2704 - reconstruction_loss: 3.4921 - kl_loss: 3.7523 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.2364 - reconstruction_loss: 3.4872 - kl_loss: 3.7531 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.2646 - reconstruction_loss: 3.4942 - kl_loss: 3.7516 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.2410 - reconstruction_loss: 3.4760 - kl_loss: 3.7578 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.2550 - reconstruction_loss: 3.4775 - kl_loss: 3.7625 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.2533 - reconstruction_loss: 3.4877 - kl_loss: 3.7540 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.2317 - reconstruction_loss: 3.4708 - kl_loss: 3.7596 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.2255 - reconstruction_loss: 3.4673 - kl_loss: 3.7580 - lr: 2.5000e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.2362 - reconstruction_loss: 3.4784 - kl_loss: 3.7571 - lr: 2.5000e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.2731 - reconstruction_loss: 3.4853 - kl_loss: 3.7614 - lr: 2.5000e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.2172 - reconstruction_loss: 3.4581 - kl_loss: 3.7589 - lr: 2.5000e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.2226 - reconstruction_loss: 3.4482 - kl_loss: 3.7622 - lr: 2.5000e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.2295 - reconstruction_loss: 3.4441 - kl_loss: 3.7686 - lr: 2.5000e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.2045 - reconstruction_loss: 3.4358 - kl_loss: 3.7768 - lr: 2.5000e-05\n",
      "Epoch 84/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 7.2290 - reconstruction_loss: 3.4457 - kl_loss: 3.7780\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.2288 - reconstruction_loss: 3.4340 - kl_loss: 3.7778 - lr: 2.5000e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.2381 - reconstruction_loss: 3.4338 - kl_loss: 3.7771 - lr: 1.2500e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1945 - reconstruction_loss: 3.4125 - kl_loss: 3.7804 - lr: 1.2500e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.2104 - reconstruction_loss: 3.4040 - kl_loss: 3.7840 - lr: 1.2500e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.2013 - reconstruction_loss: 3.3964 - kl_loss: 3.7824 - lr: 1.2500e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1977 - reconstruction_loss: 3.3963 - kl_loss: 3.7854 - lr: 1.2500e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1988 - reconstruction_loss: 3.3965 - kl_loss: 3.7870 - lr: 1.2500e-05\n",
      "Epoch 91/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 7.1868 - reconstruction_loss: 3.4170 - kl_loss: 3.7762\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1868 - reconstruction_loss: 3.4042 - kl_loss: 3.7771 - lr: 1.2500e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1860 - reconstruction_loss: 3.3927 - kl_loss: 3.7839 - lr: 6.2500e-06\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1845 - reconstruction_loss: 3.3909 - kl_loss: 3.7809 - lr: 6.2500e-06\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1690 - reconstruction_loss: 3.3849 - kl_loss: 3.7784 - lr: 6.2500e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1657 - reconstruction_loss: 3.3875 - kl_loss: 3.7725 - lr: 6.2500e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1866 - reconstruction_loss: 3.3928 - kl_loss: 3.7864 - lr: 6.2500e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1789 - reconstruction_loss: 3.3793 - kl_loss: 3.7803 - lr: 6.2500e-06\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1670 - reconstruction_loss: 3.3770 - kl_loss: 3.7888 - lr: 6.2500e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1768 - reconstruction_loss: 3.3758 - kl_loss: 3.7807 - lr: 6.2500e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1838 - reconstruction_loss: 3.3738 - kl_loss: 3.7903 - lr: 6.2500e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1739 - reconstruction_loss: 3.3702 - kl_loss: 3.7828 - lr: 6.2500e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1602 - reconstruction_loss: 3.3790 - kl_loss: 3.7815 - lr: 6.2500e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1581 - reconstruction_loss: 3.3716 - kl_loss: 3.7865 - lr: 6.2500e-06\n",
      "Epoch 104/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 7.1800 - reconstruction_loss: 3.3890 - kl_loss: 3.7857\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1798 - reconstruction_loss: 3.3775 - kl_loss: 3.7857 - lr: 6.2500e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1519 - reconstruction_loss: 3.3670 - kl_loss: 3.7814 - lr: 3.1250e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1726 - reconstruction_loss: 3.3647 - kl_loss: 3.7848 - lr: 3.1250e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1760 - reconstruction_loss: 3.3647 - kl_loss: 3.7828 - lr: 3.1250e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1576 - reconstruction_loss: 3.3684 - kl_loss: 3.7885 - lr: 3.1250e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1592 - reconstruction_loss: 3.3561 - kl_loss: 3.7931 - lr: 3.1250e-06\n",
      "Epoch 110/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 7.1598 - reconstruction_loss: 3.3694 - kl_loss: 3.7950\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1597 - reconstruction_loss: 3.3581 - kl_loss: 3.7958 - lr: 3.1250e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1589 - reconstruction_loss: 3.3614 - kl_loss: 3.7927 - lr: 1.5625e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1560 - reconstruction_loss: 3.3586 - kl_loss: 3.7905 - lr: 1.5625e-06\n",
      "Epoch 113/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 7.1579 - reconstruction_loss: 3.3703 - kl_loss: 3.7908\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1578 - reconstruction_loss: 3.3586 - kl_loss: 3.7907 - lr: 1.5625e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1500 - reconstruction_loss: 3.3521 - kl_loss: 3.7932 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1459 - reconstruction_loss: 3.3534 - kl_loss: 3.7956 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1616 - reconstruction_loss: 3.3477 - kl_loss: 3.7961 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1576 - reconstruction_loss: 3.3528 - kl_loss: 3.7922 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1522 - reconstruction_loss: 3.3533 - kl_loss: 3.7939 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1452 - reconstruction_loss: 3.3463 - kl_loss: 3.7909 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1305 - reconstruction_loss: 3.3517 - kl_loss: 3.7880 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1409 - reconstruction_loss: 3.3482 - kl_loss: 3.7885 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1483 - reconstruction_loss: 3.3516 - kl_loss: 3.7901 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1481 - reconstruction_loss: 3.3464 - kl_loss: 3.7941 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1685 - reconstruction_loss: 3.3532 - kl_loss: 3.7935 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1698 - reconstruction_loss: 3.3528 - kl_loss: 3.7967 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1382 - reconstruction_loss: 3.3435 - kl_loss: 3.7954 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1458 - reconstruction_loss: 3.3452 - kl_loss: 3.7917 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1512 - reconstruction_loss: 3.3451 - kl_loss: 3.7911 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1389 - reconstruction_loss: 3.3537 - kl_loss: 3.7917 - lr: 1.0000e-06\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1485 - reconstruction_loss: 3.3432 - kl_loss: 3.7933 - lr: 1.0000e-06\n",
      "Epoch 131/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1415 - reconstruction_loss: 3.3482 - kl_loss: 3.7878 - lr: 1.0000e-06\n",
      "Epoch 132/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1444 - reconstruction_loss: 3.3485 - kl_loss: 3.7926 - lr: 1.0000e-06\n",
      "Epoch 133/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1503 - reconstruction_loss: 3.3434 - kl_loss: 3.7980 - lr: 1.0000e-06\n",
      "Epoch 134/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1533 - reconstruction_loss: 3.3483 - kl_loss: 3.7954 - lr: 1.0000e-06\n",
      "Epoch 135/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1484 - reconstruction_loss: 3.3462 - kl_loss: 3.7880 - lr: 1.0000e-06\n",
      "Epoch 136/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1430 - reconstruction_loss: 3.3432 - kl_loss: 3.7910 - lr: 1.0000e-06\n",
      "Epoch 137/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1532 - reconstruction_loss: 3.3461 - kl_loss: 3.7946 - lr: 1.0000e-06\n",
      "Epoch 138/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1508 - reconstruction_loss: 3.3387 - kl_loss: 3.7978 - lr: 1.0000e-06\n",
      "Epoch 139/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1419 - reconstruction_loss: 3.3476 - kl_loss: 3.7916 - lr: 1.0000e-06\n",
      "Epoch 140/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1543 - reconstruction_loss: 3.3505 - kl_loss: 3.7936 - lr: 1.0000e-06\n",
      "Epoch 141/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1243 - reconstruction_loss: 3.3340 - kl_loss: 3.7887 - lr: 1.0000e-06\n",
      "Epoch 142/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.1478 - reconstruction_loss: 3.3491 - kl_loss: 3.7880 - lr: 1.0000e-06\n",
      "Epoch 143/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1220 - reconstruction_loss: 3.3405 - kl_loss: 3.7931 - lr: 1.0000e-06\n",
      "Epoch 144/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.1466 - reconstruction_loss: 3.3374 - kl_loss: 3.7929 - lr: 1.0000e-06\n",
      "Epoch 145/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1665 - reconstruction_loss: 3.3530 - kl_loss: 3.7894 - lr: 1.0000e-06\n",
      "Epoch 146/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.1469 - reconstruction_loss: 3.3491 - kl_loss: 3.7904 - lr: 1.0000e-06\n",
      "Epoch 147/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1424 - reconstruction_loss: 3.3477 - kl_loss: 3.7957 - lr: 1.0000e-06\n",
      "Epoch 148/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1374 - reconstruction_loss: 3.3385 - kl_loss: 3.7937 - lr: 1.0000e-06\n",
      "Epoch 149/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1578 - reconstruction_loss: 3.3466 - kl_loss: 3.7939 - lr: 1.0000e-06\n",
      "Epoch 150/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1407 - reconstruction_loss: 3.3332 - kl_loss: 3.7974 - lr: 1.0000e-06\n",
      "Epoch 151/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.1411 - reconstruction_loss: 3.3425 - kl_loss: 3.7918 - lr: 1.0000e-06\n",
      "Epoch 151: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 08:44:11,590]\u001b[0m Trial 4 finished with value: 0.006558732777398353 and parameters: {'encoder_layers': 7, 'encoder_units_l1': 211, 'encoder_units_l2': 400, 'encoder_units_l3': 603, 'encoder_units_l4': 22, 'encoder_units_l5': 200, 'encoder_units_l6': 35, 'encoder_units_l7': 462, 'decoder_layers': 6, 'decoder_units_l1': 837, 'decoder_units_l2': 66, 'decoder_units_l3': 16, 'decoder_units_l4': 734, 'decoder_units_l5': 194, 'decoder_units_l6': 59, 'beta': 1, 'lr': 0.0001, 'batch_size': 512}. Best is trial 1 with value: 0.004936496311177702.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 3.4928 - reconstruction_loss: 2.8601 - kl_loss: 1.9434 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9999 - reconstruction_loss: 2.5339 - kl_loss: 2.1195 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9045 - reconstruction_loss: 2.4494 - kl_loss: 2.2194 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.9575 - reconstruction_loss: 2.4943 - kl_loss: 2.3618 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.1747 - reconstruction_loss: 2.7438 - kl_loss: 2.1695 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "779/782 [============================>.] - ETA: 0s - loss: 3.0497 - reconstruction_loss: 2.5861 - kl_loss: 2.1530\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 3.0495 - reconstruction_loss: 2.5830 - kl_loss: 2.1535 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.8889 - reconstruction_loss: 2.4271 - kl_loss: 2.2170 - lr: 0.0050\n",
      "Epoch 8/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.8744 - reconstruction_loss: 2.4243 - kl_loss: 2.2634 - lr: 0.0050\n",
      "Epoch 9/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.8986 - reconstruction_loss: 2.4334 - kl_loss: 2.2941 - lr: 0.0050\n",
      "Epoch 10/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.8429 - reconstruction_loss: 2.3809 - kl_loss: 2.2474 - lr: 0.0050\n",
      "Epoch 11/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.8129 - reconstruction_loss: 2.3318 - kl_loss: 2.2809 - lr: 0.0050\n",
      "Epoch 12/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7914 - reconstruction_loss: 2.3441 - kl_loss: 2.2969 - lr: 0.0050\n",
      "Epoch 13/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.8111 - reconstruction_loss: 2.3385 - kl_loss: 2.3018 - lr: 0.0050\n",
      "Epoch 14/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7678 - reconstruction_loss: 2.2985 - kl_loss: 2.3231 - lr: 0.0050\n",
      "Epoch 15/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7765 - reconstruction_loss: 2.3204 - kl_loss: 2.3622 - lr: 0.0050\n",
      "Epoch 16/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7444 - reconstruction_loss: 2.2541 - kl_loss: 2.4337 - lr: 0.0050\n",
      "Epoch 17/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7466 - reconstruction_loss: 2.2391 - kl_loss: 2.4892 - lr: 0.0050\n",
      "Epoch 18/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.8208 - reconstruction_loss: 2.3312 - kl_loss: 2.5982 - lr: 0.0050\n",
      "Epoch 19/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7924 - reconstruction_loss: 2.3061 - kl_loss: 2.4758 - lr: 0.0050\n",
      "Epoch 20/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7465 - reconstruction_loss: 2.2408 - kl_loss: 2.4314 - lr: 0.0050\n",
      "Epoch 21/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6980 - reconstruction_loss: 2.2254 - kl_loss: 2.4216 - lr: 0.0050\n",
      "Epoch 22/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7002 - reconstruction_loss: 2.2612 - kl_loss: 2.3957 - lr: 0.0050\n",
      "Epoch 23/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7159 - reconstruction_loss: 2.2093 - kl_loss: 2.4714 - lr: 0.0050\n",
      "Epoch 24/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6701 - reconstruction_loss: 2.1796 - kl_loss: 2.4735 - lr: 0.0050\n",
      "Epoch 25/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7763 - reconstruction_loss: 2.3525 - kl_loss: 2.5212 - lr: 0.0050\n",
      "Epoch 26/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.7719 - reconstruction_loss: 2.2521 - kl_loss: 2.4820 - lr: 0.0050\n",
      "Epoch 27/300\n",
      "772/782 [============================>.] - ETA: 0s - loss: 2.6959 - reconstruction_loss: 2.1645 - kl_loss: 2.5971\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6957 - reconstruction_loss: 2.1622 - kl_loss: 2.5967 - lr: 0.0050\n",
      "Epoch 28/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6362 - reconstruction_loss: 2.1030 - kl_loss: 2.5980 - lr: 0.0025\n",
      "Epoch 29/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6497 - reconstruction_loss: 2.1217 - kl_loss: 2.6149 - lr: 0.0025\n",
      "Epoch 30/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6700 - reconstruction_loss: 2.1269 - kl_loss: 2.6055 - lr: 0.0025\n",
      "Epoch 31/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6253 - reconstruction_loss: 2.0881 - kl_loss: 2.6470 - lr: 0.0025\n",
      "Epoch 32/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6153 - reconstruction_loss: 2.0693 - kl_loss: 2.6811 - lr: 0.0025\n",
      "Epoch 33/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6383 - reconstruction_loss: 2.0878 - kl_loss: 2.7548 - lr: 0.0025\n",
      "Epoch 34/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6644 - reconstruction_loss: 2.1351 - kl_loss: 2.7919 - lr: 0.0025\n",
      "Epoch 35/300\n",
      "772/782 [============================>.] - ETA: 0s - loss: 2.6663 - reconstruction_loss: 2.1105 - kl_loss: 2.8438\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6665 - reconstruction_loss: 2.1080 - kl_loss: 2.8440 - lr: 0.0025\n",
      "Epoch 36/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6340 - reconstruction_loss: 2.0470 - kl_loss: 2.8368 - lr: 0.0012\n",
      "Epoch 37/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5703 - reconstruction_loss: 1.9952 - kl_loss: 2.8373 - lr: 0.0012\n",
      "Epoch 38/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5589 - reconstruction_loss: 1.9908 - kl_loss: 2.8401 - lr: 0.0012\n",
      "Epoch 39/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5525 - reconstruction_loss: 1.9713 - kl_loss: 2.8730 - lr: 0.0012\n",
      "Epoch 40/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5600 - reconstruction_loss: 1.9733 - kl_loss: 2.8827 - lr: 0.0012\n",
      "Epoch 41/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5583 - reconstruction_loss: 1.9675 - kl_loss: 2.8506 - lr: 0.0012\n",
      "Epoch 42/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5173 - reconstruction_loss: 1.9341 - kl_loss: 2.8708 - lr: 0.0012\n",
      "Epoch 43/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5043 - reconstruction_loss: 1.9312 - kl_loss: 2.8827 - lr: 0.0012\n",
      "Epoch 44/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5144 - reconstruction_loss: 1.9256 - kl_loss: 2.9257 - lr: 0.0012\n",
      "Epoch 45/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4976 - reconstruction_loss: 1.9186 - kl_loss: 2.9365 - lr: 0.0012\n",
      "Epoch 46/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5045 - reconstruction_loss: 1.9114 - kl_loss: 2.9626 - lr: 0.0012\n",
      "Epoch 47/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5129 - reconstruction_loss: 1.9239 - kl_loss: 2.9610 - lr: 0.0012\n",
      "Epoch 48/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4986 - reconstruction_loss: 1.9044 - kl_loss: 2.9719 - lr: 0.0012\n",
      "Epoch 49/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5188 - reconstruction_loss: 1.9050 - kl_loss: 2.9922 - lr: 0.0012\n",
      "Epoch 50/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4790 - reconstruction_loss: 1.8785 - kl_loss: 3.0113 - lr: 0.0012\n",
      "Epoch 51/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4870 - reconstruction_loss: 1.8762 - kl_loss: 3.0044 - lr: 0.0012\n",
      "Epoch 52/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4536 - reconstruction_loss: 1.8583 - kl_loss: 2.9971 - lr: 0.0012\n",
      "Epoch 53/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4723 - reconstruction_loss: 1.8513 - kl_loss: 2.9979 - lr: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4266 - reconstruction_loss: 1.8207 - kl_loss: 3.0216 - lr: 0.0012\n",
      "Epoch 55/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4308 - reconstruction_loss: 1.8118 - kl_loss: 3.0298 - lr: 0.0012\n",
      "Epoch 56/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4238 - reconstruction_loss: 1.8097 - kl_loss: 3.0645 - lr: 0.0012\n",
      "Epoch 57/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4028 - reconstruction_loss: 1.7789 - kl_loss: 3.0781 - lr: 0.0012\n",
      "Epoch 58/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3481 - reconstruction_loss: 1.7420 - kl_loss: 3.1059 - lr: 0.0012\n",
      "Epoch 59/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3461 - reconstruction_loss: 1.7278 - kl_loss: 3.1248 - lr: 0.0012\n",
      "Epoch 60/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3768 - reconstruction_loss: 1.7717 - kl_loss: 3.1617 - lr: 0.0012\n",
      "Epoch 61/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4150 - reconstruction_loss: 1.8065 - kl_loss: 3.1326 - lr: 0.0012\n",
      "Epoch 62/300\n",
      "771/782 [============================>.] - ETA: 0s - loss: 2.4204 - reconstruction_loss: 1.7602 - kl_loss: 3.1673\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.4200 - reconstruction_loss: 1.7581 - kl_loss: 3.1672 - lr: 0.0012\n",
      "Epoch 63/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3513 - reconstruction_loss: 1.6960 - kl_loss: 3.1758 - lr: 6.2500e-04\n",
      "Epoch 64/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3282 - reconstruction_loss: 1.6884 - kl_loss: 3.1810 - lr: 6.2500e-04\n",
      "Epoch 65/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3090 - reconstruction_loss: 1.6744 - kl_loss: 3.1714 - lr: 6.2500e-04\n",
      "Epoch 66/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.3095 - reconstruction_loss: 1.6662 - kl_loss: 3.1716 - lr: 6.2500e-04\n",
      "Epoch 67/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2844 - reconstruction_loss: 1.6571 - kl_loss: 3.1723 - lr: 6.2500e-04\n",
      "Epoch 68/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2983 - reconstruction_loss: 1.6638 - kl_loss: 3.1820 - lr: 6.2500e-04\n",
      "Epoch 69/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2933 - reconstruction_loss: 1.6445 - kl_loss: 3.1878 - lr: 6.2500e-04\n",
      "Epoch 70/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2786 - reconstruction_loss: 1.6457 - kl_loss: 3.1823 - lr: 6.2500e-04\n",
      "Epoch 71/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2906 - reconstruction_loss: 1.6569 - kl_loss: 3.1815 - lr: 6.2500e-04\n",
      "Epoch 72/300\n",
      "772/782 [============================>.] - ETA: 0s - loss: 2.2957 - reconstruction_loss: 1.6538 - kl_loss: 3.1772\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2956 - reconstruction_loss: 1.6520 - kl_loss: 3.1773 - lr: 6.2500e-04\n",
      "Epoch 73/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2694 - reconstruction_loss: 1.6304 - kl_loss: 3.2037 - lr: 3.1250e-04\n",
      "Epoch 74/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2615 - reconstruction_loss: 1.6188 - kl_loss: 3.2029 - lr: 3.1250e-04\n",
      "Epoch 75/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2505 - reconstruction_loss: 1.6066 - kl_loss: 3.2015 - lr: 3.1250e-04\n",
      "Epoch 76/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2426 - reconstruction_loss: 1.5986 - kl_loss: 3.2123 - lr: 3.1250e-04\n",
      "Epoch 77/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2313 - reconstruction_loss: 1.5935 - kl_loss: 3.2159 - lr: 3.1250e-04\n",
      "Epoch 78/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2258 - reconstruction_loss: 1.5887 - kl_loss: 3.2140 - lr: 3.1250e-04\n",
      "Epoch 79/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2353 - reconstruction_loss: 1.5861 - kl_loss: 3.2222 - lr: 3.1250e-04\n",
      "Epoch 80/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2266 - reconstruction_loss: 1.5841 - kl_loss: 3.2286 - lr: 3.1250e-04\n",
      "Epoch 81/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2273 - reconstruction_loss: 1.5752 - kl_loss: 3.2347 - lr: 3.1250e-04\n",
      "Epoch 82/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2340 - reconstruction_loss: 1.5802 - kl_loss: 3.2335 - lr: 3.1250e-04\n",
      "Epoch 83/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2243 - reconstruction_loss: 1.5801 - kl_loss: 3.2278 - lr: 3.1250e-04\n",
      "Epoch 84/300\n",
      "772/782 [============================>.] - ETA: 0s - loss: 2.2317 - reconstruction_loss: 1.5869 - kl_loss: 3.2392\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2317 - reconstruction_loss: 1.5859 - kl_loss: 3.2391 - lr: 3.1250e-04\n",
      "Epoch 85/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2369 - reconstruction_loss: 1.5826 - kl_loss: 3.2406 - lr: 1.5625e-04\n",
      "Epoch 86/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2360 - reconstruction_loss: 1.5838 - kl_loss: 3.2402 - lr: 1.5625e-04\n",
      "Epoch 87/300\n",
      "773/782 [============================>.] - ETA: 0s - loss: 2.2390 - reconstruction_loss: 1.5910 - kl_loss: 3.2389\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2390 - reconstruction_loss: 1.5888 - kl_loss: 3.2390 - lr: 1.5625e-04\n",
      "Epoch 88/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2307 - reconstruction_loss: 1.5801 - kl_loss: 3.2402 - lr: 7.8125e-05\n",
      "Epoch 89/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2294 - reconstruction_loss: 1.5743 - kl_loss: 3.2441 - lr: 7.8125e-05\n",
      "Epoch 90/300\n",
      "765/782 [============================>.] - ETA: 0s - loss: 2.2273 - reconstruction_loss: 1.5803 - kl_loss: 3.2465\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2273 - reconstruction_loss: 1.5785 - kl_loss: 3.2469 - lr: 7.8125e-05\n",
      "Epoch 91/300\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.2250 - reconstruction_loss: 1.5787 - kl_loss: 3.2504 - lr: 3.9062e-05\n",
      "Epoch 91: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 08:47:36,920]\u001b[0m Trial 5 finished with value: 0.0123174558060212 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 31, 'encoder_units_l2': 62, 'encoder_units_l3': 34, 'encoder_units_l4': 38, 'decoder_layers': 5, 'decoder_units_l1': 23, 'decoder_units_l2': 115, 'decoder_units_l3': 174, 'decoder_units_l4': 46, 'decoder_units_l5': 19, 'beta': 0.2, 'lr': 0.01, 'batch_size': 128}. Best is trial 1 with value: 0.004936496311177702.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 7.6281 - reconstruction_loss: 5.0677 - kl_loss: 1.7327 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 6.2637 - reconstruction_loss: 4.0632 - kl_loss: 2.0964 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.8373 - reconstruction_loss: 3.3750 - kl_loss: 2.3730 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.5985 - reconstruction_loss: 3.1225 - kl_loss: 2.4435 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 5.5365 - reconstruction_loss: 3.0576 - kl_loss: 2.4629 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 5.4968 - reconstruction_loss: 3.0091 - kl_loss: 2.4791 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.4975 - reconstruction_loss: 2.9868 - kl_loss: 2.4881 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.4648 - reconstruction_loss: 2.9452 - kl_loss: 2.5108 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.4515 - reconstruction_loss: 2.9322 - kl_loss: 2.5064 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.4369 - reconstruction_loss: 2.9127 - kl_loss: 2.5133 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.4031 - reconstruction_loss: 2.8864 - kl_loss: 2.5225 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.4030 - reconstruction_loss: 2.8775 - kl_loss: 2.5366 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 5.3865 - reconstruction_loss: 2.8580 - kl_loss: 2.5309 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 5.4053 - reconstruction_loss: 2.8374 - kl_loss: 2.5477 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3823 - reconstruction_loss: 2.8396 - kl_loss: 2.5469 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3799 - reconstruction_loss: 2.8222 - kl_loss: 2.5556 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3720 - reconstruction_loss: 2.8151 - kl_loss: 2.5718 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3575 - reconstruction_loss: 2.7798 - kl_loss: 2.5774 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3726 - reconstruction_loss: 2.7880 - kl_loss: 2.5714 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3810 - reconstruction_loss: 2.8235 - kl_loss: 2.5690 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 5.3718 - reconstruction_loss: 2.8066 - kl_loss: 2.5710\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3718 - reconstruction_loss: 2.8036 - kl_loss: 2.5711 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3541 - reconstruction_loss: 2.7300 - kl_loss: 2.6032 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3210 - reconstruction_loss: 2.7055 - kl_loss: 2.6063 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3178 - reconstruction_loss: 2.6960 - kl_loss: 2.6184 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3205 - reconstruction_loss: 2.7084 - kl_loss: 2.6103 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2976 - reconstruction_loss: 2.6820 - kl_loss: 2.6207 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3046 - reconstruction_loss: 2.6762 - kl_loss: 2.6291 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2958 - reconstruction_loss: 2.6730 - kl_loss: 2.6288 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2969 - reconstruction_loss: 2.6669 - kl_loss: 2.6415 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2998 - reconstruction_loss: 2.6738 - kl_loss: 2.6313 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2890 - reconstruction_loss: 2.6503 - kl_loss: 2.6423 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3055 - reconstruction_loss: 2.6536 - kl_loss: 2.6481 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2846 - reconstruction_loss: 2.6304 - kl_loss: 2.6612 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2825 - reconstruction_loss: 2.6221 - kl_loss: 2.6657 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3047 - reconstruction_loss: 2.6306 - kl_loss: 2.6632 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2895 - reconstruction_loss: 2.6282 - kl_loss: 2.6639 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2902 - reconstruction_loss: 2.6053 - kl_loss: 2.6816 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2901 - reconstruction_loss: 2.5979 - kl_loss: 2.6788 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2813 - reconstruction_loss: 2.5979 - kl_loss: 2.6806 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2777 - reconstruction_loss: 2.5974 - kl_loss: 2.6858 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 5.2834 - reconstruction_loss: 2.5931 - kl_loss: 2.6891\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2833 - reconstruction_loss: 2.5905 - kl_loss: 2.6883 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2834 - reconstruction_loss: 2.5553 - kl_loss: 2.7113 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2614 - reconstruction_loss: 2.5504 - kl_loss: 2.7132 - lr: 2.5000e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2793 - reconstruction_loss: 2.5608 - kl_loss: 2.7076 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2529 - reconstruction_loss: 2.5356 - kl_loss: 2.7194 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2577 - reconstruction_loss: 2.5144 - kl_loss: 2.7281 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2406 - reconstruction_loss: 2.5116 - kl_loss: 2.7389 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2529 - reconstruction_loss: 2.5157 - kl_loss: 2.7352 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 5.2554 - reconstruction_loss: 2.5239 - kl_loss: 2.7367\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2554 - reconstruction_loss: 2.5224 - kl_loss: 2.7369 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2455 - reconstruction_loss: 2.5039 - kl_loss: 2.7365 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2373 - reconstruction_loss: 2.5034 - kl_loss: 2.7322 - lr: 1.2500e-04\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2450 - reconstruction_loss: 2.4979 - kl_loss: 2.7461 - lr: 1.2500e-04\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2407 - reconstruction_loss: 2.4845 - kl_loss: 2.7479 - lr: 1.2500e-04\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2417 - reconstruction_loss: 2.4934 - kl_loss: 2.7474 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 5.2379 - reconstruction_loss: 2.4915 - kl_loss: 2.7449 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2324 - reconstruction_loss: 2.4784 - kl_loss: 2.7539 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2352 - reconstruction_loss: 2.4719 - kl_loss: 2.7641 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2427 - reconstruction_loss: 2.4844 - kl_loss: 2.7564 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 5.2459 - reconstruction_loss: 2.4741 - kl_loss: 2.7696\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2459 - reconstruction_loss: 2.4714 - kl_loss: 2.7700 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2409 - reconstruction_loss: 2.4678 - kl_loss: 2.7630 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2182 - reconstruction_loss: 2.4642 - kl_loss: 2.7627 - lr: 6.2500e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2298 - reconstruction_loss: 2.4559 - kl_loss: 2.7789 - lr: 6.2500e-05\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2326 - reconstruction_loss: 2.4613 - kl_loss: 2.7670 - lr: 6.2500e-05\n",
      "Epoch 64/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 5.2359 - reconstruction_loss: 2.4627 - kl_loss: 2.7719\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2359 - reconstruction_loss: 2.4599 - kl_loss: 2.7723 - lr: 6.2500e-05\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2323 - reconstruction_loss: 2.4451 - kl_loss: 2.7801 - lr: 3.1250e-05\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2286 - reconstruction_loss: 2.4572 - kl_loss: 2.7724 - lr: 3.1250e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2306 - reconstruction_loss: 2.4455 - kl_loss: 2.7741 - lr: 3.1250e-05\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2383 - reconstruction_loss: 2.4528 - kl_loss: 2.7721 - lr: 3.1250e-05\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2153 - reconstruction_loss: 2.4444 - kl_loss: 2.7722 - lr: 3.1250e-05\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2199 - reconstruction_loss: 2.4455 - kl_loss: 2.7701 - lr: 3.1250e-05\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2161 - reconstruction_loss: 2.4408 - kl_loss: 2.7783 - lr: 3.1250e-05\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2085 - reconstruction_loss: 2.4445 - kl_loss: 2.7765 - lr: 3.1250e-05\n",
      "Epoch 73/300\n",
      "384/391 [============================>.] - ETA: 0s - loss: 5.2271 - reconstruction_loss: 2.4464 - kl_loss: 2.7803\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2271 - reconstruction_loss: 2.4443 - kl_loss: 2.7803 - lr: 3.1250e-05\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2361 - reconstruction_loss: 2.4439 - kl_loss: 2.7814 - lr: 1.5625e-05\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2302 - reconstruction_loss: 2.4416 - kl_loss: 2.7851 - lr: 1.5625e-05\n",
      "Epoch 76/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 5.2250 - reconstruction_loss: 2.4438 - kl_loss: 2.7818\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2250 - reconstruction_loss: 2.4412 - kl_loss: 2.7817 - lr: 1.5625e-05\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2345 - reconstruction_loss: 2.4393 - kl_loss: 2.7873 - lr: 7.8125e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2186 - reconstruction_loss: 2.4298 - kl_loss: 2.7832 - lr: 7.8125e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2233 - reconstruction_loss: 2.4449 - kl_loss: 2.7770 - lr: 7.8125e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2013 - reconstruction_loss: 2.4406 - kl_loss: 2.7718 - lr: 7.8125e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2080 - reconstruction_loss: 2.4389 - kl_loss: 2.7756 - lr: 7.8125e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2196 - reconstruction_loss: 2.4428 - kl_loss: 2.7753 - lr: 7.8125e-06\n",
      "Epoch 83/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 5.2244 - reconstruction_loss: 2.4423 - kl_loss: 2.7821\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2244 - reconstruction_loss: 2.4403 - kl_loss: 2.7821 - lr: 7.8125e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2356 - reconstruction_loss: 2.4393 - kl_loss: 2.7854 - lr: 3.9063e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2198 - reconstruction_loss: 2.4333 - kl_loss: 2.7834 - lr: 3.9063e-06\n",
      "Epoch 86/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 5.2179 - reconstruction_loss: 2.4359 - kl_loss: 2.7819\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2179 - reconstruction_loss: 2.4332 - kl_loss: 2.7820 - lr: 3.9063e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2257 - reconstruction_loss: 2.4375 - kl_loss: 2.7794 - lr: 1.9531e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2349 - reconstruction_loss: 2.4358 - kl_loss: 2.7826 - lr: 1.9531e-06\n",
      "Epoch 89/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 5.2231 - reconstruction_loss: 2.4364 - kl_loss: 2.7848\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2230 - reconstruction_loss: 2.4348 - kl_loss: 2.7852 - lr: 1.9531e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.2187 - reconstruction_loss: 2.4332 - kl_loss: 2.7815 - lr: 1.0000e-06\n",
      "Epoch 90: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 08:51:34,631]\u001b[0m Trial 6 finished with value: 0.009531441478586561 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 134, 'encoder_units_l2': 44, 'encoder_units_l3': 17, 'decoder_layers': 6, 'decoder_units_l1': 330, 'decoder_units_l2': 25, 'decoder_units_l3': 287, 'decoder_units_l4': 901, 'decoder_units_l5': 27, 'decoder_units_l6': 119, 'beta': 1, 'lr': 0.001, 'batch_size': 256}. Best is trial 1 with value: 0.004936496311177702.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.9984 - reconstruction_loss: 10.8762 - kl_loss: 0.0026 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8671 - reconstruction_loss: 10.8466 - kl_loss: 8.4906e-09 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8646 - reconstruction_loss: 10.8475 - kl_loss: 2.0230e-08 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8889 - reconstruction_loss: 10.8476 - kl_loss: 2.0466e-07 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 10.8883 - reconstruction_loss: 10.8578 - kl_loss: 1.7406e-07\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8880 - reconstruction_loss: 10.8467 - kl_loss: 1.7317e-07 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8596 - reconstruction_loss: 10.8451 - kl_loss: 5.0318e-11 - lr: 0.0050\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8387 - reconstruction_loss: 10.8448 - kl_loss: 1.2505e-11 - lr: 0.0050\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8529 - reconstruction_loss: 10.8447 - kl_loss: 5.9547e-12 - lr: 0.0050\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8686 - reconstruction_loss: 10.8456 - kl_loss: 1.8945e-07 - lr: 0.0050\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8747 - reconstruction_loss: 10.8444 - kl_loss: 1.6615e-07 - lr: 0.0050\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8246 - reconstruction_loss: 10.8442 - kl_loss: 3.3176e-07 - lr: 0.0050\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8662 - reconstruction_loss: 10.8452 - kl_loss: 5.1515e-07 - lr: 0.0050\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8591 - reconstruction_loss: 10.8446 - kl_loss: 5.3249e-05 - lr: 0.0050\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 10.8559 - reconstruction_loss: 10.8452 - kl_loss: 6.3847e-09\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8559 - reconstruction_loss: 10.8452 - kl_loss: 6.3847e-09 - lr: 0.0050\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8723 - reconstruction_loss: 10.8441 - kl_loss: 1.8645e-09 - lr: 0.0025\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8567 - reconstruction_loss: 10.8439 - kl_loss: 7.6292e-10 - lr: 0.0025\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8729 - reconstruction_loss: 10.8438 - kl_loss: 2.8660e-10 - lr: 0.0025\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8527 - reconstruction_loss: 10.8438 - kl_loss: 1.5601e-10 - lr: 0.0025\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8714 - reconstruction_loss: 10.8436 - kl_loss: 9.9742e-11 - lr: 0.0025\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8484 - reconstruction_loss: 10.8441 - kl_loss: 6.9075e-11 - lr: 0.0025\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8488 - reconstruction_loss: 10.8437 - kl_loss: 9.0512e-11 - lr: 0.0025\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.8343 - reconstruction_loss: 10.8430 - kl_loss: 5.2953e-09 - lr: 0.0025\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.8490 - reconstruction_loss: 10.8441 - kl_loss: 1.3493e-08 - lr: 0.0025\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.8891 - reconstruction_loss: 10.8438 - kl_loss: 2.9693e-08 - lr: 0.0025\n",
      "Epoch 25/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 10.8344 - reconstruction_loss: 10.8535 - kl_loss: 3.2601e-08\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.8346 - reconstruction_loss: 10.8438 - kl_loss: 3.3569e-08 - lr: 0.0025\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8620 - reconstruction_loss: 10.8435 - kl_loss: 2.3224e-09 - lr: 0.0012\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.8136 - reconstruction_loss: 10.8435 - kl_loss: 1.3994e-11 - lr: 0.0012\n",
      "Epoch 28/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 10.8346 - reconstruction_loss: 10.8540 - kl_loss: 1.1642e-11\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.8346 - reconstruction_loss: 10.8433 - kl_loss: 1.1612e-11 - lr: 0.0012\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.8692 - reconstruction_loss: 10.8431 - kl_loss: 8.9321e-12 - lr: 6.2500e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.8959 - reconstruction_loss: 10.8430 - kl_loss: 6.2525e-12 - lr: 6.2500e-04\n",
      "Epoch 31/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 10.8508 - reconstruction_loss: 10.8518 - kl_loss: 5.9854e-12\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8508 - reconstruction_loss: 10.8431 - kl_loss: 5.9547e-12 - lr: 6.2500e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8216 - reconstruction_loss: 10.8429 - kl_loss: 5.3593e-12 - lr: 3.1250e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8343 - reconstruction_loss: 10.8429 - kl_loss: 5.0615e-12 - lr: 3.1250e-04\n",
      "Epoch 34/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 10.8723 - reconstruction_loss: 10.8533 - kl_loss: 4.8006e-12\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8721 - reconstruction_loss: 10.8429 - kl_loss: 4.7638e-12 - lr: 3.1250e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8349 - reconstruction_loss: 10.8428 - kl_loss: 4.1683e-12 - lr: 1.5625e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8477 - reconstruction_loss: 10.8428 - kl_loss: 3.8706e-12 - lr: 1.5625e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8320 - reconstruction_loss: 10.8428 - kl_loss: 3.5728e-12 - lr: 1.5625e-04\n",
      "Epoch 38/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 10.8698 - reconstruction_loss: 10.8532 - kl_loss: 3.3090e-12\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8695 - reconstruction_loss: 10.8428 - kl_loss: 3.2751e-12 - lr: 1.5625e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8665 - reconstruction_loss: 10.8428 - kl_loss: 3.2751e-12 - lr: 7.8125e-05\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8368 - reconstruction_loss: 10.8428 - kl_loss: 3.2751e-12 - lr: 7.8125e-05\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 10.8955 - reconstruction_loss: 10.8428 - kl_loss: 2.6796e-12\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8954 - reconstruction_loss: 10.8428 - kl_loss: 2.6796e-12 - lr: 7.8125e-05\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8359 - reconstruction_loss: 10.8428 - kl_loss: 2.6796e-12 - lr: 3.9062e-05\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8438 - reconstruction_loss: 10.8428 - kl_loss: 2.6796e-12 - lr: 3.9062e-05\n",
      "Epoch 44/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 10.8470 - reconstruction_loss: 10.8551 - kl_loss: 1.8049e-12\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8470 - reconstruction_loss: 10.8428 - kl_loss: 2.6796e-12 - lr: 3.9062e-05\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8538 - reconstruction_loss: 10.8427 - kl_loss: 2.3819e-12 - lr: 1.9531e-05\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.8600 - reconstruction_loss: 10.8427 - kl_loss: 2.6796e-12 - lr: 1.9531e-05\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8512 - reconstruction_loss: 10.8427 - kl_loss: 2.3819e-12 - lr: 1.9531e-05\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 10.8672 - reconstruction_loss: 10.8427 - kl_loss: 2.3819e-12\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8671 - reconstruction_loss: 10.8427 - kl_loss: 2.3819e-12 - lr: 1.9531e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8747 - reconstruction_loss: 10.8427 - kl_loss: 2.6796e-12 - lr: 9.7656e-06\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8580 - reconstruction_loss: 10.8427 - kl_loss: 2.3819e-12 - lr: 9.7656e-06\n",
      "Epoch 51/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 10.8715 - reconstruction_loss: 10.8551 - kl_loss: 2.1166e-12\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8711 - reconstruction_loss: 10.8427 - kl_loss: 2.0842e-12 - lr: 9.7656e-06\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8632 - reconstruction_loss: 10.8427 - kl_loss: 2.0842e-12 - lr: 4.8828e-06\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8777 - reconstruction_loss: 10.8427 - kl_loss: 2.0842e-12 - lr: 4.8828e-06\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 10.8592 - reconstruction_loss: 10.8427 - kl_loss: 2.0842e-12\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8591 - reconstruction_loss: 10.8427 - kl_loss: 2.0842e-12 - lr: 4.8828e-06\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8427 - reconstruction_loss: 10.8427 - kl_loss: 2.0842e-12 - lr: 2.4414e-06\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8900 - reconstruction_loss: 10.8427 - kl_loss: 2.0842e-12 - lr: 2.4414e-06\n",
      "Epoch 57/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 10.8698 - reconstruction_loss: 10.8535 - kl_loss: 1.8143e-12\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8695 - reconstruction_loss: 10.8427 - kl_loss: 1.7864e-12 - lr: 2.4414e-06\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8238 - reconstruction_loss: 10.8427 - kl_loss: 2.0842e-12 - lr: 1.2207e-06\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8416 - reconstruction_loss: 10.8427 - kl_loss: 2.3819e-12 - lr: 1.2207e-06\n",
      "Epoch 60/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 10.8481 - reconstruction_loss: 10.8536 - kl_loss: 1.7910e-12\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8480 - reconstruction_loss: 10.8427 - kl_loss: 1.7864e-12 - lr: 1.2207e-06\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8568 - reconstruction_loss: 10.8427 - kl_loss: 2.0842e-12 - lr: 1.0000e-06\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8161 - reconstruction_loss: 10.8427 - kl_loss: 1.7864e-12 - lr: 1.0000e-06\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8978 - reconstruction_loss: 10.8427 - kl_loss: 2.0842e-12 - lr: 1.0000e-06\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8263 - reconstruction_loss: 10.8427 - kl_loss: 2.0842e-12 - lr: 1.0000e-06\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8521 - reconstruction_loss: 10.8427 - kl_loss: 1.7864e-12 - lr: 1.0000e-06\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8246 - reconstruction_loss: 10.8427 - kl_loss: 2.0842e-12 - lr: 1.0000e-06\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8628 - reconstruction_loss: 10.8427 - kl_loss: 1.7864e-12 - lr: 1.0000e-06\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8347 - reconstruction_loss: 10.8427 - kl_loss: 1.7864e-12 - lr: 1.0000e-06\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8500 - reconstruction_loss: 10.8427 - kl_loss: 1.7864e-12 - lr: 1.0000e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8316 - reconstruction_loss: 10.8427 - kl_loss: 2.0842e-12 - lr: 1.0000e-06\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8218 - reconstruction_loss: 10.8427 - kl_loss: 2.0842e-12 - lr: 1.0000e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8574 - reconstruction_loss: 10.8427 - kl_loss: 1.7864e-12 - lr: 1.0000e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8450 - reconstruction_loss: 10.8427 - kl_loss: 1.7864e-12 - lr: 1.0000e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8960 - reconstruction_loss: 10.8427 - kl_loss: 1.7864e-12 - lr: 1.0000e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.8228 - reconstruction_loss: 10.8427 - kl_loss: 1.7864e-12 - lr: 1.0000e-06\n",
      "Epoch 75: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 08:55:57,371]\u001b[0m Trial 7 finished with value: 0.04239506880183041 and parameters: {'encoder_layers': 7, 'encoder_units_l1': 205, 'encoder_units_l2': 163, 'encoder_units_l3': 375, 'encoder_units_l4': 61, 'encoder_units_l5': 416, 'encoder_units_l6': 668, 'encoder_units_l7': 38, 'decoder_layers': 6, 'decoder_units_l1': 65, 'decoder_units_l2': 32, 'decoder_units_l3': 19, 'decoder_units_l4': 100, 'decoder_units_l5': 17, 'decoder_units_l6': 65, 'beta': 1, 'lr': 0.01, 'batch_size': 256}. Best is trial 1 with value: 0.004936496311177702.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.8194 - reconstruction_loss: 2.7160 - kl_loss: 0.0027 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7112 - reconstruction_loss: 2.7144 - kl_loss: 4.5203e-08 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7137 - reconstruction_loss: 2.7145 - kl_loss: 9.1032e-08 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7187 - reconstruction_loss: 2.7143 - kl_loss: 2.0758e-06 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7183 - reconstruction_loss: 2.7143 - kl_loss: 0.0000e+00 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7207 - reconstruction_loss: 2.7145 - kl_loss: 5.7455e-08 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7114 - reconstruction_loss: 2.7142 - kl_loss: 1.0069e-07 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "1552/1563 [============================>.] - ETA: 0s - loss: 2.7218 - reconstruction_loss: 2.7151 - kl_loss: 1.1104e-07\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7217 - reconstruction_loss: 2.7144 - kl_loss: 1.1044e-07 - lr: 0.0100\n",
      "Epoch 9/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7236 - reconstruction_loss: 2.7133 - kl_loss: 1.6928e-09 - lr: 0.0050\n",
      "Epoch 10/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7154 - reconstruction_loss: 2.7136 - kl_loss: 2.2039e-08 - lr: 0.0050\n",
      "Epoch 11/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7121 - reconstruction_loss: 2.7134 - kl_loss: 2.1585e-08 - lr: 0.0050\n",
      "Epoch 12/300\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 2.7143 - reconstruction_loss: 2.7141 - kl_loss: 3.3410e-08\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7143 - reconstruction_loss: 2.7133 - kl_loss: 3.3494e-08 - lr: 0.0050\n",
      "Epoch 13/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7097 - reconstruction_loss: 2.7130 - kl_loss: 3.8135e-11 - lr: 0.0025\n",
      "Epoch 14/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7117 - reconstruction_loss: 2.7128 - kl_loss: 8.8473e-09 - lr: 0.0025\n",
      "Epoch 15/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7115 - reconstruction_loss: 2.7129 - kl_loss: 4.9385e-09 - lr: 0.0025\n",
      "Epoch 16/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7203 - reconstruction_loss: 2.7129 - kl_loss: 5.6439e-09 - lr: 0.0025\n",
      "Epoch 17/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 2.7205 - reconstruction_loss: 2.7139 - kl_loss: 4.9830e-09\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7205 - reconstruction_loss: 2.7130 - kl_loss: 4.9766e-09 - lr: 0.0025\n",
      "Epoch 18/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7095 - reconstruction_loss: 2.7127 - kl_loss: 4.0042e-10 - lr: 0.0012\n",
      "Epoch 19/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7119 - reconstruction_loss: 2.7127 - kl_loss: 1.1250e-09 - lr: 0.0012\n",
      "Epoch 20/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7230 - reconstruction_loss: 2.7127 - kl_loss: 7.8176e-10 - lr: 0.0012\n",
      "Epoch 21/300\n",
      "1554/1563 [============================>.] - ETA: 0s - loss: 2.7184 - reconstruction_loss: 2.7139 - kl_loss: 6.9040e-10\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7183 - reconstruction_loss: 2.7127 - kl_loss: 6.8643e-10 - lr: 0.0012\n",
      "Epoch 22/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7096 - reconstruction_loss: 2.7126 - kl_loss: 0.0000e+00 - lr: 6.2500e-04\n",
      "Epoch 23/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7134 - reconstruction_loss: 2.7126 - kl_loss: 2.4788e-10 - lr: 6.2500e-04\n",
      "Epoch 24/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7153 - reconstruction_loss: 2.7126 - kl_loss: 3.8135e-11 - lr: 6.2500e-04\n",
      "Epoch 25/300\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 2.7116 - reconstruction_loss: 2.7137 - kl_loss: 0.0000e+00\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7116 - reconstruction_loss: 2.7126 - kl_loss: 0.0000e+00 - lr: 6.2500e-04\n",
      "Epoch 26/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7107 - reconstruction_loss: 2.7125 - kl_loss: 0.0000e+00 - lr: 3.1250e-04\n",
      "Epoch 27/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7052 - reconstruction_loss: 2.7125 - kl_loss: 0.0000e+00 - lr: 3.1250e-04\n",
      "Epoch 28/300\n",
      "1546/1563 [============================>.] - ETA: 0s - loss: 2.7123 - reconstruction_loss: 2.7126 - kl_loss: 0.0000e+00\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7123 - reconstruction_loss: 2.7125 - kl_loss: 0.0000e+00 - lr: 3.1250e-04\n",
      "Epoch 29/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7163 - reconstruction_loss: 2.7125 - kl_loss: 0.0000e+00 - lr: 1.5625e-04\n",
      "Epoch 30/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7158 - reconstruction_loss: 2.7125 - kl_loss: 0.0000e+00 - lr: 1.5625e-04\n",
      "Epoch 31/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7131 - reconstruction_loss: 2.7125 - kl_loss: 0.0000e+00 - lr: 1.5625e-04\n",
      "Epoch 32/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7219 - reconstruction_loss: 2.7125 - kl_loss: 0.0000e+00 - lr: 1.5625e-04\n",
      "Epoch 33/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7187 - reconstruction_loss: 2.7125 - kl_loss: 0.0000e+00 - lr: 1.5625e-04\n",
      "Epoch 34/300\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 2.7090 - reconstruction_loss: 2.7134 - kl_loss: 0.0000e+00\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7090 - reconstruction_loss: 2.7125 - kl_loss: 0.0000e+00 - lr: 1.5625e-04\n",
      "Epoch 35/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7180 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 7.8125e-05\n",
      "Epoch 36/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7117 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 7.8125e-05\n",
      "Epoch 37/300\n",
      "1546/1563 [============================>.] - ETA: 0s - loss: 2.7172 - reconstruction_loss: 2.7144 - kl_loss: 0.0000e+00\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7172 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 7.8125e-05\n",
      "Epoch 38/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7128 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 3.9062e-05\n",
      "Epoch 39/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7092 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 3.9062e-05\n",
      "Epoch 40/300\n",
      "1550/1563 [============================>.] - ETA: 0s - loss: 2.7064 - reconstruction_loss: 2.7138 - kl_loss: 0.0000e+00\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7065 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 3.9062e-05\n",
      "Epoch 41/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7197 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 1.9531e-05\n",
      "Epoch 42/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7128 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 1.9531e-05\n",
      "Epoch 43/300\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 2.7097 - reconstruction_loss: 2.7131 - kl_loss: 0.0000e+00\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7097 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 1.9531e-05\n",
      "Epoch 44/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7167 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 9.7656e-06\n",
      "Epoch 45/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7103 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 9.7656e-06\n",
      "Epoch 46/300\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 2.7102 - reconstruction_loss: 2.7137 - kl_loss: 0.0000e+00\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7102 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 9.7656e-06\n",
      "Epoch 47/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7087 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 4.8828e-06\n",
      "Epoch 48/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7155 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 4.8828e-06\n",
      "Epoch 49/300\n",
      "1550/1563 [============================>.] - ETA: 0s - loss: 2.7082 - reconstruction_loss: 2.7128 - kl_loss: 0.0000e+00\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7083 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 4.8828e-06\n",
      "Epoch 50/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7110 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 2.4414e-06\n",
      "Epoch 51/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7103 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 2.4414e-06\n",
      "Epoch 52/300\n",
      "1554/1563 [============================>.] - ETA: 0s - loss: 2.7142 - reconstruction_loss: 2.7135 - kl_loss: 0.0000e+00\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7141 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 2.4414e-06\n",
      "Epoch 53/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7119 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 1.2207e-06\n",
      "Epoch 54/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7131 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 1.2207e-06\n",
      "Epoch 55/300\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 2.7116 - reconstruction_loss: 2.7132 - kl_loss: 0.0000e+00\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7116 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 1.2207e-06\n",
      "Epoch 56/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7108 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 1.0000e-06\n",
      "Epoch 57/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7150 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 1.0000e-06\n",
      "Epoch 58/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7171 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 1.0000e-06\n",
      "Epoch 59/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7052 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 1.0000e-06\n",
      "Epoch 60/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7170 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 1.0000e-06\n",
      "Epoch 61/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7134 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 1.0000e-06\n",
      "Epoch 62/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.7180 - reconstruction_loss: 2.7124 - kl_loss: 0.0000e+00 - lr: 1.0000e-06\n",
      "Epoch 62: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 09:00:56,525]\u001b[0m Trial 8 finished with value: 0.04239506866149612 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 564, 'encoder_units_l2': 110, 'encoder_units_l3': 23, 'encoder_units_l4': 973, 'decoder_layers': 5, 'decoder_units_l1': 256, 'decoder_units_l2': 44, 'decoder_units_l3': 134, 'decoder_units_l4': 75, 'decoder_units_l5': 33, 'beta': 5, 'lr': 0.01, 'batch_size': 64}. Best is trial 1 with value: 0.004936496311177702.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 4.1047 - reconstruction_loss: 3.2818 - kl_loss: 1.5006 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.2857 - reconstruction_loss: 2.8323 - kl_loss: 2.0770 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.1322 - reconstruction_loss: 2.8272 - kl_loss: 2.1166 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.2135 - reconstruction_loss: 2.6946 - kl_loss: 2.1619 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.3454 - reconstruction_loss: 2.8698 - kl_loss: 2.0260 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.2522 - reconstruction_loss: 2.8814 - kl_loss: 1.8760 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "774/782 [============================>.] - ETA: 0s - loss: 3.2700 - reconstruction_loss: 2.8494 - kl_loss: 2.0292\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.2699 - reconstruction_loss: 2.8482 - kl_loss: 2.0277 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.2226 - reconstruction_loss: 2.7752 - kl_loss: 1.9492 - lr: 0.0050\n",
      "Epoch 9/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.9489 - reconstruction_loss: 2.5835 - kl_loss: 2.1175 - lr: 0.0050\n",
      "Epoch 10/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.1705 - reconstruction_loss: 2.6134 - kl_loss: 2.0847 - lr: 0.0050\n",
      "Epoch 11/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.1898 - reconstruction_loss: 2.8644 - kl_loss: 2.1486 - lr: 0.0050\n",
      "Epoch 12/300\n",
      "781/782 [============================>.] - ETA: 0s - loss: 3.1800 - reconstruction_loss: 2.6990 - kl_loss: 2.0708\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.1798 - reconstruction_loss: 2.6964 - kl_loss: 2.0707 - lr: 0.0050\n",
      "Epoch 13/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.9986 - reconstruction_loss: 2.5311 - kl_loss: 2.1945 - lr: 0.0025\n",
      "Epoch 14/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.9178 - reconstruction_loss: 2.4592 - kl_loss: 2.2870 - lr: 0.0025\n",
      "Epoch 15/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.8751 - reconstruction_loss: 2.4022 - kl_loss: 2.3125 - lr: 0.0025\n",
      "Epoch 16/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.8477 - reconstruction_loss: 2.3764 - kl_loss: 2.4040 - lr: 0.0025\n",
      "Epoch 17/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.8437 - reconstruction_loss: 2.3058 - kl_loss: 2.4579 - lr: 0.0025\n",
      "Epoch 18/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7602 - reconstruction_loss: 2.2491 - kl_loss: 2.5261 - lr: 0.0025\n",
      "Epoch 19/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7956 - reconstruction_loss: 2.2727 - kl_loss: 2.5712 - lr: 0.0025\n",
      "Epoch 20/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7286 - reconstruction_loss: 2.2159 - kl_loss: 2.5415 - lr: 0.0025\n",
      "Epoch 21/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6902 - reconstruction_loss: 2.1995 - kl_loss: 2.5338 - lr: 0.0025\n",
      "Epoch 22/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7116 - reconstruction_loss: 2.1955 - kl_loss: 2.5649 - lr: 0.0025\n",
      "Epoch 23/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7242 - reconstruction_loss: 2.2317 - kl_loss: 2.7122 - lr: 0.0025\n",
      "Epoch 24/300\n",
      "775/782 [============================>.] - ETA: 0s - loss: 2.7728 - reconstruction_loss: 2.2552 - kl_loss: 2.6280\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7728 - reconstruction_loss: 2.2524 - kl_loss: 2.6275 - lr: 0.0025\n",
      "Epoch 25/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7308 - reconstruction_loss: 2.2772 - kl_loss: 2.6260 - lr: 0.0012\n",
      "Epoch 26/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.8356 - reconstruction_loss: 2.2907 - kl_loss: 2.5722 - lr: 0.0012\n",
      "Epoch 27/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6577 - reconstruction_loss: 2.1233 - kl_loss: 2.6571 - lr: 0.0012\n",
      "Epoch 28/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6630 - reconstruction_loss: 2.1361 - kl_loss: 2.6586 - lr: 0.0012\n",
      "Epoch 29/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6927 - reconstruction_loss: 2.1978 - kl_loss: 2.6623 - lr: 0.0012\n",
      "Epoch 30/300\n",
      "775/782 [============================>.] - ETA: 0s - loss: 2.8186 - reconstruction_loss: 2.2121 - kl_loss: 2.6703\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.8179 - reconstruction_loss: 2.2090 - kl_loss: 2.6708 - lr: 0.0012\n",
      "Epoch 31/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6516 - reconstruction_loss: 2.1071 - kl_loss: 2.6743 - lr: 6.2500e-04\n",
      "Epoch 32/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6301 - reconstruction_loss: 2.0880 - kl_loss: 2.7011 - lr: 6.2500e-04\n",
      "Epoch 33/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5982 - reconstruction_loss: 2.0603 - kl_loss: 2.7325 - lr: 6.2500e-04\n",
      "Epoch 34/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5919 - reconstruction_loss: 2.0507 - kl_loss: 2.7099 - lr: 6.2500e-04\n",
      "Epoch 35/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5956 - reconstruction_loss: 2.0436 - kl_loss: 2.7072 - lr: 6.2500e-04\n",
      "Epoch 36/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6022 - reconstruction_loss: 2.0490 - kl_loss: 2.7333 - lr: 6.2500e-04\n",
      "Epoch 37/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5919 - reconstruction_loss: 2.0418 - kl_loss: 2.7443 - lr: 6.2500e-04\n",
      "Epoch 38/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5808 - reconstruction_loss: 2.0233 - kl_loss: 2.7394 - lr: 6.2500e-04\n",
      "Epoch 39/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5804 - reconstruction_loss: 2.0308 - kl_loss: 2.7569 - lr: 6.2500e-04\n",
      "Epoch 40/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5759 - reconstruction_loss: 2.0532 - kl_loss: 2.7533 - lr: 6.2500e-04\n",
      "Epoch 41/300\n",
      "772/782 [============================>.] - ETA: 0s - loss: 2.6447 - reconstruction_loss: 2.0701 - kl_loss: 2.7044\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6442 - reconstruction_loss: 2.0680 - kl_loss: 2.7048 - lr: 6.2500e-04\n",
      "Epoch 42/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5620 - reconstruction_loss: 2.0283 - kl_loss: 2.6862 - lr: 3.1250e-04\n",
      "Epoch 43/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5668 - reconstruction_loss: 2.0428 - kl_loss: 2.6695 - lr: 3.1250e-04\n",
      "Epoch 44/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5666 - reconstruction_loss: 2.0287 - kl_loss: 2.6806 - lr: 3.1250e-04\n",
      "Epoch 45/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5569 - reconstruction_loss: 2.0253 - kl_loss: 2.6864 - lr: 3.1250e-04\n",
      "Epoch 46/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5763 - reconstruction_loss: 2.0356 - kl_loss: 2.6536 - lr: 3.1250e-04\n",
      "Epoch 47/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5612 - reconstruction_loss: 2.0263 - kl_loss: 2.6416 - lr: 3.1250e-04\n",
      "Epoch 48/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5356 - reconstruction_loss: 2.0153 - kl_loss: 2.6453 - lr: 3.1250e-04\n",
      "Epoch 49/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5825 - reconstruction_loss: 2.0708 - kl_loss: 2.6073 - lr: 3.1250e-04\n",
      "Epoch 50/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6459 - reconstruction_loss: 2.1197 - kl_loss: 2.6110 - lr: 3.1250e-04\n",
      "Epoch 51/300\n",
      "775/782 [============================>.] - ETA: 0s - loss: 2.6480 - reconstruction_loss: 2.1011 - kl_loss: 2.6213\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6478 - reconstruction_loss: 2.0996 - kl_loss: 2.6217 - lr: 3.1250e-04\n",
      "Epoch 52/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5899 - reconstruction_loss: 2.0557 - kl_loss: 2.6496 - lr: 1.5625e-04\n",
      "Epoch 53/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5856 - reconstruction_loss: 2.0581 - kl_loss: 2.6580 - lr: 1.5625e-04\n",
      "Epoch 54/300\n",
      "777/782 [============================>.] - ETA: 0s - loss: 2.5956 - reconstruction_loss: 2.0692 - kl_loss: 2.6584\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5957 - reconstruction_loss: 2.0680 - kl_loss: 2.6587 - lr: 1.5625e-04\n",
      "Epoch 55/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5966 - reconstruction_loss: 2.0581 - kl_loss: 2.6609 - lr: 7.8125e-05\n",
      "Epoch 56/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5752 - reconstruction_loss: 2.0445 - kl_loss: 2.6651 - lr: 7.8125e-05\n",
      "Epoch 57/300\n",
      "776/782 [============================>.] - ETA: 0s - loss: 2.5761 - reconstruction_loss: 2.0405 - kl_loss: 2.6580\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5760 - reconstruction_loss: 2.0384 - kl_loss: 2.6580 - lr: 7.8125e-05\n",
      "Epoch 58/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5656 - reconstruction_loss: 2.0283 - kl_loss: 2.6533 - lr: 3.9062e-05\n",
      "Epoch 58: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 09:04:43,214]\u001b[0m Trial 9 finished with value: 0.01588048918119539 and parameters: {'encoder_layers': 7, 'encoder_units_l1': 55, 'encoder_units_l2': 34, 'encoder_units_l3': 260, 'encoder_units_l4': 41, 'encoder_units_l5': 187, 'encoder_units_l6': 983, 'encoder_units_l7': 147, 'decoder_layers': 6, 'decoder_units_l1': 89, 'decoder_units_l2': 31, 'decoder_units_l3': 112, 'decoder_units_l4': 227, 'decoder_units_l5': 20, 'decoder_units_l6': 878, 'beta': 0.2, 'lr': 0.01, 'batch_size': 128}. Best is trial 1 with value: 0.004936496311177702.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.3886 - reconstruction_loss: 8.5671 - kl_loss: 0.3201 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0969 - reconstruction_loss: 8.3296 - kl_loss: 0.3518 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.1020 - reconstruction_loss: 8.3182 - kl_loss: 0.3536 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0826 - reconstruction_loss: 8.3111 - kl_loss: 0.3525 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0716 - reconstruction_loss: 8.2955 - kl_loss: 0.3533 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0360 - reconstruction_loss: 8.2860 - kl_loss: 0.3526 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0723 - reconstruction_loss: 8.2917 - kl_loss: 0.3517 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0872 - reconstruction_loss: 8.2980 - kl_loss: 0.3534 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 10.0598 - reconstruction_loss: 8.2995 - kl_loss: 0.3551\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0601 - reconstruction_loss: 8.2960 - kl_loss: 0.3551 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0241 - reconstruction_loss: 8.2699 - kl_loss: 0.3511 - lr: 5.0000e-04\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0668 - reconstruction_loss: 8.2725 - kl_loss: 0.3555 - lr: 5.0000e-04\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0733 - reconstruction_loss: 8.2840 - kl_loss: 0.3544 - lr: 5.0000e-04\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 10.0555 - reconstruction_loss: 8.2767 - kl_loss: 0.3568\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0555 - reconstruction_loss: 8.2767 - kl_loss: 0.3568 - lr: 5.0000e-04\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0512 - reconstruction_loss: 8.2732 - kl_loss: 0.3548 - lr: 2.5000e-04\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0133 - reconstruction_loss: 8.2752 - kl_loss: 0.3520 - lr: 2.5000e-04\n",
      "Epoch 16/300\n",
      "384/391 [============================>.] - ETA: 0s - loss: 10.0420 - reconstruction_loss: 8.2679 - kl_loss: 0.3565\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0421 - reconstruction_loss: 8.2619 - kl_loss: 0.3559 - lr: 2.5000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0688 - reconstruction_loss: 8.2672 - kl_loss: 0.3540 - lr: 1.2500e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0379 - reconstruction_loss: 8.2678 - kl_loss: 0.3563 - lr: 1.2500e-04\n",
      "Epoch 19/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 10.0703 - reconstruction_loss: 8.2751 - kl_loss: 0.3560\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0701 - reconstruction_loss: 8.2670 - kl_loss: 0.3560 - lr: 1.2500e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0346 - reconstruction_loss: 8.2586 - kl_loss: 0.3552 - lr: 6.2500e-05\n",
      "Epoch 20: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 09:05:37,332]\u001b[0m Trial 10 finished with value: 0.03240195078461887 and parameters: {'encoder_layers': 5, 'encoder_units_l1': 962, 'encoder_units_l2': 171, 'encoder_units_l3': 74, 'encoder_units_l4': 223, 'encoder_units_l5': 19, 'decoder_layers': 3, 'decoder_units_l1': 277, 'decoder_units_l2': 984, 'decoder_units_l3': 46, 'beta': 5, 'lr': 0.001, 'batch_size': 256}. Best is trial 1 with value: 0.004936496311177702.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.1679 - reconstruction_loss: 2.0245 - kl_loss: 2.7857 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9773 - reconstruction_loss: 1.2594 - kl_loss: 3.4747 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9056 - reconstruction_loss: 1.2149 - kl_loss: 3.5525 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9041 - reconstruction_loss: 1.1798 - kl_loss: 3.6165 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8922 - reconstruction_loss: 1.1449 - kl_loss: 3.6640 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8498 - reconstruction_loss: 1.0869 - kl_loss: 3.7810 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7850 - reconstruction_loss: 0.9903 - kl_loss: 3.9423 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8419 - reconstruction_loss: 0.9692 - kl_loss: 4.0192 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7424 - reconstruction_loss: 0.9817 - kl_loss: 4.0590 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8196 - reconstruction_loss: 1.0260 - kl_loss: 3.9544 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7317 - reconstruction_loss: 0.9154 - kl_loss: 4.0675 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8166 - reconstruction_loss: 0.9426 - kl_loss: 4.1012 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8035 - reconstruction_loss: 0.9543 - kl_loss: 4.2141 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7284 - reconstruction_loss: 0.8929 - kl_loss: 4.1051 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8357 - reconstruction_loss: 0.9159 - kl_loss: 4.2352 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6612 - reconstruction_loss: 0.8196 - kl_loss: 4.0648 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5888 - reconstruction_loss: 0.7792 - kl_loss: 4.0434 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5800 - reconstruction_loss: 0.7713 - kl_loss: 4.0388 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6769 - reconstruction_loss: 0.9785 - kl_loss: 4.0077 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8647 - reconstruction_loss: 1.0833 - kl_loss: 3.8432 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "777/782 [============================>.] - ETA: 0s - loss: 1.8442 - reconstruction_loss: 1.0849 - kl_loss: 3.8590\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8443 - reconstruction_loss: 1.0835 - kl_loss: 3.8593 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7156 - reconstruction_loss: 0.9397 - kl_loss: 3.8379 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6972 - reconstruction_loss: 0.9003 - kl_loss: 3.9291 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.6646 - reconstruction_loss: 0.8779 - kl_loss: 3.9575\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6646 - reconstruction_loss: 0.8771 - kl_loss: 3.9576 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6461 - reconstruction_loss: 0.8433 - kl_loss: 3.9831 - lr: 2.5000e-04\n",
      "Epoch 26/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6363 - reconstruction_loss: 0.8383 - kl_loss: 3.9883 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "776/782 [============================>.] - ETA: 0s - loss: 1.6263 - reconstruction_loss: 0.8290 - kl_loss: 3.9975\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6263 - reconstruction_loss: 0.8286 - kl_loss: 3.9976 - lr: 2.5000e-04\n",
      "Epoch 28/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6277 - reconstruction_loss: 0.8209 - kl_loss: 4.0166 - lr: 1.2500e-04\n",
      "Epoch 28: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 09:07:27,753]\u001b[0m Trial 11 finished with value: 0.006396309793350976 and parameters: {'encoder_layers': 6, 'encoder_units_l1': 16, 'encoder_units_l2': 967, 'encoder_units_l3': 155, 'encoder_units_l4': 86, 'encoder_units_l5': 36, 'encoder_units_l6': 94, 'decoder_layers': 8, 'decoder_units_l1': 167, 'decoder_units_l2': 308, 'decoder_units_l3': 898, 'decoder_units_l4': 293, 'decoder_units_l5': 72, 'decoder_units_l6': 20, 'decoder_units_l7': 18, 'decoder_units_l8': 18, 'beta': 0.2, 'lr': 0.001, 'batch_size': 128}. Best is trial 1 with value: 0.004936496311177702.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.9105 - reconstruction_loss: 3.2273 - kl_loss: 1.5001 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.8947 - reconstruction_loss: 2.0926 - kl_loss: 2.6083 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2369 - reconstruction_loss: 1.4761 - kl_loss: 3.2017 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9329 - reconstruction_loss: 1.2341 - kl_loss: 3.3898 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8662 - reconstruction_loss: 1.1688 - kl_loss: 3.4603 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8464 - reconstruction_loss: 1.1359 - kl_loss: 3.5137 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8200 - reconstruction_loss: 1.1123 - kl_loss: 3.5476 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8167 - reconstruction_loss: 1.0959 - kl_loss: 3.5767 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8056 - reconstruction_loss: 1.0916 - kl_loss: 3.5976 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8084 - reconstruction_loss: 1.0833 - kl_loss: 3.6286 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8494 - reconstruction_loss: 1.0894 - kl_loss: 3.6594 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8033 - reconstruction_loss: 1.0741 - kl_loss: 3.6713 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8152 - reconstruction_loss: 1.0596 - kl_loss: 3.6867 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7846 - reconstruction_loss: 1.0348 - kl_loss: 3.7232 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7648 - reconstruction_loss: 1.0266 - kl_loss: 3.7449 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7741 - reconstruction_loss: 1.0185 - kl_loss: 3.7595 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7704 - reconstruction_loss: 1.0499 - kl_loss: 3.7504 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7568 - reconstruction_loss: 0.9985 - kl_loss: 3.7510 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7562 - reconstruction_loss: 1.0115 - kl_loss: 3.7767 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7810 - reconstruction_loss: 1.0091 - kl_loss: 3.8089 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7181 - reconstruction_loss: 0.9658 - kl_loss: 3.8441 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7231 - reconstruction_loss: 0.9440 - kl_loss: 3.8391 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7516 - reconstruction_loss: 0.9685 - kl_loss: 3.8829 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7572 - reconstruction_loss: 0.9734 - kl_loss: 3.9002 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.7357 - reconstruction_loss: 0.9351 - kl_loss: 3.9624\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7357 - reconstruction_loss: 0.9341 - kl_loss: 3.9621 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6792 - reconstruction_loss: 0.8890 - kl_loss: 3.9570 - lr: 5.0000e-05\n",
      "Epoch 27/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6945 - reconstruction_loss: 0.8939 - kl_loss: 3.9723 - lr: 5.0000e-05\n",
      "Epoch 28/300\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 1.6705 - reconstruction_loss: 0.8633 - kl_loss: 3.9908 - lr: 5.0000e-05\n",
      "Epoch 29/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6498 - reconstruction_loss: 0.8517 - kl_loss: 3.9995 - lr: 5.0000e-05\n",
      "Epoch 30/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6422 - reconstruction_loss: 0.8365 - kl_loss: 4.0136 - lr: 5.0000e-05\n",
      "Epoch 31/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6322 - reconstruction_loss: 0.8268 - kl_loss: 4.0196 - lr: 5.0000e-05\n",
      "Epoch 32/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6964 - reconstruction_loss: 0.8858 - kl_loss: 4.0151 - lr: 5.0000e-05\n",
      "Epoch 33/300\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 1.6829 - reconstruction_loss: 0.8661 - kl_loss: 4.0251 - lr: 5.0000e-05\n",
      "Epoch 34/300\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.6747 - reconstruction_loss: 0.8599 - kl_loss: 4.0350\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6747 - reconstruction_loss: 0.8599 - kl_loss: 4.0350 - lr: 5.0000e-05\n",
      "Epoch 35/300\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 1.6314 - reconstruction_loss: 0.8205 - kl_loss: 4.0453 - lr: 2.5000e-05\n",
      "Epoch 36/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6300 - reconstruction_loss: 0.8165 - kl_loss: 4.0371 - lr: 2.5000e-05\n",
      "Epoch 37/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6383 - reconstruction_loss: 0.8251 - kl_loss: 4.0539 - lr: 2.5000e-05\n",
      "Epoch 38/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6443 - reconstruction_loss: 0.8217 - kl_loss: 4.0820 - lr: 2.5000e-05\n",
      "Epoch 39/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6208 - reconstruction_loss: 0.8067 - kl_loss: 4.0594 - lr: 2.5000e-05\n",
      "Epoch 40/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6136 - reconstruction_loss: 0.7945 - kl_loss: 4.0711 - lr: 2.5000e-05\n",
      "Epoch 41/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6102 - reconstruction_loss: 0.7940 - kl_loss: 4.0734 - lr: 2.5000e-05\n",
      "Epoch 42/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6061 - reconstruction_loss: 0.7900 - kl_loss: 4.0740 - lr: 2.5000e-05\n",
      "Epoch 43/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5961 - reconstruction_loss: 0.7819 - kl_loss: 4.0759 - lr: 2.5000e-05\n",
      "Epoch 44/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6026 - reconstruction_loss: 0.8057 - kl_loss: 4.0604 - lr: 2.5000e-05\n",
      "Epoch 45/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6675 - reconstruction_loss: 0.8269 - kl_loss: 4.0718 - lr: 2.5000e-05\n",
      "Epoch 46/300\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.6160 - reconstruction_loss: 0.7980 - kl_loss: 4.0860\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6160 - reconstruction_loss: 0.7972 - kl_loss: 4.0862 - lr: 2.5000e-05\n",
      "Epoch 47/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5931 - reconstruction_loss: 0.7789 - kl_loss: 4.0709 - lr: 1.2500e-05\n",
      "Epoch 48/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5919 - reconstruction_loss: 0.7755 - kl_loss: 4.0828 - lr: 1.2500e-05\n",
      "Epoch 49/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5935 - reconstruction_loss: 0.7756 - kl_loss: 4.0705 - lr: 1.2500e-05\n",
      "Epoch 50/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5897 - reconstruction_loss: 0.7744 - kl_loss: 4.0709 - lr: 1.2500e-05\n",
      "Epoch 51/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5949 - reconstruction_loss: 0.7771 - kl_loss: 4.0770 - lr: 1.2500e-05\n",
      "Epoch 52/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5956 - reconstruction_loss: 0.7775 - kl_loss: 4.0724 - lr: 1.2500e-05\n",
      "Epoch 53/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5873 - reconstruction_loss: 0.7733 - kl_loss: 4.0681 - lr: 1.2500e-05\n",
      "Epoch 54/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5902 - reconstruction_loss: 0.7792 - kl_loss: 4.0572 - lr: 1.2500e-05\n",
      "Epoch 55/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5878 - reconstruction_loss: 0.7743 - kl_loss: 4.0622 - lr: 1.2500e-05\n",
      "Epoch 56/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5939 - reconstruction_loss: 0.7790 - kl_loss: 4.0621 - lr: 1.2500e-05\n",
      "Epoch 57/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5834 - reconstruction_loss: 0.7713 - kl_loss: 4.0622 - lr: 1.2500e-05\n",
      "Epoch 58/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5799 - reconstruction_loss: 0.7688 - kl_loss: 4.0678 - lr: 1.2500e-05\n",
      "Epoch 59/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5802 - reconstruction_loss: 0.7667 - kl_loss: 4.0574 - lr: 1.2500e-05\n",
      "Epoch 60/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5824 - reconstruction_loss: 0.7656 - kl_loss: 4.0611 - lr: 1.2500e-05\n",
      "Epoch 61/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5740 - reconstruction_loss: 0.7656 - kl_loss: 4.0505 - lr: 1.2500e-05\n",
      "Epoch 62/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5743 - reconstruction_loss: 0.7617 - kl_loss: 4.0597 - lr: 1.2500e-05\n",
      "Epoch 63/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5690 - reconstruction_loss: 0.7571 - kl_loss: 4.0649 - lr: 1.2500e-05\n",
      "Epoch 64/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5753 - reconstruction_loss: 0.7593 - kl_loss: 4.0706 - lr: 1.2500e-05\n",
      "Epoch 65/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5755 - reconstruction_loss: 0.7586 - kl_loss: 4.0809 - lr: 1.2500e-05\n",
      "Epoch 66/300\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.5738 - reconstruction_loss: 0.7579 - kl_loss: 4.0741\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5738 - reconstruction_loss: 0.7571 - kl_loss: 4.0745 - lr: 1.2500e-05\n",
      "Epoch 67/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5720 - reconstruction_loss: 0.7539 - kl_loss: 4.0673 - lr: 6.2500e-06\n",
      "Epoch 68/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5706 - reconstruction_loss: 0.7525 - kl_loss: 4.0811 - lr: 6.2500e-06\n",
      "Epoch 69/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5614 - reconstruction_loss: 0.7517 - kl_loss: 4.0732 - lr: 6.2500e-06\n",
      "Epoch 70/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5652 - reconstruction_loss: 0.7561 - kl_loss: 4.0651 - lr: 6.2500e-06\n",
      "Epoch 71/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5707 - reconstruction_loss: 0.7543 - kl_loss: 4.0747 - lr: 6.2500e-06\n",
      "Epoch 72/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5616 - reconstruction_loss: 0.7514 - kl_loss: 4.0560 - lr: 6.2500e-06\n",
      "Epoch 73/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5678 - reconstruction_loss: 0.7518 - kl_loss: 4.0768 - lr: 6.2500e-06\n",
      "Epoch 74/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5721 - reconstruction_loss: 0.7585 - kl_loss: 4.0684 - lr: 6.2500e-06\n",
      "Epoch 75/300\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.5832 - reconstruction_loss: 0.7621 - kl_loss: 4.0608\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5831 - reconstruction_loss: 0.7614 - kl_loss: 4.0606 - lr: 6.2500e-06\n",
      "Epoch 76/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5642 - reconstruction_loss: 0.7502 - kl_loss: 4.0676 - lr: 3.1250e-06\n",
      "Epoch 77/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5676 - reconstruction_loss: 0.7496 - kl_loss: 4.0711 - lr: 3.1250e-06\n",
      "Epoch 78/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5625 - reconstruction_loss: 0.7484 - kl_loss: 4.0643 - lr: 3.1250e-06\n",
      "Epoch 79/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5611 - reconstruction_loss: 0.7488 - kl_loss: 4.0675 - lr: 3.1250e-06\n",
      "Epoch 80/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5600 - reconstruction_loss: 0.7481 - kl_loss: 4.0705 - lr: 3.1250e-06\n",
      "Epoch 81/300\n",
      "775/782 [============================>.] - ETA: 0s - loss: 1.5627 - reconstruction_loss: 0.7487 - kl_loss: 4.0685\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5627 - reconstruction_loss: 0.7481 - kl_loss: 4.0682 - lr: 3.1250e-06\n",
      "Epoch 82/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5600 - reconstruction_loss: 0.7487 - kl_loss: 4.0613 - lr: 1.5625e-06\n",
      "Epoch 83/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5621 - reconstruction_loss: 0.7473 - kl_loss: 4.0682 - lr: 1.5625e-06\n",
      "Epoch 84/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5637 - reconstruction_loss: 0.7468 - kl_loss: 4.0710 - lr: 1.5625e-06\n",
      "Epoch 85/300\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.5563 - reconstruction_loss: 0.7477 - kl_loss: 4.0694\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5563 - reconstruction_loss: 0.7473 - kl_loss: 4.0695 - lr: 1.5625e-06\n",
      "Epoch 86/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5595 - reconstruction_loss: 0.7481 - kl_loss: 4.0704 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5655 - reconstruction_loss: 0.7474 - kl_loss: 4.0667 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5603 - reconstruction_loss: 0.7485 - kl_loss: 4.0674 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5606 - reconstruction_loss: 0.7460 - kl_loss: 4.0733 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5588 - reconstruction_loss: 0.7475 - kl_loss: 4.0646 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5606 - reconstruction_loss: 0.7474 - kl_loss: 4.0639 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5562 - reconstruction_loss: 0.7462 - kl_loss: 4.0667 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5618 - reconstruction_loss: 0.7453 - kl_loss: 4.0694 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5602 - reconstruction_loss: 0.7464 - kl_loss: 4.0651 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5624 - reconstruction_loss: 0.7476 - kl_loss: 4.0630 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5587 - reconstruction_loss: 0.7446 - kl_loss: 4.0660 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5592 - reconstruction_loss: 0.7466 - kl_loss: 4.0601 - lr: 1.0000e-06\n",
      "Epoch 98/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5559 - reconstruction_loss: 0.7468 - kl_loss: 4.0567 - lr: 1.0000e-06\n",
      "Epoch 99/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5613 - reconstruction_loss: 0.7449 - kl_loss: 4.0666 - lr: 1.0000e-06\n",
      "Epoch 100/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5556 - reconstruction_loss: 0.7454 - kl_loss: 4.0665 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5598 - reconstruction_loss: 0.7446 - kl_loss: 4.0663 - lr: 1.0000e-06\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5605 - reconstruction_loss: 0.7459 - kl_loss: 4.0620 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5602 - reconstruction_loss: 0.7445 - kl_loss: 4.0647 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5597 - reconstruction_loss: 0.7453 - kl_loss: 4.0606 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5580 - reconstruction_loss: 0.7441 - kl_loss: 4.0677 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5555 - reconstruction_loss: 0.7437 - kl_loss: 4.0609 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5568 - reconstruction_loss: 0.7427 - kl_loss: 4.0676 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5585 - reconstruction_loss: 0.7430 - kl_loss: 4.0741 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5599 - reconstruction_loss: 0.7434 - kl_loss: 4.0724 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5571 - reconstruction_loss: 0.7450 - kl_loss: 4.0571 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 1.5580 - reconstruction_loss: 0.7446 - kl_loss: 4.0636 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5562 - reconstruction_loss: 0.7423 - kl_loss: 4.0688 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5603 - reconstruction_loss: 0.7442 - kl_loss: 4.0650 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5550 - reconstruction_loss: 0.7435 - kl_loss: 4.0672 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5586 - reconstruction_loss: 0.7422 - kl_loss: 4.0669 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5586 - reconstruction_loss: 0.7424 - kl_loss: 4.0652 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5597 - reconstruction_loss: 0.7430 - kl_loss: 4.0652 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5571 - reconstruction_loss: 0.7426 - kl_loss: 4.0684 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5560 - reconstruction_loss: 0.7420 - kl_loss: 4.0576 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5589 - reconstruction_loss: 0.7450 - kl_loss: 4.0701 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5567 - reconstruction_loss: 0.7426 - kl_loss: 4.0681 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5598 - reconstruction_loss: 0.7439 - kl_loss: 4.0713 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5585 - reconstruction_loss: 0.7435 - kl_loss: 4.0716 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5606 - reconstruction_loss: 0.7449 - kl_loss: 4.0657 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5563 - reconstruction_loss: 0.7433 - kl_loss: 4.0628 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5610 - reconstruction_loss: 0.7442 - kl_loss: 4.0653 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5606 - reconstruction_loss: 0.7452 - kl_loss: 4.0656 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5565 - reconstruction_loss: 0.7452 - kl_loss: 4.0624 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5602 - reconstruction_loss: 0.7452 - kl_loss: 4.0572 - lr: 1.0000e-06\n",
      "Epoch 129: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 09:16:28,352]\u001b[0m Trial 12 finished with value: 0.005835532853478607 and parameters: {'encoder_layers': 8, 'encoder_units_l1': 305, 'encoder_units_l2': 402, 'encoder_units_l3': 126, 'encoder_units_l4': 362, 'encoder_units_l5': 59, 'encoder_units_l6': 117, 'encoder_units_l7': 18, 'encoder_units_l8': 282, 'decoder_layers': 4, 'decoder_units_l1': 656, 'decoder_units_l2': 228, 'decoder_units_l3': 38, 'decoder_units_l4': 301, 'beta': 0.2, 'lr': 0.0001, 'batch_size': 128}. Best is trial 1 with value: 0.004936496311177702.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 6.9995 - reconstruction_loss: 5.5427 - kl_loss: 2.3560 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 4.2819 - reconstruction_loss: 3.2645 - kl_loss: 3.7152 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 3.1064 - reconstruction_loss: 2.1819 - kl_loss: 4.3022 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.9541 - reconstruction_loss: 2.0623 - kl_loss: 4.3920 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.8126 - reconstruction_loss: 1.8876 - kl_loss: 4.5198 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.7352 - reconstruction_loss: 1.7550 - kl_loss: 4.6877 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.6733 - reconstruction_loss: 1.6929 - kl_loss: 4.7267 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.5500 - reconstruction_loss: 1.5924 - kl_loss: 4.8338 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.5978 - reconstruction_loss: 1.6062 - kl_loss: 4.8737 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.5370 - reconstruction_loss: 1.5728 - kl_loss: 4.9191 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 3.3907 - reconstruction_loss: 1.9592 - kl_loss: 4.7356 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.6422 - reconstruction_loss: 1.6990 - kl_loss: 4.8482 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.6215 - reconstruction_loss: 1.6958 - kl_loss: 4.8533\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.6218 - reconstruction_loss: 1.6948 - kl_loss: 4.8517 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.5022 - reconstruction_loss: 1.5209 - kl_loss: 4.9171 - lr: 5.0000e-04\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4417 - reconstruction_loss: 1.4398 - kl_loss: 4.8740 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4743 - reconstruction_loss: 1.4486 - kl_loss: 4.9658 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3883 - reconstruction_loss: 1.4119 - kl_loss: 4.9685 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3998 - reconstruction_loss: 1.4087 - kl_loss: 4.8587 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3812 - reconstruction_loss: 1.4205 - kl_loss: 4.9302 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4681 - reconstruction_loss: 1.4778 - kl_loss: 5.0094 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3831 - reconstruction_loss: 1.5033 - kl_loss: 4.9609\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3834 - reconstruction_loss: 1.5033 - kl_loss: 4.9609 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5350 - reconstruction_loss: 1.4542 - kl_loss: 5.2430 - lr: 2.5000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4312 - reconstruction_loss: 1.3846 - kl_loss: 5.1336 - lr: 2.5000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3741 - reconstruction_loss: 1.3519 - kl_loss: 5.0312 - lr: 2.5000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3242 - reconstruction_loss: 1.3286 - kl_loss: 4.9493 - lr: 2.5000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2915 - reconstruction_loss: 1.3157 - kl_loss: 4.8691 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2760 - reconstruction_loss: 1.3028 - kl_loss: 4.8170 - lr: 2.5000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2465 - reconstruction_loss: 1.2930 - kl_loss: 4.7843 - lr: 2.5000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2558 - reconstruction_loss: 1.2884 - kl_loss: 4.7722 - lr: 2.5000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2257 - reconstruction_loss: 1.2829 - kl_loss: 4.7438 - lr: 2.5000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2319 - reconstruction_loss: 1.2777 - kl_loss: 4.7701 - lr: 2.5000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2249 - reconstruction_loss: 1.2730 - kl_loss: 4.7526 - lr: 2.5000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2289 - reconstruction_loss: 1.2755 - kl_loss: 4.7484 - lr: 2.5000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2133 - reconstruction_loss: 1.2734 - kl_loss: 4.7394 - lr: 2.5000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2141 - reconstruction_loss: 1.2711 - kl_loss: 4.7335 - lr: 2.5000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2265 - reconstruction_loss: 1.2836 - kl_loss: 4.7530 - lr: 2.5000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2116 - reconstruction_loss: 1.2656 - kl_loss: 4.7321 - lr: 2.5000e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2202 - reconstruction_loss: 1.2706 - kl_loss: 4.7612 - lr: 2.5000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2449 - reconstruction_loss: 1.2771 - kl_loss: 4.7885 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2212 - reconstruction_loss: 1.2680 - kl_loss: 4.7223\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2211 - reconstruction_loss: 1.2680 - kl_loss: 4.7223 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1960 - reconstruction_loss: 1.2522 - kl_loss: 4.7139 - lr: 1.2500e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1939 - reconstruction_loss: 1.2553 - kl_loss: 4.7139 - lr: 1.2500e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1902 - reconstruction_loss: 1.2512 - kl_loss: 4.7051 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1913 - reconstruction_loss: 1.2518 - kl_loss: 4.6897 - lr: 1.2500e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1874 - reconstruction_loss: 1.2488 - kl_loss: 4.6857 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1924 - reconstruction_loss: 1.2520 - kl_loss: 4.6886 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1916 - reconstruction_loss: 1.2518 - kl_loss: 4.6900 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1908 - reconstruction_loss: 1.2497 - kl_loss: 4.6878\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1908 - reconstruction_loss: 1.2497 - kl_loss: 4.6878 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1834 - reconstruction_loss: 1.2416 - kl_loss: 4.6807 - lr: 6.2500e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1821 - reconstruction_loss: 1.2429 - kl_loss: 4.6747 - lr: 6.2500e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1806 - reconstruction_loss: 1.2433 - kl_loss: 4.6836 - lr: 6.2500e-05\n",
      "Epoch 52/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.1886 - reconstruction_loss: 1.2468 - kl_loss: 4.6830\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1886 - reconstruction_loss: 1.2453 - kl_loss: 4.6830 - lr: 6.2500e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1803 - reconstruction_loss: 1.2418 - kl_loss: 4.6731 - lr: 3.1250e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1745 - reconstruction_loss: 1.2403 - kl_loss: 4.6727 - lr: 3.1250e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1761 - reconstruction_loss: 1.2421 - kl_loss: 4.6820 - lr: 3.1250e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1777 - reconstruction_loss: 1.2420 - kl_loss: 4.6736 - lr: 3.1250e-05\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1822 - reconstruction_loss: 1.2412 - kl_loss: 4.6788\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1821 - reconstruction_loss: 1.2412 - kl_loss: 4.6788 - lr: 3.1250e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1747 - reconstruction_loss: 1.2379 - kl_loss: 4.6744 - lr: 1.5625e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1732 - reconstruction_loss: 1.2377 - kl_loss: 4.6714 - lr: 1.5625e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1766 - reconstruction_loss: 1.2374 - kl_loss: 4.6788 - lr: 1.5625e-05\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1710 - reconstruction_loss: 1.2374 - kl_loss: 4.6698 - lr: 1.5625e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1701 - reconstruction_loss: 1.2363 - kl_loss: 4.6774 - lr: 1.5625e-05\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1717 - reconstruction_loss: 1.2370 - kl_loss: 4.6681 - lr: 1.5625e-05\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1686 - reconstruction_loss: 1.2372 - kl_loss: 4.6775 - lr: 1.5625e-05\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1785 - reconstruction_loss: 1.2382 - kl_loss: 4.6685 - lr: 1.5625e-05\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1768 - reconstruction_loss: 1.2370 - kl_loss: 4.6738\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1768 - reconstruction_loss: 1.2370 - kl_loss: 4.6738 - lr: 1.5625e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1703 - reconstruction_loss: 1.2352 - kl_loss: 4.6774 - lr: 7.8125e-06\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1728 - reconstruction_loss: 1.2358 - kl_loss: 4.6694 - lr: 7.8125e-06\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1674 - reconstruction_loss: 1.2380 - kl_loss: 4.6645 - lr: 7.8125e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1698 - reconstruction_loss: 1.2365 - kl_loss: 4.6678 - lr: 7.8125e-06\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1697 - reconstruction_loss: 1.2359 - kl_loss: 4.6688\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1697 - reconstruction_loss: 1.2359 - kl_loss: 4.6688 - lr: 7.8125e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1749 - reconstruction_loss: 1.2354 - kl_loss: 4.6707 - lr: 3.9063e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1708 - reconstruction_loss: 1.2354 - kl_loss: 4.6760 - lr: 3.9063e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1685 - reconstruction_loss: 1.2355 - kl_loss: 4.6711 - lr: 3.9063e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1769 - reconstruction_loss: 1.2347 - kl_loss: 4.6749\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1769 - reconstruction_loss: 1.2347 - kl_loss: 4.6749 - lr: 3.9063e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1712 - reconstruction_loss: 1.2362 - kl_loss: 4.6723 - lr: 1.9531e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1682 - reconstruction_loss: 1.2340 - kl_loss: 4.6800 - lr: 1.9531e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1744 - reconstruction_loss: 1.2331 - kl_loss: 4.6778 - lr: 1.9531e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1748 - reconstruction_loss: 1.2330 - kl_loss: 4.6754 - lr: 1.9531e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1676 - reconstruction_loss: 1.2333 - kl_loss: 4.6673 - lr: 1.9531e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1728 - reconstruction_loss: 1.2345 - kl_loss: 4.6677 - lr: 1.9531e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1765 - reconstruction_loss: 1.2350 - kl_loss: 4.6687 - lr: 1.9531e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1684 - reconstruction_loss: 1.2354 - kl_loss: 4.6733\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1684 - reconstruction_loss: 1.2354 - kl_loss: 4.6733 - lr: 1.9531e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1735 - reconstruction_loss: 1.2342 - kl_loss: 4.6772 - lr: 1.0000e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1671 - reconstruction_loss: 1.2333 - kl_loss: 4.6759 - lr: 1.0000e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1737 - reconstruction_loss: 1.2356 - kl_loss: 4.6748 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1695 - reconstruction_loss: 1.2341 - kl_loss: 4.6768 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1709 - reconstruction_loss: 1.2338 - kl_loss: 4.6764 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1738 - reconstruction_loss: 1.2341 - kl_loss: 4.6733 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1748 - reconstruction_loss: 1.2335 - kl_loss: 4.6728 - lr: 1.0000e-06\n",
      "Epoch 90: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 09:23:09,331]\u001b[0m Trial 13 finished with value: 0.004823270072671707 and parameters: {'encoder_layers': 5, 'encoder_units_l1': 105, 'encoder_units_l2': 393, 'encoder_units_l3': 911, 'encoder_units_l4': 966, 'encoder_units_l5': 54, 'decoder_layers': 7, 'decoder_units_l1': 142, 'decoder_units_l2': 252, 'decoder_units_l3': 297, 'decoder_units_l4': 205, 'decoder_units_l5': 75, 'decoder_units_l6': 283, 'decoder_units_l7': 42, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 13 with value: 0.004823270072671707.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 7.2818 - reconstruction_loss: 5.9784 - kl_loss: 2.0922 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 4.2306 - reconstruction_loss: 3.1409 - kl_loss: 3.7974 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 3.1828 - reconstruction_loss: 2.2673 - kl_loss: 4.2746 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.9191 - reconstruction_loss: 1.9811 - kl_loss: 4.4222 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.8889 - reconstruction_loss: 1.8933 - kl_loss: 4.6228 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6674 - reconstruction_loss: 1.6945 - kl_loss: 4.8196 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6819 - reconstruction_loss: 1.6958 - kl_loss: 4.9308 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6109 - reconstruction_loss: 1.5816 - kl_loss: 4.9482 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5400 - reconstruction_loss: 1.5179 - kl_loss: 4.9585 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4598 - reconstruction_loss: 1.5261 - kl_loss: 4.9526 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4140 - reconstruction_loss: 1.4461 - kl_loss: 4.9466 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4154 - reconstruction_loss: 1.4142 - kl_loss: 4.9076 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5637 - reconstruction_loss: 1.5175 - kl_loss: 4.9870 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4095 - reconstruction_loss: 1.4497 - kl_loss: 4.8955 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.4458 - reconstruction_loss: 1.4206 - kl_loss: 4.8959\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4452 - reconstruction_loss: 1.4187 - kl_loss: 4.8964 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3605 - reconstruction_loss: 1.4139 - kl_loss: 4.9788 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4270 - reconstruction_loss: 1.3969 - kl_loss: 4.9237 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3498 - reconstruction_loss: 1.3605 - kl_loss: 4.8706 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3061 - reconstruction_loss: 1.3341 - kl_loss: 4.8472 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3202 - reconstruction_loss: 1.3663 - kl_loss: 4.9384 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3618 - reconstruction_loss: 1.3644 - kl_loss: 4.9071 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3024 - reconstruction_loss: 1.3301 - kl_loss: 4.8376 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3014 - reconstruction_loss: 1.3324 - kl_loss: 4.8498 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2936 - reconstruction_loss: 1.3252 - kl_loss: 4.8134 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2869 - reconstruction_loss: 1.3143 - kl_loss: 4.8374 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2864 - reconstruction_loss: 1.3167 - kl_loss: 4.8490 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2712 - reconstruction_loss: 1.3028 - kl_loss: 4.8070 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3046 - reconstruction_loss: 1.3250 - kl_loss: 4.8367 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2811 - reconstruction_loss: 1.3099 - kl_loss: 4.8077 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.3063 - reconstruction_loss: 1.3326 - kl_loss: 4.8316\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3062 - reconstruction_loss: 1.3315 - kl_loss: 4.8308 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2415 - reconstruction_loss: 1.2935 - kl_loss: 4.7969 - lr: 2.5000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2488 - reconstruction_loss: 1.2837 - kl_loss: 4.8215 - lr: 2.5000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2344 - reconstruction_loss: 1.2766 - kl_loss: 4.8022 - lr: 2.5000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2430 - reconstruction_loss: 1.2777 - kl_loss: 4.8061 - lr: 2.5000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2456 - reconstruction_loss: 1.2852 - kl_loss: 4.7992 - lr: 2.5000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2546 - reconstruction_loss: 1.2866 - kl_loss: 4.8411\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2546 - reconstruction_loss: 1.2866 - kl_loss: 4.8411 - lr: 2.5000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2384 - reconstruction_loss: 1.2731 - kl_loss: 4.7893 - lr: 1.2500e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2320 - reconstruction_loss: 1.2684 - kl_loss: 4.7827 - lr: 1.2500e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2321 - reconstruction_loss: 1.2638 - kl_loss: 4.7878 - lr: 1.2500e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2247 - reconstruction_loss: 1.2649 - kl_loss: 4.7774 - lr: 1.2500e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2302 - reconstruction_loss: 1.2670 - kl_loss: 4.7744 - lr: 1.2500e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2200 - reconstruction_loss: 1.2643 - kl_loss: 4.7763 - lr: 1.2500e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2226 - reconstruction_loss: 1.2594 - kl_loss: 4.8091 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2346 - reconstruction_loss: 1.2666 - kl_loss: 4.8036 - lr: 1.2500e-04\n",
      "Epoch 45/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2272 - reconstruction_loss: 1.2672 - kl_loss: 4.7927\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2272 - reconstruction_loss: 1.2659 - kl_loss: 4.7925 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2222 - reconstruction_loss: 1.2581 - kl_loss: 4.7969 - lr: 6.2500e-05\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2124 - reconstruction_loss: 1.2576 - kl_loss: 4.7844 - lr: 6.2500e-05\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2058 - reconstruction_loss: 1.2569 - kl_loss: 4.7672 - lr: 6.2500e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2096 - reconstruction_loss: 1.2552 - kl_loss: 4.7654 - lr: 6.2500e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2110 - reconstruction_loss: 1.2535 - kl_loss: 4.7804 - lr: 6.2500e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2112 - reconstruction_loss: 1.2571 - kl_loss: 4.7854 - lr: 6.2500e-05\n",
      "Epoch 52/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2152 - reconstruction_loss: 1.2570 - kl_loss: 4.7786\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2152 - reconstruction_loss: 1.2558 - kl_loss: 4.7787 - lr: 6.2500e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2056 - reconstruction_loss: 1.2529 - kl_loss: 4.7900 - lr: 3.1250e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2153 - reconstruction_loss: 1.2535 - kl_loss: 4.7957 - lr: 3.1250e-05\n",
      "Epoch 55/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2035 - reconstruction_loss: 1.2528 - kl_loss: 4.7912\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2035 - reconstruction_loss: 1.2517 - kl_loss: 4.7912 - lr: 3.1250e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2058 - reconstruction_loss: 1.2511 - kl_loss: 4.7935 - lr: 1.5625e-05\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2153 - reconstruction_loss: 1.2515 - kl_loss: 4.7878 - lr: 1.5625e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2105 - reconstruction_loss: 1.2522 - kl_loss: 4.7902\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2105 - reconstruction_loss: 1.2522 - kl_loss: 4.7902 - lr: 1.5625e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2120 - reconstruction_loss: 1.2495 - kl_loss: 4.7870 - lr: 7.8125e-06\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2088 - reconstruction_loss: 1.2488 - kl_loss: 4.7729 - lr: 7.8125e-06\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2067 - reconstruction_loss: 1.2494 - kl_loss: 4.7756 - lr: 7.8125e-06\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2072 - reconstruction_loss: 1.2492 - kl_loss: 4.7766 - lr: 7.8125e-06\n",
      "Epoch 63/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2070 - reconstruction_loss: 1.2508 - kl_loss: 4.7739\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2069 - reconstruction_loss: 1.2498 - kl_loss: 4.7744 - lr: 7.8125e-06\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2078 - reconstruction_loss: 1.2500 - kl_loss: 4.7771 - lr: 3.9063e-06\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2073 - reconstruction_loss: 1.2471 - kl_loss: 4.7807 - lr: 3.9063e-06\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2015 - reconstruction_loss: 1.2479 - kl_loss: 4.7775\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2015 - reconstruction_loss: 1.2479 - kl_loss: 4.7775 - lr: 3.9063e-06\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2007 - reconstruction_loss: 1.2457 - kl_loss: 4.7764 - lr: 1.9531e-06\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2042 - reconstruction_loss: 1.2468 - kl_loss: 4.7763 - lr: 1.9531e-06\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2016 - reconstruction_loss: 1.2467 - kl_loss: 4.7767 - lr: 1.9531e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2012 - reconstruction_loss: 1.2468 - kl_loss: 4.7726\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2012 - reconstruction_loss: 1.2468 - kl_loss: 4.7726 - lr: 1.9531e-06\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2028 - reconstruction_loss: 1.2460 - kl_loss: 4.7725 - lr: 1.0000e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1981 - reconstruction_loss: 1.2463 - kl_loss: 4.7715 - lr: 1.0000e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1991 - reconstruction_loss: 1.2465 - kl_loss: 4.7735 - lr: 1.0000e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2024 - reconstruction_loss: 1.2451 - kl_loss: 4.7774 - lr: 1.0000e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1991 - reconstruction_loss: 1.2458 - kl_loss: 4.7777 - lr: 1.0000e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1981 - reconstruction_loss: 1.2456 - kl_loss: 4.7756 - lr: 1.0000e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2039 - reconstruction_loss: 1.2456 - kl_loss: 4.7753 - lr: 1.0000e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2038 - reconstruction_loss: 1.2452 - kl_loss: 4.7780 - lr: 1.0000e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2067 - reconstruction_loss: 1.2446 - kl_loss: 4.7781 - lr: 1.0000e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2046 - reconstruction_loss: 1.2458 - kl_loss: 4.7777 - lr: 1.0000e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2033 - reconstruction_loss: 1.2459 - kl_loss: 4.7772 - lr: 1.0000e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2059 - reconstruction_loss: 1.2463 - kl_loss: 4.7794 - lr: 1.0000e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2014 - reconstruction_loss: 1.2454 - kl_loss: 4.7795 - lr: 1.0000e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2035 - reconstruction_loss: 1.2439 - kl_loss: 4.7816 - lr: 1.0000e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2037 - reconstruction_loss: 1.2441 - kl_loss: 4.7775 - lr: 1.0000e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1959 - reconstruction_loss: 1.2448 - kl_loss: 4.7752 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2010 - reconstruction_loss: 1.2450 - kl_loss: 4.7753 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1965 - reconstruction_loss: 1.2434 - kl_loss: 4.7737 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2106 - reconstruction_loss: 1.2449 - kl_loss: 4.7750 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1988 - reconstruction_loss: 1.2450 - kl_loss: 4.7760 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2024 - reconstruction_loss: 1.2448 - kl_loss: 4.7757 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1996 - reconstruction_loss: 1.2448 - kl_loss: 4.7762 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2042 - reconstruction_loss: 1.2443 - kl_loss: 4.7786 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2011 - reconstruction_loss: 1.2445 - kl_loss: 4.7792 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1988 - reconstruction_loss: 1.2444 - kl_loss: 4.7797 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1939 - reconstruction_loss: 1.2436 - kl_loss: 4.7771 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2047 - reconstruction_loss: 1.2449 - kl_loss: 4.7792 - lr: 1.0000e-06\n",
      "Epoch 98/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2032 - reconstruction_loss: 1.2426 - kl_loss: 4.7780 - lr: 1.0000e-06\n",
      "Epoch 98: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 09:29:35,455]\u001b[0m Trial 14 finished with value: 0.004862001863070236 and parameters: {'encoder_layers': 5, 'encoder_units_l1': 367, 'encoder_units_l2': 102, 'encoder_units_l3': 1001, 'encoder_units_l4': 958, 'encoder_units_l5': 93, 'decoder_layers': 8, 'decoder_units_l1': 156, 'decoder_units_l2': 497, 'decoder_units_l3': 69, 'decoder_units_l4': 456, 'decoder_units_l5': 70, 'decoder_units_l6': 287, 'decoder_units_l7': 45, 'decoder_units_l8': 32, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 13 with value: 0.004823270072671707.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 5.9080 - reconstruction_loss: 4.1547 - kl_loss: 3.3015 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.9448 - reconstruction_loss: 2.9647 - kl_loss: 4.0021 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 3.4125 - reconstruction_loss: 2.4532 - kl_loss: 4.2887 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 3.0765 - reconstruction_loss: 2.1574 - kl_loss: 4.5180 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.9158 - reconstruction_loss: 1.9400 - kl_loss: 4.6072 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.8177 - reconstruction_loss: 1.8132 - kl_loss: 4.7796 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6760 - reconstruction_loss: 1.7016 - kl_loss: 4.8259 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6326 - reconstruction_loss: 1.6731 - kl_loss: 4.9081 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6099 - reconstruction_loss: 1.5647 - kl_loss: 4.9374 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4821 - reconstruction_loss: 1.4962 - kl_loss: 4.9156 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5020 - reconstruction_loss: 1.5401 - kl_loss: 4.9440 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5303 - reconstruction_loss: 1.4998 - kl_loss: 4.9545 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4234 - reconstruction_loss: 1.4532 - kl_loss: 4.9332 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5740 - reconstruction_loss: 1.5277 - kl_loss: 5.0254 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4613 - reconstruction_loss: 1.4380 - kl_loss: 5.0238 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3863 - reconstruction_loss: 1.4001 - kl_loss: 4.9664 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3901 - reconstruction_loss: 1.3970 - kl_loss: 4.9486 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4065 - reconstruction_loss: 1.4260 - kl_loss: 5.0288 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.7154 - reconstruction_loss: 1.5165 - kl_loss: 5.0733 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.4408 - reconstruction_loss: 1.4044 - kl_loss: 5.1151\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4405 - reconstruction_loss: 1.4026 - kl_loss: 5.1133 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3167 - reconstruction_loss: 1.3177 - kl_loss: 4.9375 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2832 - reconstruction_loss: 1.3021 - kl_loss: 4.9025 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2759 - reconstruction_loss: 1.3021 - kl_loss: 4.8739 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2809 - reconstruction_loss: 1.3056 - kl_loss: 4.8615 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2690 - reconstruction_loss: 1.2921 - kl_loss: 4.8496 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2629 - reconstruction_loss: 1.2994 - kl_loss: 4.8334 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2499 - reconstruction_loss: 1.2858 - kl_loss: 4.8209 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2731 - reconstruction_loss: 1.2915 - kl_loss: 4.8359 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2479 - reconstruction_loss: 1.2765 - kl_loss: 4.8028 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2406 - reconstruction_loss: 1.2841 - kl_loss: 4.7996 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2505 - reconstruction_loss: 1.2881 - kl_loss: 4.8267 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2369 - reconstruction_loss: 1.2787 - kl_loss: 4.7863 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2438 - reconstruction_loss: 1.2867 - kl_loss: 4.7691 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2378 - reconstruction_loss: 1.2801 - kl_loss: 4.7553 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2327 - reconstruction_loss: 1.2820 - kl_loss: 4.7580 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2515 - reconstruction_loss: 1.2870 - kl_loss: 4.7654 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2238 - reconstruction_loss: 1.2753 - kl_loss: 4.7438 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2201 - reconstruction_loss: 1.2712 - kl_loss: 4.7521 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2268 - reconstruction_loss: 1.2755 - kl_loss: 4.7544 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2340 - reconstruction_loss: 1.2730 - kl_loss: 4.7693 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2352 - reconstruction_loss: 1.2892 - kl_loss: 4.7733\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2352 - reconstruction_loss: 1.2879 - kl_loss: 4.7728 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2132 - reconstruction_loss: 1.2578 - kl_loss: 4.7305 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2026 - reconstruction_loss: 1.2529 - kl_loss: 4.7336 - lr: 2.5000e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1937 - reconstruction_loss: 1.2521 - kl_loss: 4.7277 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2016 - reconstruction_loss: 1.2515 - kl_loss: 4.7351 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1977 - reconstruction_loss: 1.2488 - kl_loss: 4.7211 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1939 - reconstruction_loss: 1.2521 - kl_loss: 4.7203 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1956 - reconstruction_loss: 1.2527 - kl_loss: 4.7200 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2050 - reconstruction_loss: 1.2564 - kl_loss: 4.7368\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2050 - reconstruction_loss: 1.2552 - kl_loss: 4.7369 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1892 - reconstruction_loss: 1.2429 - kl_loss: 4.7335 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1896 - reconstruction_loss: 1.2430 - kl_loss: 4.7329 - lr: 1.2500e-04\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1865 - reconstruction_loss: 1.2416 - kl_loss: 4.7293 - lr: 1.2500e-04\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1833 - reconstruction_loss: 1.2423 - kl_loss: 4.7167 - lr: 1.2500e-04\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1913 - reconstruction_loss: 1.2419 - kl_loss: 4.7194 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1867 - reconstruction_loss: 1.2447 - kl_loss: 4.7262 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.1940 - reconstruction_loss: 1.2421 - kl_loss: 4.7252\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1940 - reconstruction_loss: 1.2406 - kl_loss: 4.7252 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1866 - reconstruction_loss: 1.2384 - kl_loss: 4.7146 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1832 - reconstruction_loss: 1.2367 - kl_loss: 4.7107 - lr: 6.2500e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1805 - reconstruction_loss: 1.2375 - kl_loss: 4.7138 - lr: 6.2500e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1799 - reconstruction_loss: 1.2376 - kl_loss: 4.7120 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.1821 - reconstruction_loss: 1.2405 - kl_loss: 4.7066\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1821 - reconstruction_loss: 1.2388 - kl_loss: 4.7065 - lr: 6.2500e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1809 - reconstruction_loss: 1.2337 - kl_loss: 4.7110 - lr: 3.1250e-05\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1823 - reconstruction_loss: 1.2322 - kl_loss: 4.7311 - lr: 3.1250e-05\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1743 - reconstruction_loss: 1.2355 - kl_loss: 4.7021 - lr: 3.1250e-05\n",
      "Epoch 65/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.1798 - reconstruction_loss: 1.2344 - kl_loss: 4.7120\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1798 - reconstruction_loss: 1.2334 - kl_loss: 4.7128 - lr: 3.1250e-05\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1737 - reconstruction_loss: 1.2318 - kl_loss: 4.7071 - lr: 1.5625e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1711 - reconstruction_loss: 1.2326 - kl_loss: 4.7081 - lr: 1.5625e-05\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1715 - reconstruction_loss: 1.2331 - kl_loss: 4.7065 - lr: 1.5625e-05\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1817 - reconstruction_loss: 1.2315 - kl_loss: 4.7071 - lr: 1.5625e-05\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1834 - reconstruction_loss: 1.2323 - kl_loss: 4.7131 - lr: 1.5625e-05\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1708 - reconstruction_loss: 1.2308 - kl_loss: 4.7095 - lr: 1.5625e-05\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1719 - reconstruction_loss: 1.2298 - kl_loss: 4.7173 - lr: 1.5625e-05\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1771 - reconstruction_loss: 1.2323 - kl_loss: 4.7084 - lr: 1.5625e-05\n",
      "Epoch 74/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.1795 - reconstruction_loss: 1.2322 - kl_loss: 4.7140\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1795 - reconstruction_loss: 1.2310 - kl_loss: 4.7137 - lr: 1.5625e-05\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1735 - reconstruction_loss: 1.2301 - kl_loss: 4.7207 - lr: 7.8125e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1755 - reconstruction_loss: 1.2309 - kl_loss: 4.7001 - lr: 7.8125e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1736 - reconstruction_loss: 1.2310 - kl_loss: 4.7033 - lr: 7.8125e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1761 - reconstruction_loss: 1.2313 - kl_loss: 4.7109 - lr: 7.8125e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1780 - reconstruction_loss: 1.2289 - kl_loss: 4.7172\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1780 - reconstruction_loss: 1.2289 - kl_loss: 4.7172 - lr: 7.8125e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1731 - reconstruction_loss: 1.2264 - kl_loss: 4.7219 - lr: 3.9063e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1778 - reconstruction_loss: 1.2310 - kl_loss: 4.7095 - lr: 3.9063e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1751 - reconstruction_loss: 1.2295 - kl_loss: 4.7100 - lr: 3.9063e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1740 - reconstruction_loss: 1.2282 - kl_loss: 4.7140\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1740 - reconstruction_loss: 1.2282 - kl_loss: 4.7140 - lr: 3.9063e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1737 - reconstruction_loss: 1.2282 - kl_loss: 4.7124 - lr: 1.9531e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1671 - reconstruction_loss: 1.2287 - kl_loss: 4.7093 - lr: 1.9531e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1746 - reconstruction_loss: 1.2292 - kl_loss: 4.7031 - lr: 1.9531e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1741 - reconstruction_loss: 1.2315 - kl_loss: 4.7017 - lr: 1.9531e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1727 - reconstruction_loss: 1.2285 - kl_loss: 4.7080 - lr: 1.9531e-06\n",
      "Epoch 89/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.1711 - reconstruction_loss: 1.2314 - kl_loss: 4.7057\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1711 - reconstruction_loss: 1.2304 - kl_loss: 4.7063 - lr: 1.9531e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1736 - reconstruction_loss: 1.2296 - kl_loss: 4.7108 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1669 - reconstruction_loss: 1.2282 - kl_loss: 4.7069 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1704 - reconstruction_loss: 1.2300 - kl_loss: 4.7003 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1733 - reconstruction_loss: 1.2316 - kl_loss: 4.7032 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1678 - reconstruction_loss: 1.2274 - kl_loss: 4.7043 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1693 - reconstruction_loss: 1.2293 - kl_loss: 4.7012 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1686 - reconstruction_loss: 1.2295 - kl_loss: 4.7027 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1715 - reconstruction_loss: 1.2305 - kl_loss: 4.7051 - lr: 1.0000e-06\n",
      "Epoch 98/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1793 - reconstruction_loss: 1.2277 - kl_loss: 4.7072 - lr: 1.0000e-06\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1743 - reconstruction_loss: 1.2288 - kl_loss: 4.7070 - lr: 1.0000e-06\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1742 - reconstruction_loss: 1.2297 - kl_loss: 4.7058 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1723 - reconstruction_loss: 1.2285 - kl_loss: 4.7049 - lr: 1.0000e-06\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1702 - reconstruction_loss: 1.2293 - kl_loss: 4.7005 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1685 - reconstruction_loss: 1.2289 - kl_loss: 4.6997 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1760 - reconstruction_loss: 1.2308 - kl_loss: 4.7021 - lr: 1.0000e-06\n",
      "Epoch 104: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 09:36:12,181]\u001b[0m Trial 15 finished with value: 0.004807672164569487 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 511, 'encoder_units_l2': 245, 'encoder_units_l3': 918, 'decoder_layers': 7, 'decoder_units_l1': 460, 'decoder_units_l2': 118, 'decoder_units_l3': 221, 'decoder_units_l4': 520, 'decoder_units_l5': 102, 'decoder_units_l6': 371, 'decoder_units_l7': 34, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 8ms/step - loss: 5.8694 - reconstruction_loss: 4.1198 - kl_loss: 3.3596 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 4.1818 - reconstruction_loss: 3.2069 - kl_loss: 3.8278 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.3838 - reconstruction_loss: 2.3901 - kl_loss: 4.3649 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.0478 - reconstruction_loss: 2.0747 - kl_loss: 4.5859 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.9110 - reconstruction_loss: 1.9567 - kl_loss: 4.7012 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.8650 - reconstruction_loss: 1.8754 - kl_loss: 4.7238 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.7608 - reconstruction_loss: 1.7912 - kl_loss: 4.7448 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.6971 - reconstruction_loss: 1.7249 - kl_loss: 4.8046 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.6470 - reconstruction_loss: 1.6773 - kl_loss: 4.8150 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5892 - reconstruction_loss: 1.6114 - kl_loss: 4.8410 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5732 - reconstruction_loss: 1.6028 - kl_loss: 4.8584 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5531 - reconstruction_loss: 1.5853 - kl_loss: 4.8643 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5099 - reconstruction_loss: 1.5266 - kl_loss: 4.8731 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4924 - reconstruction_loss: 1.5235 - kl_loss: 4.8830 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5192 - reconstruction_loss: 1.5038 - kl_loss: 4.8932 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4542 - reconstruction_loss: 1.4862 - kl_loss: 4.8655 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4397 - reconstruction_loss: 1.4899 - kl_loss: 4.8764 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4628 - reconstruction_loss: 1.4826 - kl_loss: 4.8891 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4583 - reconstruction_loss: 1.4800 - kl_loss: 4.8803 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4569 - reconstruction_loss: 1.4639 - kl_loss: 4.8936 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4472 - reconstruction_loss: 1.4654 - kl_loss: 4.8732 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4544 - reconstruction_loss: 1.4679 - kl_loss: 4.8898 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4414 - reconstruction_loss: 1.4424 - kl_loss: 4.8686 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4031 - reconstruction_loss: 1.4389 - kl_loss: 4.8976 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4140 - reconstruction_loss: 1.4190 - kl_loss: 4.9568 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.4542 - reconstruction_loss: 1.4500 - kl_loss: 4.9520 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4381 - reconstruction_loss: 1.4951 - kl_loss: 5.0490 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4023 - reconstruction_loss: 1.3826 - kl_loss: 4.9619 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3435 - reconstruction_loss: 1.3747 - kl_loss: 4.8346 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.4428 - reconstruction_loss: 1.4279 - kl_loss: 5.0862 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.4047 - reconstruction_loss: 1.3607 - kl_loss: 4.9956 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2962 - reconstruction_loss: 1.3194 - kl_loss: 4.8167 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2660 - reconstruction_loss: 1.3174 - kl_loss: 4.7705 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2668 - reconstruction_loss: 1.3211 - kl_loss: 4.7815 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2600 - reconstruction_loss: 1.3149 - kl_loss: 4.7672 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2341 - reconstruction_loss: 1.2859 - kl_loss: 4.7404 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2434 - reconstruction_loss: 1.2961 - kl_loss: 4.7339 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2591 - reconstruction_loss: 1.3000 - kl_loss: 4.7386 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2315 - reconstruction_loss: 1.2846 - kl_loss: 4.7328 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2412 - reconstruction_loss: 1.3095 - kl_loss: 4.7399 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2806 - reconstruction_loss: 1.2927 - kl_loss: 4.7439 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2312 - reconstruction_loss: 1.3093 - kl_loss: 4.7508\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2314 - reconstruction_loss: 1.3085 - kl_loss: 4.7522 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2399 - reconstruction_loss: 1.2719 - kl_loss: 4.7499 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2041 - reconstruction_loss: 1.2601 - kl_loss: 4.7160 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2049 - reconstruction_loss: 1.2634 - kl_loss: 4.7166 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2065 - reconstruction_loss: 1.2621 - kl_loss: 4.7521 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2022 - reconstruction_loss: 1.2626 - kl_loss: 4.7142\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2022 - reconstruction_loss: 1.2626 - kl_loss: 4.7142 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1894 - reconstruction_loss: 1.2525 - kl_loss: 4.6959 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1971 - reconstruction_loss: 1.2506 - kl_loss: 4.7162 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1900 - reconstruction_loss: 1.2489 - kl_loss: 4.7014 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1891 - reconstruction_loss: 1.2525 - kl_loss: 4.6959 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2005 - reconstruction_loss: 1.2524 - kl_loss: 4.7115 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1876 - reconstruction_loss: 1.2484 - kl_loss: 4.6922 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1895 - reconstruction_loss: 1.2490 - kl_loss: 4.6903 - lr: 2.5000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1918 - reconstruction_loss: 1.2483 - kl_loss: 4.7023 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1812 - reconstruction_loss: 1.2490 - kl_loss: 4.6854 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.1891 - reconstruction_loss: 1.2476 - kl_loss: 4.6967 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1891 - reconstruction_loss: 1.2464 - kl_loss: 4.6911 - lr: 2.5000e-04\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1857 - reconstruction_loss: 1.2474 - kl_loss: 4.6761 - lr: 2.5000e-04\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1842 - reconstruction_loss: 1.2461 - kl_loss: 4.6911 - lr: 2.5000e-04\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1861 - reconstruction_loss: 1.2490 - kl_loss: 4.6884 - lr: 2.5000e-04\n",
      "Epoch 62/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.1903 - reconstruction_loss: 1.2490 - kl_loss: 4.6976\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1902 - reconstruction_loss: 1.2481 - kl_loss: 4.6977 - lr: 2.5000e-04\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1801 - reconstruction_loss: 1.2442 - kl_loss: 4.6825 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1777 - reconstruction_loss: 1.2388 - kl_loss: 4.6939 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1739 - reconstruction_loss: 1.2371 - kl_loss: 4.7002 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1809 - reconstruction_loss: 1.2403 - kl_loss: 4.6843 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1706 - reconstruction_loss: 1.2414 - kl_loss: 4.7027 - lr: 1.2500e-04\n",
      "Epoch 68/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.1870 - reconstruction_loss: 1.2440 - kl_loss: 4.6985\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1870 - reconstruction_loss: 1.2425 - kl_loss: 4.6988 - lr: 1.2500e-04\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1791 - reconstruction_loss: 1.2390 - kl_loss: 4.6876 - lr: 6.2500e-05\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1807 - reconstruction_loss: 1.2366 - kl_loss: 4.7119 - lr: 6.2500e-05\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1807 - reconstruction_loss: 1.2377 - kl_loss: 4.6865 - lr: 6.2500e-05\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1731 - reconstruction_loss: 1.2344 - kl_loss: 4.7049 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1750 - reconstruction_loss: 1.2365 - kl_loss: 4.6888 - lr: 6.2500e-05\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1805 - reconstruction_loss: 1.2409 - kl_loss: 4.6732 - lr: 6.2500e-05\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1790 - reconstruction_loss: 1.2370 - kl_loss: 4.6913 - lr: 6.2500e-05\n",
      "Epoch 76/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.1731 - reconstruction_loss: 1.2391 - kl_loss: 4.6924\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1731 - reconstruction_loss: 1.2380 - kl_loss: 4.6923 - lr: 6.2500e-05\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1779 - reconstruction_loss: 1.2354 - kl_loss: 4.6813 - lr: 3.1250e-05\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1773 - reconstruction_loss: 1.2368 - kl_loss: 4.6858 - lr: 3.1250e-05\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1732 - reconstruction_loss: 1.2355 - kl_loss: 4.6889 - lr: 3.1250e-05\n",
      "Epoch 80/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.1757 - reconstruction_loss: 1.2392 - kl_loss: 4.6862\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1757 - reconstruction_loss: 1.2381 - kl_loss: 4.6861 - lr: 3.1250e-05\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1794 - reconstruction_loss: 1.2374 - kl_loss: 4.6883 - lr: 1.5625e-05\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1728 - reconstruction_loss: 1.2367 - kl_loss: 4.6778 - lr: 1.5625e-05\n",
      "Epoch 83/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.1799 - reconstruction_loss: 1.2361 - kl_loss: 4.6967\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1798 - reconstruction_loss: 1.2351 - kl_loss: 4.6961 - lr: 1.5625e-05\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1705 - reconstruction_loss: 1.2361 - kl_loss: 4.6880 - lr: 7.8125e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1730 - reconstruction_loss: 1.2346 - kl_loss: 4.6940 - lr: 7.8125e-06\n",
      "Epoch 86/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.1763 - reconstruction_loss: 1.2342 - kl_loss: 4.6930\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1763 - reconstruction_loss: 1.2336 - kl_loss: 4.6935 - lr: 7.8125e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1713 - reconstruction_loss: 1.2337 - kl_loss: 4.6909 - lr: 3.9063e-06\n",
      "Epoch 87: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 09:40:45,885]\u001b[0m Trial 16 finished with value: 0.0048385550624663355 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 114, 'encoder_units_l2': 252, 'encoder_units_l3': 837, 'decoder_layers': 7, 'decoder_units_l1': 478, 'decoder_units_l2': 94, 'decoder_units_l3': 196, 'decoder_units_l4': 173, 'decoder_units_l5': 144, 'decoder_units_l6': 475, 'decoder_units_l7': 21, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 8ms/step - loss: 10.7349 - reconstruction_loss: 9.6682 - kl_loss: 0.1692 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0924 - reconstruction_loss: 8.3325 - kl_loss: 0.3488 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.1024 - reconstruction_loss: 8.3188 - kl_loss: 0.3533 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0758 - reconstruction_loss: 8.3207 - kl_loss: 0.3533 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0741 - reconstruction_loss: 8.3260 - kl_loss: 0.3495 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.1018 - reconstruction_loss: 8.2945 - kl_loss: 0.3572 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0472 - reconstruction_loss: 8.2880 - kl_loss: 0.3522 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0640 - reconstruction_loss: 8.3073 - kl_loss: 0.3531 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0403 - reconstruction_loss: 8.3071 - kl_loss: 0.3520 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 10.0678 - reconstruction_loss: 8.3049 - kl_loss: 0.3534\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0678 - reconstruction_loss: 8.2967 - kl_loss: 0.3534 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0162 - reconstruction_loss: 8.2682 - kl_loss: 0.3551 - lr: 5.0000e-04\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.0394 - reconstruction_loss: 8.2758 - kl_loss: 0.3519 - lr: 5.0000e-04\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.0605 - reconstruction_loss: 8.2838 - kl_loss: 0.3558 - lr: 5.0000e-04\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0555 - reconstruction_loss: 8.2900 - kl_loss: 0.3540 - lr: 5.0000e-04\n",
      "Epoch 15/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 10.0379 - reconstruction_loss: 8.2996 - kl_loss: 0.3532\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.0381 - reconstruction_loss: 8.2902 - kl_loss: 0.3535 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0484 - reconstruction_loss: 8.2551 - kl_loss: 0.3551 - lr: 2.5000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0610 - reconstruction_loss: 8.2802 - kl_loss: 0.3524 - lr: 2.5000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0560 - reconstruction_loss: 8.2632 - kl_loss: 0.3582 - lr: 2.5000e-04\n",
      "Epoch 19/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 10.0395 - reconstruction_loss: 8.2646 - kl_loss: 0.3585\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0396 - reconstruction_loss: 8.2561 - kl_loss: 0.3585 - lr: 2.5000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0585 - reconstruction_loss: 8.2744 - kl_loss: 0.3556 - lr: 1.2500e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0866 - reconstruction_loss: 8.2620 - kl_loss: 0.3533 - lr: 1.2500e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0571 - reconstruction_loss: 8.2517 - kl_loss: 0.3546 - lr: 1.2500e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0599 - reconstruction_loss: 8.2632 - kl_loss: 0.3531 - lr: 1.2500e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0397 - reconstruction_loss: 8.2760 - kl_loss: 0.3545 - lr: 1.2500e-04\n",
      "Epoch 25/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 10.0586 - reconstruction_loss: 8.2921 - kl_loss: 0.3560\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0587 - reconstruction_loss: 8.2801 - kl_loss: 0.3561 - lr: 1.2500e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0607 - reconstruction_loss: 8.2403 - kl_loss: 0.3579 - lr: 6.2500e-05\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0500 - reconstruction_loss: 8.2716 - kl_loss: 0.3551 - lr: 6.2500e-05\n",
      "Epoch 28/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 10.0606 - reconstruction_loss: 8.2662 - kl_loss: 0.3564\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.0604 - reconstruction_loss: 8.2582 - kl_loss: 0.3564 - lr: 6.2500e-05\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 10.0307 - reconstruction_loss: 8.2665 - kl_loss: 0.3543 - lr: 3.1250e-05\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0306 - reconstruction_loss: 8.2680 - kl_loss: 0.3527 - lr: 3.1250e-05\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 10.0582 - reconstruction_loss: 8.2494 - kl_loss: 0.3561\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0581 - reconstruction_loss: 8.2494 - kl_loss: 0.3561 - lr: 3.1250e-05\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0127 - reconstruction_loss: 8.2653 - kl_loss: 0.3544 - lr: 1.5625e-05\n",
      "Epoch 32: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 09:42:31,802]\u001b[0m Trial 17 finished with value: 0.03230399406762828 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 771, 'encoder_units_l2': 805, 'encoder_units_l3': 395, 'decoder_layers': 7, 'decoder_units_l1': 548, 'decoder_units_l2': 16, 'decoder_units_l3': 688, 'decoder_units_l4': 565, 'decoder_units_l5': 77, 'decoder_units_l6': 355, 'decoder_units_l7': 138, 'beta': 5, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 6.5258 - reconstruction_loss: 4.3881 - kl_loss: 3.1652 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.4262 - reconstruction_loss: 2.4065 - kl_loss: 4.1871 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.9987 - reconstruction_loss: 2.0835 - kl_loss: 4.4117 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.8180 - reconstruction_loss: 1.8780 - kl_loss: 4.5759 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.7139 - reconstruction_loss: 1.7831 - kl_loss: 4.7177 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.6454 - reconstruction_loss: 1.6427 - kl_loss: 4.8104 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5585 - reconstruction_loss: 1.5931 - kl_loss: 4.8637 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5063 - reconstruction_loss: 1.5229 - kl_loss: 4.8859 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4930 - reconstruction_loss: 1.5030 - kl_loss: 4.9110 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4219 - reconstruction_loss: 1.4593 - kl_loss: 4.8976 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4158 - reconstruction_loss: 1.4343 - kl_loss: 4.9086 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5078 - reconstruction_loss: 1.5010 - kl_loss: 4.9050 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5292 - reconstruction_loss: 1.4933 - kl_loss: 4.9476 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3990 - reconstruction_loss: 1.3949 - kl_loss: 4.8856 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4122 - reconstruction_loss: 1.5436 - kl_loss: 4.9254 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5367 - reconstruction_loss: 1.4976 - kl_loss: 5.0827 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.4509 - reconstruction_loss: 1.3929 - kl_loss: 5.0423\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4506 - reconstruction_loss: 1.3915 - kl_loss: 5.0416 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3313 - reconstruction_loss: 1.3343 - kl_loss: 4.9081 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2926 - reconstruction_loss: 1.3287 - kl_loss: 4.8643 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2936 - reconstruction_loss: 1.3291 - kl_loss: 4.8201 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3256 - reconstruction_loss: 1.3319 - kl_loss: 4.8121 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2681 - reconstruction_loss: 1.3059 - kl_loss: 4.7876 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2459 - reconstruction_loss: 1.2916 - kl_loss: 4.7704 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2513 - reconstruction_loss: 1.2988 - kl_loss: 4.7865 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2486 - reconstruction_loss: 1.2865 - kl_loss: 4.7570 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2268 - reconstruction_loss: 1.2757 - kl_loss: 4.7441 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2289 - reconstruction_loss: 1.2881 - kl_loss: 4.7495 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2462 - reconstruction_loss: 1.2922 - kl_loss: 4.7733 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2265 - reconstruction_loss: 1.2755 - kl_loss: 4.7317 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2231 - reconstruction_loss: 1.2770 - kl_loss: 4.7254 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2257 - reconstruction_loss: 1.2792 - kl_loss: 4.7434 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2135 - reconstruction_loss: 1.2726 - kl_loss: 4.6935 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2331 - reconstruction_loss: 1.2836 - kl_loss: 4.7223 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2176 - reconstruction_loss: 1.2778 - kl_loss: 4.7117 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.2133 - reconstruction_loss: 1.2746 - kl_loss: 4.7124\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2134 - reconstruction_loss: 1.2736 - kl_loss: 4.7124 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2000 - reconstruction_loss: 1.2548 - kl_loss: 4.7017 - lr: 2.5000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.1921 - reconstruction_loss: 1.2526 - kl_loss: 4.6824 - lr: 2.5000e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2013 - reconstruction_loss: 1.2525 - kl_loss: 4.6959 - lr: 2.5000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.1882 - reconstruction_loss: 1.2518 - kl_loss: 4.6818 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2090 - reconstruction_loss: 1.2602 - kl_loss: 4.6909 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.1853 - reconstruction_loss: 1.2497 - kl_loss: 4.6902 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.1886 - reconstruction_loss: 1.2561 - kl_loss: 4.6896 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2139 - reconstruction_loss: 1.2779 - kl_loss: 4.6990 - lr: 2.5000e-04\n",
      "Epoch 44/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2219 - reconstruction_loss: 1.2827 - kl_loss: 4.7097\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2219 - reconstruction_loss: 1.2818 - kl_loss: 4.7100 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2318 - reconstruction_loss: 1.2787 - kl_loss: 4.7104 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2253 - reconstruction_loss: 1.2688 - kl_loss: 4.7294 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2087 - reconstruction_loss: 1.2677 - kl_loss: 4.6901\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2087 - reconstruction_loss: 1.2665 - kl_loss: 4.6900 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2059 - reconstruction_loss: 1.2648 - kl_loss: 4.6829 - lr: 6.2500e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2012 - reconstruction_loss: 1.2569 - kl_loss: 4.6989 - lr: 6.2500e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1999 - reconstruction_loss: 1.2577 - kl_loss: 4.6787\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.1999 - reconstruction_loss: 1.2577 - kl_loss: 4.6787 - lr: 6.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.1851 - reconstruction_loss: 1.2553 - kl_loss: 4.6823 - lr: 3.1250e-05\n",
      "Epoch 51: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 09:44:51,892]\u001b[0m Trial 18 finished with value: 0.004908493321815964 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 115, 'encoder_units_l2': 264, 'encoder_units_l3': 665, 'encoder_units_l4': 309, 'decoder_layers': 7, 'decoder_units_l1': 19, 'decoder_units_l2': 168, 'decoder_units_l3': 280, 'decoder_units_l4': 47, 'decoder_units_l5': 245, 'decoder_units_l6': 278, 'decoder_units_l7': 48, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6723 - reconstruction_loss: 1.1079 - kl_loss: 2.0950 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3304 - reconstruction_loss: 0.7879 - kl_loss: 2.5693 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2682 - reconstruction_loss: 0.7343 - kl_loss: 2.6548 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2459 - reconstruction_loss: 0.7037 - kl_loss: 2.7032 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2342 - reconstruction_loss: 0.6827 - kl_loss: 2.7556 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "1563/1563 [==============================] - 5s 4ms/step - loss: 1.2391 - reconstruction_loss: 0.6747 - kl_loss: 2.7932 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "1563/1563 [==============================] - 5s 4ms/step - loss: 1.2238 - reconstruction_loss: 0.6555 - kl_loss: 2.8227 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2168 - reconstruction_loss: 0.6502 - kl_loss: 2.8386 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "1563/1563 [==============================] - 5s 4ms/step - loss: 1.2102 - reconstruction_loss: 0.6346 - kl_loss: 2.8725 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2107 - reconstruction_loss: 0.6330 - kl_loss: 2.8818 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2059 - reconstruction_loss: 0.6194 - kl_loss: 2.9299 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2069 - reconstruction_loss: 0.6163 - kl_loss: 2.9515 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "1563/1563 [==============================] - 5s 4ms/step - loss: 1.2089 - reconstruction_loss: 0.6189 - kl_loss: 2.9576 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "1563/1563 [==============================] - 5s 4ms/step - loss: 1.2026 - reconstruction_loss: 0.5989 - kl_loss: 3.0097 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1962 - reconstruction_loss: 0.5888 - kl_loss: 3.0331 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1999 - reconstruction_loss: 0.5772 - kl_loss: 3.0798 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1914 - reconstruction_loss: 0.5747 - kl_loss: 3.0795 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1895 - reconstruction_loss: 0.5713 - kl_loss: 3.0983 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "1563/1563 [==============================] - 5s 4ms/step - loss: 1.1873 - reconstruction_loss: 0.5714 - kl_loss: 3.1008 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "1551/1563 [============================>.] - ETA: 0s - loss: 1.1942 - reconstruction_loss: 0.5693 - kl_loss: 3.1258\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1942 - reconstruction_loss: 0.5690 - kl_loss: 3.1254 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1726 - reconstruction_loss: 0.5355 - kl_loss: 3.1666 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1681 - reconstruction_loss: 0.5298 - kl_loss: 3.1838 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1713 - reconstruction_loss: 0.5304 - kl_loss: 3.1852 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1637 - reconstruction_loss: 0.5239 - kl_loss: 3.2002 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1659 - reconstruction_loss: 0.5259 - kl_loss: 3.2031 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1627 - reconstruction_loss: 0.5198 - kl_loss: 3.2080 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1607 - reconstruction_loss: 0.5182 - kl_loss: 3.2158 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1655 - reconstruction_loss: 0.5231 - kl_loss: 3.2107 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "1550/1563 [============================>.] - ETA: 0s - loss: 1.1639 - reconstruction_loss: 0.5197 - kl_loss: 3.2155\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1639 - reconstruction_loss: 0.5197 - kl_loss: 3.2153 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1616 - reconstruction_loss: 0.5115 - kl_loss: 3.2349 - lr: 2.5000e-04\n",
      "Epoch 31/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1572 - reconstruction_loss: 0.5059 - kl_loss: 3.2431 - lr: 2.5000e-04\n",
      "Epoch 32/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1617 - reconstruction_loss: 0.5096 - kl_loss: 3.2482 - lr: 2.5000e-04\n",
      "Epoch 33/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1578 - reconstruction_loss: 0.5063 - kl_loss: 3.2535 - lr: 2.5000e-04\n",
      "Epoch 34/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1503 - reconstruction_loss: 0.5018 - kl_loss: 3.2506 - lr: 2.5000e-04\n",
      "Epoch 35/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1546 - reconstruction_loss: 0.5027 - kl_loss: 3.2540 - lr: 2.5000e-04\n",
      "Epoch 36/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1511 - reconstruction_loss: 0.5010 - kl_loss: 3.2522 - lr: 2.5000e-04\n",
      "Epoch 37/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1492 - reconstruction_loss: 0.5005 - kl_loss: 3.2519 - lr: 2.5000e-04\n",
      "Epoch 38/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1532 - reconstruction_loss: 0.5001 - kl_loss: 3.2605 - lr: 2.5000e-04\n",
      "Epoch 39/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1518 - reconstruction_loss: 0.4983 - kl_loss: 3.2624 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "1557/1563 [============================>.] - ETA: 0s - loss: 1.1529 - reconstruction_loss: 0.5014 - kl_loss: 3.2512\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1529 - reconstruction_loss: 0.5012 - kl_loss: 3.2512 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1496 - reconstruction_loss: 0.4949 - kl_loss: 3.2694 - lr: 1.2500e-04\n",
      "Epoch 42/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1459 - reconstruction_loss: 0.4946 - kl_loss: 3.2659 - lr: 1.2500e-04\n",
      "Epoch 43/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1503 - reconstruction_loss: 0.4952 - kl_loss: 3.2627 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1459 - reconstruction_loss: 0.4920 - kl_loss: 3.2697 - lr: 1.2500e-04\n",
      "Epoch 45/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1435 - reconstruction_loss: 0.4916 - kl_loss: 3.2662 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1478 - reconstruction_loss: 0.4919 - kl_loss: 3.2654 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1470 - reconstruction_loss: 0.4935 - kl_loss: 3.2572 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1458 - reconstruction_loss: 0.4930 - kl_loss: 3.2530 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1441 - reconstruction_loss: 0.4910 - kl_loss: 3.2607 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1427 - reconstruction_loss: 0.4912 - kl_loss: 3.2617 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1427 - reconstruction_loss: 0.4909 - kl_loss: 3.2660 - lr: 1.2500e-04\n",
      "Epoch 52/300\n",
      "1559/1563 [============================>.] - ETA: 0s - loss: 1.1439 - reconstruction_loss: 0.4916 - kl_loss: 3.2651\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1439 - reconstruction_loss: 0.4914 - kl_loss: 3.2654 - lr: 1.2500e-04\n",
      "Epoch 53/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1417 - reconstruction_loss: 0.4870 - kl_loss: 3.2698 - lr: 6.2500e-05\n",
      "Epoch 54/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1422 - reconstruction_loss: 0.4889 - kl_loss: 3.2690 - lr: 6.2500e-05\n",
      "Epoch 55/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1411 - reconstruction_loss: 0.4871 - kl_loss: 3.2641 - lr: 6.2500e-05\n",
      "Epoch 56/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1396 - reconstruction_loss: 0.4883 - kl_loss: 3.2670 - lr: 6.2500e-05\n",
      "Epoch 57/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1410 - reconstruction_loss: 0.4880 - kl_loss: 3.2629 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1404 - reconstruction_loss: 0.4879 - kl_loss: 3.2591 - lr: 6.2500e-05\n",
      "Epoch 59/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1403 - reconstruction_loss: 0.4875 - kl_loss: 3.2659 - lr: 6.2500e-05\n",
      "Epoch 60/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1413 - reconstruction_loss: 0.4880 - kl_loss: 3.2611 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 1.1404 - reconstruction_loss: 0.4874 - kl_loss: 3.2628\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1404 - reconstruction_loss: 0.4872 - kl_loss: 3.2626 - lr: 6.2500e-05\n",
      "Epoch 62/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1362 - reconstruction_loss: 0.4862 - kl_loss: 3.2609 - lr: 3.1250e-05\n",
      "Epoch 63/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1410 - reconstruction_loss: 0.4876 - kl_loss: 3.2619 - lr: 3.1250e-05\n",
      "Epoch 64/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1396 - reconstruction_loss: 0.4867 - kl_loss: 3.2661 - lr: 3.1250e-05\n",
      "Epoch 65/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 1.1396 - reconstruction_loss: 0.4866 - kl_loss: 3.2624\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1396 - reconstruction_loss: 0.4864 - kl_loss: 3.2625 - lr: 3.1250e-05\n",
      "Epoch 66/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1426 - reconstruction_loss: 0.4872 - kl_loss: 3.2622 - lr: 1.5625e-05\n",
      "Epoch 67/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1376 - reconstruction_loss: 0.4862 - kl_loss: 3.2606 - lr: 1.5625e-05\n",
      "Epoch 68/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1383 - reconstruction_loss: 0.4846 - kl_loss: 3.2667 - lr: 1.5625e-05\n",
      "Epoch 69/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1411 - reconstruction_loss: 0.4856 - kl_loss: 3.2701 - lr: 1.5625e-05\n",
      "Epoch 70/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1367 - reconstruction_loss: 0.4853 - kl_loss: 3.2595 - lr: 1.5625e-05\n",
      "Epoch 71/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1382 - reconstruction_loss: 0.4840 - kl_loss: 3.2669 - lr: 1.5625e-05\n",
      "Epoch 72/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1392 - reconstruction_loss: 0.4856 - kl_loss: 3.2592 - lr: 1.5625e-05\n",
      "Epoch 73/300\n",
      "1553/1563 [============================>.] - ETA: 0s - loss: 1.1375 - reconstruction_loss: 0.4849 - kl_loss: 3.2683\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1375 - reconstruction_loss: 0.4848 - kl_loss: 3.2683 - lr: 1.5625e-05\n",
      "Epoch 74/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1385 - reconstruction_loss: 0.4836 - kl_loss: 3.2747 - lr: 7.8125e-06\n",
      "Epoch 75/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1363 - reconstruction_loss: 0.4831 - kl_loss: 3.2698 - lr: 7.8125e-06\n",
      "Epoch 76/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1381 - reconstruction_loss: 0.4839 - kl_loss: 3.2605 - lr: 7.8125e-06\n",
      "Epoch 77/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1359 - reconstruction_loss: 0.4853 - kl_loss: 3.2639 - lr: 7.8125e-06\n",
      "Epoch 78/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1373 - reconstruction_loss: 0.4843 - kl_loss: 3.2633 - lr: 7.8125e-06\n",
      "Epoch 79/300\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 1.1396 - reconstruction_loss: 0.4844 - kl_loss: 3.2695\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1396 - reconstruction_loss: 0.4844 - kl_loss: 3.2695 - lr: 7.8125e-06\n",
      "Epoch 80/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1378 - reconstruction_loss: 0.4852 - kl_loss: 3.2651 - lr: 3.9063e-06\n",
      "Epoch 81/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1385 - reconstruction_loss: 0.4845 - kl_loss: 3.2685 - lr: 3.9063e-06\n",
      "Epoch 82/300\n",
      "1552/1563 [============================>.] - ETA: 0s - loss: 1.1385 - reconstruction_loss: 0.4842 - kl_loss: 3.2686\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1385 - reconstruction_loss: 0.4843 - kl_loss: 3.2690 - lr: 3.9063e-06\n",
      "Epoch 83/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1380 - reconstruction_loss: 0.4833 - kl_loss: 3.2693 - lr: 1.9531e-06\n",
      "Epoch 84/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1369 - reconstruction_loss: 0.4844 - kl_loss: 3.2680 - lr: 1.9531e-06\n",
      "Epoch 85/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 1.1367 - reconstruction_loss: 0.4836 - kl_loss: 3.2697\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1367 - reconstruction_loss: 0.4834 - kl_loss: 3.2697 - lr: 1.9531e-06\n",
      "Epoch 86/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1369 - reconstruction_loss: 0.4842 - kl_loss: 3.2693 - lr: 1.0000e-06\n",
      "Epoch 86: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 09:52:59,779]\u001b[0m Trial 19 finished with value: 0.007558081976507837 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 462, 'encoder_units_l2': 649, 'encoder_units_l3': 1020, 'decoder_layers': 7, 'decoder_units_l1': 436, 'decoder_units_l2': 78, 'decoder_units_l3': 303, 'decoder_units_l4': 166, 'decoder_units_l5': 101, 'decoder_units_l6': 107, 'decoder_units_l7': 31, 'beta': 0.2, 'lr': 0.001, 'batch_size': 64}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 19.0151 - reconstruction_loss: 15.0266 - kl_loss: 0.6139 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.7318 - reconstruction_loss: 14.2271 - kl_loss: 0.6797 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.6281 - reconstruction_loss: 14.2267 - kl_loss: 0.6777 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.6897 - reconstruction_loss: 14.2104 - kl_loss: 0.6789 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.7111 - reconstruction_loss: 14.1890 - kl_loss: 0.6847 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.6171 - reconstruction_loss: 14.1511 - kl_loss: 0.6839 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.6696 - reconstruction_loss: 14.1672 - kl_loss: 0.6817 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.6569 - reconstruction_loss: 14.1673 - kl_loss: 0.6860 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 17.6702 - reconstruction_loss: 14.1620 - kl_loss: 0.6835\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.6698 - reconstruction_loss: 14.1620 - kl_loss: 0.6835 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.5804 - reconstruction_loss: 14.1124 - kl_loss: 0.6867 - lr: 5.0000e-04\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.5975 - reconstruction_loss: 14.1061 - kl_loss: 0.6897 - lr: 5.0000e-04\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.6220 - reconstruction_loss: 14.1025 - kl_loss: 0.6898 - lr: 5.0000e-04\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.5668 - reconstruction_loss: 14.0925 - kl_loss: 0.6886 - lr: 5.0000e-04\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.6186 - reconstruction_loss: 14.1019 - kl_loss: 0.6862 - lr: 5.0000e-04\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.5743 - reconstruction_loss: 14.0732 - kl_loss: 0.6898 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.6333 - reconstruction_loss: 14.0627 - kl_loss: 0.6925 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.6195 - reconstruction_loss: 14.0231 - kl_loss: 0.7019 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.5972 - reconstruction_loss: 13.8603 - kl_loss: 0.7240 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.4104 - reconstruction_loss: 13.0426 - kl_loss: 0.8485 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.1419 - reconstruction_loss: 12.4594 - kl_loss: 0.9225 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.1643 - reconstruction_loss: 12.3120 - kl_loss: 0.9484 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 17.0380 - reconstruction_loss: 12.3961 - kl_loss: 0.9413 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.2452 - reconstruction_loss: 12.3268 - kl_loss: 0.9660 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 17.1526 - reconstruction_loss: 12.2563 - kl_loss: 0.9648\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.1523 - reconstruction_loss: 12.2563 - kl_loss: 0.9648 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.0323 - reconstruction_loss: 12.2184 - kl_loss: 0.9552 - lr: 2.5000e-04\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.0169 - reconstruction_loss: 12.1314 - kl_loss: 0.9712 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.0372 - reconstruction_loss: 12.0636 - kl_loss: 0.9798 - lr: 2.5000e-04\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.0031 - reconstruction_loss: 12.0628 - kl_loss: 0.9783 - lr: 2.5000e-04\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 17.0177 - reconstruction_loss: 12.0526 - kl_loss: 0.9801 - lr: 2.5000e-04\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 17.0128 - reconstruction_loss: 12.0594 - kl_loss: 0.9751 - lr: 2.5000e-04\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 17.0028 - reconstruction_loss: 12.0561 - kl_loss: 0.9786 - lr: 2.5000e-04\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 16.9471 - reconstruction_loss: 11.9947 - kl_loss: 0.9878 - lr: 2.5000e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.0317 - reconstruction_loss: 11.9692 - kl_loss: 1.0018 - lr: 2.5000e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.0401 - reconstruction_loss: 11.9522 - kl_loss: 1.0020 - lr: 2.5000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 17.2034 - reconstruction_loss: 11.9963 - kl_loss: 1.0575\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 17.2038 - reconstruction_loss: 11.9963 - kl_loss: 1.0575 - lr: 2.5000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.0386 - reconstruction_loss: 11.9447 - kl_loss: 1.0083 - lr: 1.2500e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.9543 - reconstruction_loss: 11.8504 - kl_loss: 1.0249 - lr: 1.2500e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 17.0364 - reconstruction_loss: 11.8087 - kl_loss: 1.0294\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 17.0360 - reconstruction_loss: 11.8087 - kl_loss: 1.0294 - lr: 1.2500e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 16.9667 - reconstruction_loss: 11.6934 - kl_loss: 1.0445 - lr: 6.2500e-05\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.9309 - reconstruction_loss: 11.6825 - kl_loss: 1.0349 - lr: 6.2500e-05\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.8605 - reconstruction_loss: 11.6077 - kl_loss: 1.0455 - lr: 6.2500e-05\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.9138 - reconstruction_loss: 11.3738 - kl_loss: 1.0907 - lr: 6.2500e-05\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.8339 - reconstruction_loss: 11.3374 - kl_loss: 1.0879 - lr: 6.2500e-05\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.8090 - reconstruction_loss: 11.2514 - kl_loss: 1.0997 - lr: 6.2500e-05\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.7826 - reconstruction_loss: 11.2378 - kl_loss: 1.1040 - lr: 6.2500e-05\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.7120 - reconstruction_loss: 11.1223 - kl_loss: 1.1197 - lr: 6.2500e-05\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 16.7634 - reconstruction_loss: 11.1149 - kl_loss: 1.1220 - lr: 6.2500e-05\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 16.7521 - reconstruction_loss: 11.0388 - kl_loss: 1.1329 - lr: 6.2500e-05\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 16.7619 - reconstruction_loss: 11.0301 - kl_loss: 1.1311 - lr: 6.2500e-05\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.7568 - reconstruction_loss: 10.9665 - kl_loss: 1.1456 - lr: 6.2500e-05\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.7475 - reconstruction_loss: 10.9278 - kl_loss: 1.1489 - lr: 6.2500e-05\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.7651 - reconstruction_loss: 10.8968 - kl_loss: 1.1606 - lr: 6.2500e-05\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.7032 - reconstruction_loss: 10.9165 - kl_loss: 1.1561 - lr: 6.2500e-05\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.7179 - reconstruction_loss: 10.8740 - kl_loss: 1.1589 - lr: 6.2500e-05\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.7011 - reconstruction_loss: 10.8297 - kl_loss: 1.1686 - lr: 6.2500e-05\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.7106 - reconstruction_loss: 10.7734 - kl_loss: 1.1781 - lr: 6.2500e-05\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6620 - reconstruction_loss: 10.7021 - kl_loss: 1.1887 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6868 - reconstruction_loss: 10.7026 - kl_loss: 1.1901 - lr: 6.2500e-05\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 16.7266 - reconstruction_loss: 10.7990 - kl_loss: 1.1771 - lr: 6.2500e-05\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 16.7140 - reconstruction_loss: 10.7984 - kl_loss: 1.1720\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.7137 - reconstruction_loss: 10.7984 - kl_loss: 1.1720 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6885 - reconstruction_loss: 10.7467 - kl_loss: 1.1756 - lr: 3.1250e-05\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6409 - reconstruction_loss: 10.7678 - kl_loss: 1.1732 - lr: 3.1250e-05\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.5881 - reconstruction_loss: 10.7154 - kl_loss: 1.1817 - lr: 3.1250e-05\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6512 - reconstruction_loss: 10.6260 - kl_loss: 1.1989 - lr: 3.1250e-05\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6161 - reconstruction_loss: 10.6222 - kl_loss: 1.1979 - lr: 3.1250e-05\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 16.6488 - reconstruction_loss: 10.5923 - kl_loss: 1.2075 - lr: 3.1250e-05\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6117 - reconstruction_loss: 10.5782 - kl_loss: 1.2054 - lr: 3.1250e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6424 - reconstruction_loss: 10.5631 - kl_loss: 1.2128 - lr: 3.1250e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 16.6362 - reconstruction_loss: 10.5163 - kl_loss: 1.2145 - lr: 3.1250e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 16.6229 - reconstruction_loss: 10.5743 - kl_loss: 1.2083 - lr: 3.1250e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 16.6117 - reconstruction_loss: 10.5877 - kl_loss: 1.2030 - lr: 3.1250e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 16.6479 - reconstruction_loss: 10.5896 - kl_loss: 1.2059\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6477 - reconstruction_loss: 10.5896 - kl_loss: 1.2059 - lr: 3.1250e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6398 - reconstruction_loss: 10.5665 - kl_loss: 1.2102 - lr: 1.5625e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6350 - reconstruction_loss: 10.5579 - kl_loss: 1.2134 - lr: 1.5625e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 16.6515 - reconstruction_loss: 10.5380 - kl_loss: 1.2135\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6513 - reconstruction_loss: 10.5380 - kl_loss: 1.2135 - lr: 1.5625e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6444 - reconstruction_loss: 10.5297 - kl_loss: 1.2146 - lr: 7.8125e-06\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.5629 - reconstruction_loss: 10.5170 - kl_loss: 1.2100 - lr: 7.8125e-06\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6043 - reconstruction_loss: 10.5063 - kl_loss: 1.2164 - lr: 7.8125e-06\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6338 - reconstruction_loss: 10.5000 - kl_loss: 1.2158 - lr: 7.8125e-06\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 16.6061 - reconstruction_loss: 10.4910 - kl_loss: 1.2205\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6061 - reconstruction_loss: 10.4910 - kl_loss: 1.2205 - lr: 7.8125e-06\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.5853 - reconstruction_loss: 10.4852 - kl_loss: 1.2220 - lr: 3.9063e-06\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6001 - reconstruction_loss: 10.4847 - kl_loss: 1.2226 - lr: 3.9063e-06\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 16.6369 - reconstruction_loss: 10.4855 - kl_loss: 1.2194\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6367 - reconstruction_loss: 10.4855 - kl_loss: 1.2194 - lr: 3.9063e-06\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6231 - reconstruction_loss: 10.4857 - kl_loss: 1.2252 - lr: 1.9531e-06\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6289 - reconstruction_loss: 10.4642 - kl_loss: 1.2235 - lr: 1.9531e-06\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 16.6423 - reconstruction_loss: 10.4654 - kl_loss: 1.2251\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6420 - reconstruction_loss: 10.4654 - kl_loss: 1.2251 - lr: 1.9531e-06\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 16.6273 - reconstruction_loss: 10.4746 - kl_loss: 1.2247 - lr: 1.0000e-06\n",
      "Epoch 87: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 09:59:26,637]\u001b[0m Trial 20 finished with value: 0.020474406557536996 and parameters: {'encoder_layers': 8, 'encoder_units_l1': 36, 'encoder_units_l2': 277, 'encoder_units_l3': 319, 'encoder_units_l4': 135, 'encoder_units_l5': 737, 'encoder_units_l6': 329, 'encoder_units_l7': 976, 'encoder_units_l8': 22, 'decoder_layers': 5, 'decoder_units_l1': 1011, 'decoder_units_l2': 144, 'decoder_units_l3': 484, 'decoder_units_l4': 563, 'decoder_units_l5': 51, 'beta': 5, 'lr': 0.001, 'batch_size': 512}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 5.9581 - reconstruction_loss: 4.0454 - kl_loss: 3.4722 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 3.4868 - reconstruction_loss: 2.4896 - kl_loss: 4.2825 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 3.0936 - reconstruction_loss: 2.1662 - kl_loss: 4.4089 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.9489 - reconstruction_loss: 2.0293 - kl_loss: 4.4862 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.8719 - reconstruction_loss: 1.9390 - kl_loss: 4.5814 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.7549 - reconstruction_loss: 1.8127 - kl_loss: 4.6822 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.7444 - reconstruction_loss: 1.7737 - kl_loss: 4.7413 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6758 - reconstruction_loss: 1.7347 - kl_loss: 4.7803 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6464 - reconstruction_loss: 1.6779 - kl_loss: 4.8363 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.7142 - reconstruction_loss: 1.6600 - kl_loss: 4.8737 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6128 - reconstruction_loss: 1.6273 - kl_loss: 4.8798 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5764 - reconstruction_loss: 1.6149 - kl_loss: 4.8850 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5514 - reconstruction_loss: 1.5740 - kl_loss: 4.9190 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5386 - reconstruction_loss: 1.5527 - kl_loss: 4.9279 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5543 - reconstruction_loss: 1.5768 - kl_loss: 4.9436 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5113 - reconstruction_loss: 1.5102 - kl_loss: 4.9454 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4899 - reconstruction_loss: 1.4991 - kl_loss: 4.9415 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4447 - reconstruction_loss: 1.4898 - kl_loss: 4.9163 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4540 - reconstruction_loss: 1.4481 - kl_loss: 4.9263 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4209 - reconstruction_loss: 1.4479 - kl_loss: 4.9530 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4601 - reconstruction_loss: 1.4589 - kl_loss: 4.9227 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.4362 - reconstruction_loss: 1.4662 - kl_loss: 4.9354\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4364 - reconstruction_loss: 1.4644 - kl_loss: 4.9355 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3774 - reconstruction_loss: 1.3807 - kl_loss: 4.9094 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3576 - reconstruction_loss: 1.3634 - kl_loss: 4.8962 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3288 - reconstruction_loss: 1.3510 - kl_loss: 4.9059 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3470 - reconstruction_loss: 1.3629 - kl_loss: 4.9376 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3234 - reconstruction_loss: 1.3529 - kl_loss: 4.8986 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3311 - reconstruction_loss: 1.3452 - kl_loss: 4.9032 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3086 - reconstruction_loss: 1.3361 - kl_loss: 4.8894 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3331 - reconstruction_loss: 1.3451 - kl_loss: 4.8745 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3064 - reconstruction_loss: 1.3325 - kl_loss: 4.8669 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3138 - reconstruction_loss: 1.3415 - kl_loss: 4.8720 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2997 - reconstruction_loss: 1.3275 - kl_loss: 4.8664 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3027 - reconstruction_loss: 1.3234 - kl_loss: 4.8677 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2875 - reconstruction_loss: 1.3193 - kl_loss: 4.8447 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2922 - reconstruction_loss: 1.3231 - kl_loss: 4.8583 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2923 - reconstruction_loss: 1.3222 - kl_loss: 4.8389 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2999 - reconstruction_loss: 1.3196 - kl_loss: 4.8517\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2998 - reconstruction_loss: 1.3186 - kl_loss: 4.8515 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2691 - reconstruction_loss: 1.2917 - kl_loss: 4.8234 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2584 - reconstruction_loss: 1.2945 - kl_loss: 4.8221 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2566 - reconstruction_loss: 1.2926 - kl_loss: 4.8247 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2556 - reconstruction_loss: 1.2910 - kl_loss: 4.8187 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2480 - reconstruction_loss: 1.2835 - kl_loss: 4.8062 - lr: 2.5000e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2679 - reconstruction_loss: 1.3063 - kl_loss: 4.8191 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2553 - reconstruction_loss: 1.2955 - kl_loss: 4.8058 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2586 - reconstruction_loss: 1.2966 - kl_loss: 4.8069\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2586 - reconstruction_loss: 1.2950 - kl_loss: 4.8071 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2465 - reconstruction_loss: 1.2805 - kl_loss: 4.8121 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2408 - reconstruction_loss: 1.2843 - kl_loss: 4.7996 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2430 - reconstruction_loss: 1.2784 - kl_loss: 4.7822 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2338 - reconstruction_loss: 1.2732 - kl_loss: 4.7906 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2400 - reconstruction_loss: 1.2722 - kl_loss: 4.8111 - lr: 1.2500e-04\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2309 - reconstruction_loss: 1.2706 - kl_loss: 4.7903 - lr: 1.2500e-04\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2348 - reconstruction_loss: 1.2750 - kl_loss: 4.7942 - lr: 1.2500e-04\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2342 - reconstruction_loss: 1.2736 - kl_loss: 4.7828 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2356 - reconstruction_loss: 1.2816 - kl_loss: 4.7905\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2356 - reconstruction_loss: 1.2803 - kl_loss: 4.7904 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2250 - reconstruction_loss: 1.2659 - kl_loss: 4.7904 - lr: 6.2500e-05\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2227 - reconstruction_loss: 1.2641 - kl_loss: 4.7991 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2291 - reconstruction_loss: 1.2633 - kl_loss: 4.7967 - lr: 6.2500e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2245 - reconstruction_loss: 1.2637 - kl_loss: 4.7773 - lr: 6.2500e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2252 - reconstruction_loss: 1.2624 - kl_loss: 4.7892 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2198 - reconstruction_loss: 1.2640 - kl_loss: 4.7851 - lr: 6.2500e-05\n",
      "Epoch 62/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2144 - reconstruction_loss: 1.2647 - kl_loss: 4.7841\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2144 - reconstruction_loss: 1.2633 - kl_loss: 4.7841 - lr: 6.2500e-05\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2174 - reconstruction_loss: 1.2573 - kl_loss: 4.7929 - lr: 3.1250e-05\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2165 - reconstruction_loss: 1.2639 - kl_loss: 4.7746 - lr: 3.1250e-05\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2236 - reconstruction_loss: 1.2610 - kl_loss: 4.7918 - lr: 3.1250e-05\n",
      "Epoch 66/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2172 - reconstruction_loss: 1.2605 - kl_loss: 4.7835\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2172 - reconstruction_loss: 1.2591 - kl_loss: 4.7834 - lr: 3.1250e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2171 - reconstruction_loss: 1.2585 - kl_loss: 4.7862 - lr: 1.5625e-05\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2156 - reconstruction_loss: 1.2568 - kl_loss: 4.7905 - lr: 1.5625e-05\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2111 - reconstruction_loss: 1.2567 - kl_loss: 4.7873 - lr: 1.5625e-05\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2192 - reconstruction_loss: 1.2564 - kl_loss: 4.7751 - lr: 1.5625e-05\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2192 - reconstruction_loss: 1.2571 - kl_loss: 4.7839 - lr: 1.5625e-05\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2101 - reconstruction_loss: 1.2553 - kl_loss: 4.7774 - lr: 1.5625e-05\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2095 - reconstruction_loss: 1.2574 - kl_loss: 4.7765 - lr: 1.5625e-05\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2151 - reconstruction_loss: 1.2576 - kl_loss: 4.7671 - lr: 1.5625e-05\n",
      "Epoch 75/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2189 - reconstruction_loss: 1.2591 - kl_loss: 4.7754\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2188 - reconstruction_loss: 1.2575 - kl_loss: 4.7754 - lr: 1.5625e-05\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2089 - reconstruction_loss: 1.2559 - kl_loss: 4.7735 - lr: 7.8125e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2051 - reconstruction_loss: 1.2535 - kl_loss: 4.7745 - lr: 7.8125e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2116 - reconstruction_loss: 1.2553 - kl_loss: 4.7705 - lr: 7.8125e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2072 - reconstruction_loss: 1.2540 - kl_loss: 4.7834 - lr: 7.8125e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2099 - reconstruction_loss: 1.2518 - kl_loss: 4.7813 - lr: 7.8125e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2166 - reconstruction_loss: 1.2547 - kl_loss: 4.7757 - lr: 7.8125e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2120 - reconstruction_loss: 1.2542 - kl_loss: 4.7770 - lr: 7.8125e-06\n",
      "Epoch 83/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.2122 - reconstruction_loss: 1.2551 - kl_loss: 4.7814\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2122 - reconstruction_loss: 1.2539 - kl_loss: 4.7818 - lr: 7.8125e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2146 - reconstruction_loss: 1.2501 - kl_loss: 4.7950 - lr: 3.9063e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2046 - reconstruction_loss: 1.2516 - kl_loss: 4.7783 - lr: 3.9063e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2008 - reconstruction_loss: 1.2511 - kl_loss: 4.7750 - lr: 3.9063e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2113 - reconstruction_loss: 1.2544 - kl_loss: 4.7764 - lr: 3.9063e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2118 - reconstruction_loss: 1.2519 - kl_loss: 4.7735 - lr: 3.9063e-06\n",
      "Epoch 89/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2117 - reconstruction_loss: 1.2529 - kl_loss: 4.7740\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2117 - reconstruction_loss: 1.2519 - kl_loss: 4.7738 - lr: 3.9063e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2055 - reconstruction_loss: 1.2532 - kl_loss: 4.7689 - lr: 1.9531e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2055 - reconstruction_loss: 1.2542 - kl_loss: 4.7706 - lr: 1.9531e-06\n",
      "Epoch 92/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2071 - reconstruction_loss: 1.2540 - kl_loss: 4.7747\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2071 - reconstruction_loss: 1.2526 - kl_loss: 4.7744 - lr: 1.9531e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2057 - reconstruction_loss: 1.2530 - kl_loss: 4.7702 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2092 - reconstruction_loss: 1.2542 - kl_loss: 4.7726 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2110 - reconstruction_loss: 1.2526 - kl_loss: 4.7731 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2092 - reconstruction_loss: 1.2552 - kl_loss: 4.7727 - lr: 1.0000e-06\n",
      "Epoch 96: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 10:05:44,340]\u001b[0m Trial 21 finished with value: 0.004894498030578038 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 114, 'encoder_units_l2': 262, 'encoder_units_l3': 713, 'decoder_layers': 7, 'decoder_units_l1': 420, 'decoder_units_l2': 105, 'decoder_units_l3': 191, 'decoder_units_l4': 216, 'decoder_units_l5': 149, 'decoder_units_l6': 494, 'decoder_units_l7': 17, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 8ms/step - loss: 6.4018 - reconstruction_loss: 4.4582 - kl_loss: 3.1285 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.4364 - reconstruction_loss: 2.4157 - kl_loss: 4.1778 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.0144 - reconstruction_loss: 2.0943 - kl_loss: 4.4107 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.8497 - reconstruction_loss: 1.9226 - kl_loss: 4.5628 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.7698 - reconstruction_loss: 1.7760 - kl_loss: 4.7882 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.6999 - reconstruction_loss: 1.7164 - kl_loss: 4.8443 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5893 - reconstruction_loss: 1.5893 - kl_loss: 4.9058 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5794 - reconstruction_loss: 1.6174 - kl_loss: 4.9373 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5021 - reconstruction_loss: 1.5139 - kl_loss: 4.9481 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4763 - reconstruction_loss: 1.5189 - kl_loss: 4.9563 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4840 - reconstruction_loss: 1.4843 - kl_loss: 4.9298 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4822 - reconstruction_loss: 1.4925 - kl_loss: 4.9380 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5080 - reconstruction_loss: 1.4838 - kl_loss: 4.9669 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4097 - reconstruction_loss: 1.4499 - kl_loss: 4.9072 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4517 - reconstruction_loss: 1.4577 - kl_loss: 4.9342 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4535 - reconstruction_loss: 1.4794 - kl_loss: 4.9447 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.4126 - reconstruction_loss: 1.4789 - kl_loss: 4.9783\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4137 - reconstruction_loss: 1.4798 - kl_loss: 4.9834 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4320 - reconstruction_loss: 1.4198 - kl_loss: 4.9285 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3411 - reconstruction_loss: 1.3710 - kl_loss: 4.8714 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3229 - reconstruction_loss: 1.3409 - kl_loss: 4.8671 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3136 - reconstruction_loss: 1.3407 - kl_loss: 4.9106 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3548 - reconstruction_loss: 1.3844 - kl_loss: 4.9273 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.4540 - reconstruction_loss: 1.4635 - kl_loss: 5.0072\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4540 - reconstruction_loss: 1.4620 - kl_loss: 5.0070 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3530 - reconstruction_loss: 1.3584 - kl_loss: 4.9292 - lr: 2.5000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3303 - reconstruction_loss: 1.3399 - kl_loss: 4.9532 - lr: 2.5000e-04\n",
      "Epoch 26/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.3264 - reconstruction_loss: 1.3350 - kl_loss: 4.9082\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3262 - reconstruction_loss: 1.3329 - kl_loss: 4.9082 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2817 - reconstruction_loss: 1.3064 - kl_loss: 4.8665 - lr: 1.2500e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2825 - reconstruction_loss: 1.3043 - kl_loss: 4.8572 - lr: 1.2500e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2732 - reconstruction_loss: 1.3023 - kl_loss: 4.8697 - lr: 1.2500e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2722 - reconstruction_loss: 1.2965 - kl_loss: 4.8534 - lr: 1.2500e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2599 - reconstruction_loss: 1.2929 - kl_loss: 4.8387 - lr: 1.2500e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2617 - reconstruction_loss: 1.2946 - kl_loss: 4.8417 - lr: 1.2500e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2576 - reconstruction_loss: 1.2878 - kl_loss: 4.8490 - lr: 1.2500e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2500 - reconstruction_loss: 1.2819 - kl_loss: 4.8420 - lr: 1.2500e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2587 - reconstruction_loss: 1.2854 - kl_loss: 4.8376 - lr: 1.2500e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2545 - reconstruction_loss: 1.2865 - kl_loss: 4.8451 - lr: 1.2500e-04\n",
      "Epoch 37/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2529 - reconstruction_loss: 1.2912 - kl_loss: 4.8368\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2530 - reconstruction_loss: 1.2897 - kl_loss: 4.8363 - lr: 1.2500e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2455 - reconstruction_loss: 1.2798 - kl_loss: 4.8205 - lr: 6.2500e-05\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2490 - reconstruction_loss: 1.2795 - kl_loss: 4.8257 - lr: 6.2500e-05\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2456 - reconstruction_loss: 1.2759 - kl_loss: 4.8377 - lr: 6.2500e-05\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2444 - reconstruction_loss: 1.2784 - kl_loss: 4.8459 - lr: 6.2500e-05\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2484 - reconstruction_loss: 1.2766 - kl_loss: 4.8269 - lr: 6.2500e-05\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2472 - reconstruction_loss: 1.2779 - kl_loss: 4.8351 - lr: 6.2500e-05\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2476 - reconstruction_loss: 1.2805 - kl_loss: 4.8434 - lr: 6.2500e-05\n",
      "Epoch 45/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2485 - reconstruction_loss: 1.2795 - kl_loss: 4.8423\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2485 - reconstruction_loss: 1.2785 - kl_loss: 4.8428 - lr: 6.2500e-05\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2411 - reconstruction_loss: 1.2742 - kl_loss: 4.8329 - lr: 3.1250e-05\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2362 - reconstruction_loss: 1.2738 - kl_loss: 4.8275 - lr: 3.1250e-05\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2348 - reconstruction_loss: 1.2716 - kl_loss: 4.8300 - lr: 3.1250e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2421 - reconstruction_loss: 1.2710 - kl_loss: 4.8296 - lr: 3.1250e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2388 - reconstruction_loss: 1.2714 - kl_loss: 4.8304 - lr: 3.1250e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2380 - reconstruction_loss: 1.2748 - kl_loss: 4.8347 - lr: 3.1250e-05\n",
      "Epoch 52/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2455 - reconstruction_loss: 1.2740 - kl_loss: 4.8378\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2454 - reconstruction_loss: 1.2727 - kl_loss: 4.8382 - lr: 3.1250e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2304 - reconstruction_loss: 1.2699 - kl_loss: 4.8434 - lr: 1.5625e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2367 - reconstruction_loss: 1.2689 - kl_loss: 4.8391 - lr: 1.5625e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2385 - reconstruction_loss: 1.2698 - kl_loss: 4.8354 - lr: 1.5625e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2337 - reconstruction_loss: 1.2697 - kl_loss: 4.8272 - lr: 1.5625e-05\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2301 - reconstruction_loss: 1.2693 - kl_loss: 4.8239 - lr: 1.5625e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2353 - reconstruction_loss: 1.2691 - kl_loss: 4.8322 - lr: 1.5625e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2402 - reconstruction_loss: 1.2771 - kl_loss: 4.8345 - lr: 1.5625e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2478 - reconstruction_loss: 1.2737 - kl_loss: 4.8352\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2478 - reconstruction_loss: 1.2737 - kl_loss: 4.8352 - lr: 1.5625e-05\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2379 - reconstruction_loss: 1.2708 - kl_loss: 4.8382 - lr: 7.8125e-06\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2413 - reconstruction_loss: 1.2696 - kl_loss: 4.8366 - lr: 7.8125e-06\n",
      "Epoch 63/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2349 - reconstruction_loss: 1.2691 - kl_loss: 4.8387\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2350 - reconstruction_loss: 1.2684 - kl_loss: 4.8388 - lr: 7.8125e-06\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2388 - reconstruction_loss: 1.2676 - kl_loss: 4.8301 - lr: 3.9063e-06\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2307 - reconstruction_loss: 1.2679 - kl_loss: 4.8306 - lr: 3.9063e-06\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2377 - reconstruction_loss: 1.2680 - kl_loss: 4.8355 - lr: 3.9063e-06\n",
      "Epoch 67/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2357 - reconstruction_loss: 1.2665 - kl_loss: 4.8430\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2357 - reconstruction_loss: 1.2657 - kl_loss: 4.8428 - lr: 3.9063e-06\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2312 - reconstruction_loss: 1.2639 - kl_loss: 4.8361 - lr: 1.9531e-06\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2318 - reconstruction_loss: 1.2658 - kl_loss: 4.8331 - lr: 1.9531e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2369 - reconstruction_loss: 1.2663 - kl_loss: 4.8318 - lr: 1.9531e-06\n",
      "Epoch 71/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.2305 - reconstruction_loss: 1.2660 - kl_loss: 4.8324\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2305 - reconstruction_loss: 1.2655 - kl_loss: 4.8322 - lr: 1.9531e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2344 - reconstruction_loss: 1.2654 - kl_loss: 4.8301 - lr: 1.0000e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2346 - reconstruction_loss: 1.2669 - kl_loss: 4.8309 - lr: 1.0000e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2284 - reconstruction_loss: 1.2641 - kl_loss: 4.8340 - lr: 1.0000e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2339 - reconstruction_loss: 1.2634 - kl_loss: 4.8325 - lr: 1.0000e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2313 - reconstruction_loss: 1.2652 - kl_loss: 4.8312 - lr: 1.0000e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2288 - reconstruction_loss: 1.2651 - kl_loss: 4.8332 - lr: 1.0000e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2277 - reconstruction_loss: 1.2647 - kl_loss: 4.8326 - lr: 1.0000e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2332 - reconstruction_loss: 1.2627 - kl_loss: 4.8324 - lr: 1.0000e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2334 - reconstruction_loss: 1.2633 - kl_loss: 4.8314 - lr: 1.0000e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2320 - reconstruction_loss: 1.2654 - kl_loss: 4.8293 - lr: 1.0000e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2321 - reconstruction_loss: 1.2630 - kl_loss: 4.8324 - lr: 1.0000e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2270 - reconstruction_loss: 1.2652 - kl_loss: 4.8326 - lr: 1.0000e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2277 - reconstruction_loss: 1.2646 - kl_loss: 4.8330 - lr: 1.0000e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2305 - reconstruction_loss: 1.2657 - kl_loss: 4.8370 - lr: 1.0000e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2325 - reconstruction_loss: 1.2643 - kl_loss: 4.8392 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2278 - reconstruction_loss: 1.2615 - kl_loss: 4.8359 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2332 - reconstruction_loss: 1.2640 - kl_loss: 4.8323 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2350 - reconstruction_loss: 1.2633 - kl_loss: 4.8304 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2369 - reconstruction_loss: 1.2637 - kl_loss: 4.8322 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2344 - reconstruction_loss: 1.2638 - kl_loss: 4.8321 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2328 - reconstruction_loss: 1.2635 - kl_loss: 4.8322 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2370 - reconstruction_loss: 1.2629 - kl_loss: 4.8329 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2242 - reconstruction_loss: 1.2629 - kl_loss: 4.8309 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2338 - reconstruction_loss: 1.2631 - kl_loss: 4.8357 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2326 - reconstruction_loss: 1.2629 - kl_loss: 4.8361 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2361 - reconstruction_loss: 1.2631 - kl_loss: 4.8365 - lr: 1.0000e-06\n",
      "Epoch 97: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 10:10:46,138]\u001b[0m Trial 22 finished with value: 0.004940181217522342 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 166, 'encoder_units_l2': 184, 'encoder_units_l3': 981, 'encoder_units_l4': 16, 'decoder_layers': 7, 'decoder_units_l1': 222, 'decoder_units_l2': 74, 'decoder_units_l3': 184, 'decoder_units_l4': 190, 'decoder_units_l5': 121, 'decoder_units_l6': 496, 'decoder_units_l7': 26, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 6.9318 - reconstruction_loss: 4.9738 - kl_loss: 2.7233 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 4.0030 - reconstruction_loss: 2.8716 - kl_loss: 3.9984 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.1370 - reconstruction_loss: 2.2014 - kl_loss: 4.3581 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.9586 - reconstruction_loss: 2.0258 - kl_loss: 4.5418 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.8430 - reconstruction_loss: 1.8780 - kl_loss: 4.6865 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.7494 - reconstruction_loss: 1.7850 - kl_loss: 4.7595 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.7107 - reconstruction_loss: 1.7139 - kl_loss: 4.7877 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.6583 - reconstruction_loss: 1.6772 - kl_loss: 4.8617 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.6339 - reconstruction_loss: 1.6504 - kl_loss: 4.8721 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5704 - reconstruction_loss: 1.5974 - kl_loss: 4.8943 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5458 - reconstruction_loss: 1.5732 - kl_loss: 4.8789 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5682 - reconstruction_loss: 1.5543 - kl_loss: 4.9356 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5468 - reconstruction_loss: 1.5386 - kl_loss: 4.8722 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4588 - reconstruction_loss: 1.4905 - kl_loss: 4.8689 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4640 - reconstruction_loss: 1.4965 - kl_loss: 4.8750 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5080 - reconstruction_loss: 1.5275 - kl_loss: 4.9027 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.5123 - reconstruction_loss: 1.5260 - kl_loss: 4.8885\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5122 - reconstruction_loss: 1.5243 - kl_loss: 4.8889 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4030 - reconstruction_loss: 1.4126 - kl_loss: 4.8792 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3667 - reconstruction_loss: 1.3999 - kl_loss: 4.9021 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3593 - reconstruction_loss: 1.3904 - kl_loss: 4.8794 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3611 - reconstruction_loss: 1.3892 - kl_loss: 4.8630 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3675 - reconstruction_loss: 1.3961 - kl_loss: 4.8720 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3719 - reconstruction_loss: 1.3929 - kl_loss: 4.8813 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3591 - reconstruction_loss: 1.3735 - kl_loss: 4.8783 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3586 - reconstruction_loss: 1.3972 - kl_loss: 4.8821 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3830 - reconstruction_loss: 1.4010 - kl_loss: 4.8825 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.3694 - reconstruction_loss: 1.3877 - kl_loss: 4.8785\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3693 - reconstruction_loss: 1.3864 - kl_loss: 4.8781 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3239 - reconstruction_loss: 1.3496 - kl_loss: 4.8719 - lr: 2.5000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3160 - reconstruction_loss: 1.3464 - kl_loss: 4.8471 - lr: 2.5000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3224 - reconstruction_loss: 1.3555 - kl_loss: 4.8566 - lr: 2.5000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3130 - reconstruction_loss: 1.3355 - kl_loss: 4.8435 - lr: 2.5000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3126 - reconstruction_loss: 1.3424 - kl_loss: 4.8515 - lr: 2.5000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3127 - reconstruction_loss: 1.3401 - kl_loss: 4.8352 - lr: 2.5000e-04\n",
      "Epoch 34/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.3252 - reconstruction_loss: 1.3458 - kl_loss: 4.8503\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3251 - reconstruction_loss: 1.3443 - kl_loss: 4.8502 - lr: 2.5000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2940 - reconstruction_loss: 1.3249 - kl_loss: 4.8378 - lr: 1.2500e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2955 - reconstruction_loss: 1.3251 - kl_loss: 4.8337 - lr: 1.2500e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2972 - reconstruction_loss: 1.3248 - kl_loss: 4.8209 - lr: 1.2500e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2873 - reconstruction_loss: 1.3213 - kl_loss: 4.8546 - lr: 1.2500e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2925 - reconstruction_loss: 1.3226 - kl_loss: 4.8509 - lr: 1.2500e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2845 - reconstruction_loss: 1.3121 - kl_loss: 4.8525 - lr: 1.2500e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2789 - reconstruction_loss: 1.3142 - kl_loss: 4.8491 - lr: 1.2500e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2770 - reconstruction_loss: 1.3143 - kl_loss: 4.8355 - lr: 1.2500e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2770 - reconstruction_loss: 1.3150 - kl_loss: 4.8263 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2826 - reconstruction_loss: 1.3132 - kl_loss: 4.8198 - lr: 1.2500e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2761 - reconstruction_loss: 1.3066 - kl_loss: 4.8321 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2846 - reconstruction_loss: 1.3129 - kl_loss: 4.8796 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2777 - reconstruction_loss: 1.3073 - kl_loss: 4.8452 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2749 - reconstruction_loss: 1.3075 - kl_loss: 4.8370\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2749 - reconstruction_loss: 1.3063 - kl_loss: 4.8369 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2734 - reconstruction_loss: 1.2988 - kl_loss: 4.8501 - lr: 6.2500e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2729 - reconstruction_loss: 1.3011 - kl_loss: 4.8286 - lr: 6.2500e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2618 - reconstruction_loss: 1.2965 - kl_loss: 4.8337 - lr: 6.2500e-05\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2515 - reconstruction_loss: 1.2948 - kl_loss: 4.8260 - lr: 6.2500e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2589 - reconstruction_loss: 1.2928 - kl_loss: 4.8352 - lr: 6.2500e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2567 - reconstruction_loss: 1.2943 - kl_loss: 4.8277 - lr: 6.2500e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2616 - reconstruction_loss: 1.2962 - kl_loss: 4.8260 - lr: 6.2500e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2604 - reconstruction_loss: 1.2914 - kl_loss: 4.8257 - lr: 6.2500e-05\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2521 - reconstruction_loss: 1.2912 - kl_loss: 4.8217 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2655 - reconstruction_loss: 1.2923 - kl_loss: 4.8267 - lr: 6.2500e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2542 - reconstruction_loss: 1.2949 - kl_loss: 4.8101 - lr: 6.2500e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2553 - reconstruction_loss: 1.2897 - kl_loss: 4.8266 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2531 - reconstruction_loss: 1.2905 - kl_loss: 4.8232 - lr: 6.2500e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2574 - reconstruction_loss: 1.2907 - kl_loss: 4.8297 - lr: 6.2500e-05\n",
      "Epoch 63/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.2552 - reconstruction_loss: 1.2951 - kl_loss: 4.8225\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2553 - reconstruction_loss: 1.2941 - kl_loss: 4.8227 - lr: 6.2500e-05\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2445 - reconstruction_loss: 1.2887 - kl_loss: 4.8177 - lr: 3.1250e-05\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2512 - reconstruction_loss: 1.2852 - kl_loss: 4.8289 - lr: 3.1250e-05\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2546 - reconstruction_loss: 1.2864 - kl_loss: 4.8313 - lr: 3.1250e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2545 - reconstruction_loss: 1.2887 - kl_loss: 4.8087 - lr: 3.1250e-05\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2508 - reconstruction_loss: 1.2881 - kl_loss: 4.8047 - lr: 3.1250e-05\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2559 - reconstruction_loss: 1.2890 - kl_loss: 4.8109 - lr: 3.1250e-05\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2496 - reconstruction_loss: 1.2865 - kl_loss: 4.8145 - lr: 3.1250e-05\n",
      "Epoch 71/300\n",
      "384/391 [============================>.] - ETA: 0s - loss: 2.2530 - reconstruction_loss: 1.2851 - kl_loss: 4.8277\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2530 - reconstruction_loss: 1.2845 - kl_loss: 4.8275 - lr: 3.1250e-05\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2502 - reconstruction_loss: 1.2817 - kl_loss: 4.8144 - lr: 1.5625e-05\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2482 - reconstruction_loss: 1.2835 - kl_loss: 4.8213 - lr: 1.5625e-05\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2387 - reconstruction_loss: 1.2804 - kl_loss: 4.8148 - lr: 1.5625e-05\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2406 - reconstruction_loss: 1.2834 - kl_loss: 4.8124 - lr: 1.5625e-05\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2471 - reconstruction_loss: 1.2806 - kl_loss: 4.8169 - lr: 1.5625e-05\n",
      "Epoch 77/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2453 - reconstruction_loss: 1.2816 - kl_loss: 4.8143\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2453 - reconstruction_loss: 1.2809 - kl_loss: 4.8141 - lr: 1.5625e-05\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2374 - reconstruction_loss: 1.2782 - kl_loss: 4.8174 - lr: 7.8125e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2372 - reconstruction_loss: 1.2774 - kl_loss: 4.8180 - lr: 7.8125e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2489 - reconstruction_loss: 1.2782 - kl_loss: 4.8202 - lr: 7.8125e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2320 - reconstruction_loss: 1.2750 - kl_loss: 4.8185 - lr: 7.8125e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2394 - reconstruction_loss: 1.2791 - kl_loss: 4.8111 - lr: 7.8125e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2397 - reconstruction_loss: 1.2780 - kl_loss: 4.8097 - lr: 7.8125e-06\n",
      "Epoch 84/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2380 - reconstruction_loss: 1.2777 - kl_loss: 4.8167\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2380 - reconstruction_loss: 1.2767 - kl_loss: 4.8167 - lr: 7.8125e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2425 - reconstruction_loss: 1.2744 - kl_loss: 4.8244 - lr: 3.9063e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2434 - reconstruction_loss: 1.2750 - kl_loss: 4.8221 - lr: 3.9063e-06\n",
      "Epoch 87/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2449 - reconstruction_loss: 1.2760 - kl_loss: 4.8208\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2449 - reconstruction_loss: 1.2748 - kl_loss: 4.8210 - lr: 3.9063e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2338 - reconstruction_loss: 1.2734 - kl_loss: 4.8214 - lr: 1.9531e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2386 - reconstruction_loss: 1.2742 - kl_loss: 4.8183 - lr: 1.9531e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2357 - reconstruction_loss: 1.2748 - kl_loss: 4.8144 - lr: 1.9531e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2440 - reconstruction_loss: 1.2742 - kl_loss: 4.8180\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2440 - reconstruction_loss: 1.2742 - kl_loss: 4.8180 - lr: 1.9531e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2362 - reconstruction_loss: 1.2743 - kl_loss: 4.8227 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2388 - reconstruction_loss: 1.2738 - kl_loss: 4.8190 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2456 - reconstruction_loss: 1.2753 - kl_loss: 4.8172 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2340 - reconstruction_loss: 1.2730 - kl_loss: 4.8177 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2440 - reconstruction_loss: 1.2730 - kl_loss: 4.8201 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2381 - reconstruction_loss: 1.2716 - kl_loss: 4.8196 - lr: 1.0000e-06\n",
      "Epoch 98/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2367 - reconstruction_loss: 1.2728 - kl_loss: 4.8202 - lr: 1.0000e-06\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2414 - reconstruction_loss: 1.2735 - kl_loss: 4.8201 - lr: 1.0000e-06\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2426 - reconstruction_loss: 1.2741 - kl_loss: 4.8198 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2406 - reconstruction_loss: 1.2736 - kl_loss: 4.8205 - lr: 1.0000e-06\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2374 - reconstruction_loss: 1.2751 - kl_loss: 4.8195 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2382 - reconstruction_loss: 1.2710 - kl_loss: 4.8183 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2461 - reconstruction_loss: 1.2744 - kl_loss: 4.8163 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2358 - reconstruction_loss: 1.2734 - kl_loss: 4.8176 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2407 - reconstruction_loss: 1.2730 - kl_loss: 4.8198 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2273 - reconstruction_loss: 1.2715 - kl_loss: 4.8203 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2340 - reconstruction_loss: 1.2725 - kl_loss: 4.8172 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2402 - reconstruction_loss: 1.2734 - kl_loss: 4.8153 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2379 - reconstruction_loss: 1.2743 - kl_loss: 4.8155 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2376 - reconstruction_loss: 1.2740 - kl_loss: 4.8184 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2387 - reconstruction_loss: 1.2738 - kl_loss: 4.8186 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2314 - reconstruction_loss: 1.2741 - kl_loss: 4.8186 - lr: 1.0000e-06\n",
      "Epoch 113: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 10:16:15,388]\u001b[0m Trial 23 finished with value: 0.004983134672580036 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 94, 'encoder_units_l2': 563, 'encoder_units_l3': 528, 'decoder_layers': 8, 'decoder_units_l1': 652, 'decoder_units_l2': 333, 'decoder_units_l3': 211, 'decoder_units_l4': 345, 'decoder_units_l5': 266, 'decoder_units_l6': 464, 'decoder_units_l7': 79, 'decoder_units_l8': 808, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 5.9827 - reconstruction_loss: 3.9340 - kl_loss: 3.4276 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.1706 - reconstruction_loss: 2.2365 - kl_loss: 4.2202 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.9682 - reconstruction_loss: 2.0585 - kl_loss: 4.3939 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.8191 - reconstruction_loss: 1.8974 - kl_loss: 4.5740 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.8771 - reconstruction_loss: 1.8318 - kl_loss: 4.7275 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.7424 - reconstruction_loss: 1.6923 - kl_loss: 4.8475 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5221 - reconstruction_loss: 1.5721 - kl_loss: 4.8775 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5585 - reconstruction_loss: 1.5646 - kl_loss: 4.9381 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5586 - reconstruction_loss: 1.5890 - kl_loss: 4.9333 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5388 - reconstruction_loss: 1.5219 - kl_loss: 5.0297 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4928 - reconstruction_loss: 1.4384 - kl_loss: 5.0338 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4190 - reconstruction_loss: 1.4416 - kl_loss: 4.9196 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4663 - reconstruction_loss: 1.4707 - kl_loss: 5.0110 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4535 - reconstruction_loss: 1.4285 - kl_loss: 5.0115 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3800 - reconstruction_loss: 1.5167 - kl_loss: 4.8131\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3802 - reconstruction_loss: 1.5167 - kl_loss: 4.8131 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2840 - reconstruction_loss: 1.3057 - kl_loss: 4.8030 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2475 - reconstruction_loss: 1.2925 - kl_loss: 4.7690 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2593 - reconstruction_loss: 1.3027 - kl_loss: 4.7502 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2681 - reconstruction_loss: 1.3018 - kl_loss: 4.7411 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.4914 - reconstruction_loss: 1.9518 - kl_loss: 4.7666\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4925 - reconstruction_loss: 1.9518 - kl_loss: 4.7666 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.0164 - reconstruction_loss: 1.9624 - kl_loss: 4.8074 - lr: 2.5000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.7889 - reconstruction_loss: 1.7644 - kl_loss: 4.8820 - lr: 2.5000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.6825 - reconstruction_loss: 1.6801 - kl_loss: 4.8830\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.6824 - reconstruction_loss: 1.6801 - kl_loss: 4.8830 - lr: 2.5000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5752 - reconstruction_loss: 1.5958 - kl_loss: 4.8658 - lr: 1.2500e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5617 - reconstruction_loss: 1.5810 - kl_loss: 4.8877 - lr: 1.2500e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.5261 - reconstruction_loss: 1.5527 - kl_loss: 4.8658\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5261 - reconstruction_loss: 1.5527 - kl_loss: 4.8658 - lr: 1.2500e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5113 - reconstruction_loss: 1.5248 - kl_loss: 4.8722 - lr: 6.2500e-05\n",
      "Epoch 27: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 10:17:53,856]\u001b[0m Trial 24 finished with value: 0.005884158091865727 and parameters: {'encoder_layers': 5, 'encoder_units_l1': 90, 'encoder_units_l2': 338, 'encoder_units_l3': 781, 'encoder_units_l4': 598, 'encoder_units_l5': 18, 'decoder_layers': 7, 'decoder_units_l1': 352, 'decoder_units_l2': 204, 'decoder_units_l3': 121, 'decoder_units_l4': 1003, 'decoder_units_l5': 439, 'decoder_units_l6': 248, 'decoder_units_l7': 33, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 8ms/step - loss: 6.2965 - reconstruction_loss: 4.3817 - kl_loss: 3.1592 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.8194 - reconstruction_loss: 2.6985 - kl_loss: 4.1362 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.0852 - reconstruction_loss: 2.1425 - kl_loss: 4.4235 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.8590 - reconstruction_loss: 1.9259 - kl_loss: 4.5827 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.7556 - reconstruction_loss: 1.8052 - kl_loss: 4.6964 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.7084 - reconstruction_loss: 1.7183 - kl_loss: 4.7930 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.6365 - reconstruction_loss: 1.6555 - kl_loss: 4.8803 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.6668 - reconstruction_loss: 1.6308 - kl_loss: 4.8853 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5311 - reconstruction_loss: 1.5425 - kl_loss: 4.8898 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5086 - reconstruction_loss: 1.5483 - kl_loss: 4.8971 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5144 - reconstruction_loss: 1.5517 - kl_loss: 4.9115 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.5439 - reconstruction_loss: 1.5466 - kl_loss: 4.9275\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5437 - reconstruction_loss: 1.5447 - kl_loss: 4.9258 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4187 - reconstruction_loss: 1.4279 - kl_loss: 4.9226 - lr: 5.0000e-04\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3789 - reconstruction_loss: 1.4004 - kl_loss: 4.9091 - lr: 5.0000e-04\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3814 - reconstruction_loss: 1.3929 - kl_loss: 4.9321 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3802 - reconstruction_loss: 1.4077 - kl_loss: 4.9279 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3845 - reconstruction_loss: 1.3978 - kl_loss: 4.9246 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.3785 - reconstruction_loss: 1.3962 - kl_loss: 4.9352\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3786 - reconstruction_loss: 1.3953 - kl_loss: 4.9335 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3731 - reconstruction_loss: 1.3680 - kl_loss: 4.9706 - lr: 2.5000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3364 - reconstruction_loss: 1.3481 - kl_loss: 4.9301 - lr: 2.5000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3507 - reconstruction_loss: 1.3606 - kl_loss: 4.9340 - lr: 2.5000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3411 - reconstruction_loss: 1.3451 - kl_loss: 4.9012 - lr: 2.5000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3182 - reconstruction_loss: 1.3375 - kl_loss: 4.8893 - lr: 2.5000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3197 - reconstruction_loss: 1.3401 - kl_loss: 4.9028 - lr: 2.5000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3094 - reconstruction_loss: 1.3407 - kl_loss: 4.8711 - lr: 2.5000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3166 - reconstruction_loss: 1.3358 - kl_loss: 4.8744 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3189 - reconstruction_loss: 1.3354 - kl_loss: 4.8843 - lr: 2.5000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3228 - reconstruction_loss: 1.3352 - kl_loss: 4.8824 - lr: 2.5000e-04\n",
      "Epoch 29/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.3170 - reconstruction_loss: 1.3439 - kl_loss: 4.8957\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3171 - reconstruction_loss: 1.3425 - kl_loss: 4.8945 - lr: 2.5000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2960 - reconstruction_loss: 1.3150 - kl_loss: 4.8960 - lr: 1.2500e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2993 - reconstruction_loss: 1.3159 - kl_loss: 4.8886 - lr: 1.2500e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2877 - reconstruction_loss: 1.3071 - kl_loss: 4.8741 - lr: 1.2500e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2769 - reconstruction_loss: 1.3064 - kl_loss: 4.8677 - lr: 1.2500e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2837 - reconstruction_loss: 1.3075 - kl_loss: 4.8602 - lr: 1.2500e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2798 - reconstruction_loss: 1.3056 - kl_loss: 4.8501 - lr: 1.2500e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2865 - reconstruction_loss: 1.3080 - kl_loss: 4.8801 - lr: 1.2500e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2712 - reconstruction_loss: 1.2995 - kl_loss: 4.8714 - lr: 1.2500e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2834 - reconstruction_loss: 1.3094 - kl_loss: 4.8612 - lr: 1.2500e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2833 - reconstruction_loss: 1.3088 - kl_loss: 4.8707 - lr: 1.2500e-04\n",
      "Epoch 40/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2827 - reconstruction_loss: 1.3092 - kl_loss: 4.8553\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2826 - reconstruction_loss: 1.3079 - kl_loss: 4.8544 - lr: 1.2500e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2680 - reconstruction_loss: 1.2961 - kl_loss: 4.8477 - lr: 6.2500e-05\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2695 - reconstruction_loss: 1.3010 - kl_loss: 4.8447 - lr: 6.2500e-05\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2636 - reconstruction_loss: 1.2930 - kl_loss: 4.8531 - lr: 6.2500e-05\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2571 - reconstruction_loss: 1.2889 - kl_loss: 4.8491 - lr: 6.2500e-05\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2598 - reconstruction_loss: 1.2938 - kl_loss: 4.8465 - lr: 6.2500e-05\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2639 - reconstruction_loss: 1.2883 - kl_loss: 4.8561 - lr: 6.2500e-05\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2618 - reconstruction_loss: 1.2923 - kl_loss: 4.8429\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2618 - reconstruction_loss: 1.2923 - kl_loss: 4.8429 - lr: 6.2500e-05\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2507 - reconstruction_loss: 1.2834 - kl_loss: 4.8510 - lr: 3.1250e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2563 - reconstruction_loss: 1.2885 - kl_loss: 4.8350 - lr: 3.1250e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2591 - reconstruction_loss: 1.2850 - kl_loss: 4.8522 - lr: 3.1250e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2489 - reconstruction_loss: 1.2813 - kl_loss: 4.8444 - lr: 3.1250e-05\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2581 - reconstruction_loss: 1.2854 - kl_loss: 4.8421 - lr: 3.1250e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2495 - reconstruction_loss: 1.2818 - kl_loss: 4.8414 - lr: 3.1250e-05\n",
      "Epoch 54/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2607 - reconstruction_loss: 1.2868 - kl_loss: 4.8436\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2607 - reconstruction_loss: 1.2863 - kl_loss: 4.8439 - lr: 3.1250e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2549 - reconstruction_loss: 1.2807 - kl_loss: 4.8475 - lr: 1.5625e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2542 - reconstruction_loss: 1.2861 - kl_loss: 4.8360 - lr: 1.5625e-05\n",
      "Epoch 57/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.2477 - reconstruction_loss: 1.2832 - kl_loss: 4.8476\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2478 - reconstruction_loss: 1.2822 - kl_loss: 4.8475 - lr: 1.5625e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2446 - reconstruction_loss: 1.2805 - kl_loss: 4.8390 - lr: 7.8125e-06\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2557 - reconstruction_loss: 1.2825 - kl_loss: 4.8412 - lr: 7.8125e-06\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2559 - reconstruction_loss: 1.2813 - kl_loss: 4.8417 - lr: 7.8125e-06\n",
      "Epoch 61/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2517 - reconstruction_loss: 1.2809 - kl_loss: 4.8449\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2517 - reconstruction_loss: 1.2797 - kl_loss: 4.8450 - lr: 7.8125e-06\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2481 - reconstruction_loss: 1.2774 - kl_loss: 4.8422 - lr: 3.9063e-06\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2409 - reconstruction_loss: 1.2787 - kl_loss: 4.8366 - lr: 3.9063e-06\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2479 - reconstruction_loss: 1.2793 - kl_loss: 4.8312 - lr: 3.9063e-06\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2413 - reconstruction_loss: 1.2805 - kl_loss: 4.8350 - lr: 3.9063e-06\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2497 - reconstruction_loss: 1.2792 - kl_loss: 4.8377 - lr: 3.9063e-06\n",
      "Epoch 67/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2505 - reconstruction_loss: 1.2800 - kl_loss: 4.8420\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2505 - reconstruction_loss: 1.2790 - kl_loss: 4.8420 - lr: 3.9063e-06\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2493 - reconstruction_loss: 1.2761 - kl_loss: 4.8442 - lr: 1.9531e-06\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2417 - reconstruction_loss: 1.2764 - kl_loss: 4.8398 - lr: 1.9531e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2505 - reconstruction_loss: 1.2769 - kl_loss: 4.8381 - lr: 1.9531e-06\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2436 - reconstruction_loss: 1.2779 - kl_loss: 4.8383 - lr: 1.9531e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2480 - reconstruction_loss: 1.2752 - kl_loss: 4.8463\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2480 - reconstruction_loss: 1.2752 - kl_loss: 4.8463 - lr: 1.9531e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2424 - reconstruction_loss: 1.2754 - kl_loss: 4.8434 - lr: 1.0000e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2470 - reconstruction_loss: 1.2755 - kl_loss: 4.8411 - lr: 1.0000e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2483 - reconstruction_loss: 1.2742 - kl_loss: 4.8418 - lr: 1.0000e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2454 - reconstruction_loss: 1.2754 - kl_loss: 4.8374 - lr: 1.0000e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2464 - reconstruction_loss: 1.2766 - kl_loss: 4.8343 - lr: 1.0000e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2437 - reconstruction_loss: 1.2760 - kl_loss: 4.8378 - lr: 1.0000e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2461 - reconstruction_loss: 1.2750 - kl_loss: 4.8383 - lr: 1.0000e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2453 - reconstruction_loss: 1.2745 - kl_loss: 4.8393 - lr: 1.0000e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2422 - reconstruction_loss: 1.2742 - kl_loss: 4.8435 - lr: 1.0000e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2457 - reconstruction_loss: 1.2760 - kl_loss: 4.8396 - lr: 1.0000e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2459 - reconstruction_loss: 1.2747 - kl_loss: 4.8389 - lr: 1.0000e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2405 - reconstruction_loss: 1.2754 - kl_loss: 4.8393 - lr: 1.0000e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2468 - reconstruction_loss: 1.2753 - kl_loss: 4.8420 - lr: 1.0000e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2407 - reconstruction_loss: 1.2749 - kl_loss: 4.8407 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2475 - reconstruction_loss: 1.2759 - kl_loss: 4.8403 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2451 - reconstruction_loss: 1.2742 - kl_loss: 4.8438 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2479 - reconstruction_loss: 1.2761 - kl_loss: 4.8448 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2434 - reconstruction_loss: 1.2762 - kl_loss: 4.8434 - lr: 1.0000e-06\n",
      "Epoch 90: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 10:22:32,037]\u001b[0m Trial 25 finished with value: 0.004979230702259568 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 181, 'encoder_units_l2': 213, 'encoder_units_l3': 478, 'decoder_layers': 7, 'decoder_units_l1': 494, 'decoder_units_l2': 104, 'decoder_units_l3': 271, 'decoder_units_l4': 141, 'decoder_units_l5': 151, 'decoder_units_l6': 698, 'decoder_units_l7': 16, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 6.2334 - reconstruction_loss: 4.3370 - kl_loss: 3.1524 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.6526 - reconstruction_loss: 2.5661 - kl_loss: 4.1338 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.0292 - reconstruction_loss: 2.1041 - kl_loss: 4.4185 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.8731 - reconstruction_loss: 1.9489 - kl_loss: 4.5531 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.7854 - reconstruction_loss: 1.8336 - kl_loss: 4.6725 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.6899 - reconstruction_loss: 1.7245 - kl_loss: 4.7567 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.6386 - reconstruction_loss: 1.6626 - kl_loss: 4.8055 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5544 - reconstruction_loss: 1.5739 - kl_loss: 4.8302 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.6101 - reconstruction_loss: 1.6405 - kl_loss: 4.8668 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5726 - reconstruction_loss: 1.5571 - kl_loss: 4.8521 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5086 - reconstruction_loss: 1.5468 - kl_loss: 4.8895 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5363 - reconstruction_loss: 1.5392 - kl_loss: 4.9280 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4863 - reconstruction_loss: 1.6990 - kl_loss: 4.9663 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.2467 - reconstruction_loss: 2.0730 - kl_loss: 4.7398\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.2461 - reconstruction_loss: 2.0730 - kl_loss: 4.7398 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.7579 - reconstruction_loss: 1.7633 - kl_loss: 4.7902 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.6493 - reconstruction_loss: 1.6626 - kl_loss: 4.8787 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.6023 - reconstruction_loss: 1.6184 - kl_loss: 4.8929\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.6022 - reconstruction_loss: 1.6171 - kl_loss: 4.8924 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5661 - reconstruction_loss: 1.5632 - kl_loss: 4.8966 - lr: 2.5000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5046 - reconstruction_loss: 1.5095 - kl_loss: 4.9010 - lr: 2.5000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4891 - reconstruction_loss: 1.4884 - kl_loss: 4.9297 - lr: 2.5000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4730 - reconstruction_loss: 1.4974 - kl_loss: 4.9412 - lr: 2.5000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4347 - reconstruction_loss: 1.4449 - kl_loss: 4.9237 - lr: 2.5000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4274 - reconstruction_loss: 1.4407 - kl_loss: 4.9126 - lr: 2.5000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4025 - reconstruction_loss: 1.4229 - kl_loss: 4.9161 - lr: 2.5000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3990 - reconstruction_loss: 1.4172 - kl_loss: 4.9073 - lr: 2.5000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3950 - reconstruction_loss: 1.4075 - kl_loss: 4.9105 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3880 - reconstruction_loss: 1.4051 - kl_loss: 4.9102 - lr: 2.5000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3819 - reconstruction_loss: 1.3978 - kl_loss: 4.9246 - lr: 2.5000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3949 - reconstruction_loss: 1.3976 - kl_loss: 4.9169 - lr: 2.5000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3680 - reconstruction_loss: 1.3875 - kl_loss: 4.9069 - lr: 2.5000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3760 - reconstruction_loss: 1.3939 - kl_loss: 4.9163 - lr: 2.5000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3757 - reconstruction_loss: 1.3776 - kl_loss: 4.9210 - lr: 2.5000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3547 - reconstruction_loss: 1.3620 - kl_loss: 4.9098 - lr: 2.5000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3516 - reconstruction_loss: 1.3663 - kl_loss: 4.9058 - lr: 2.5000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3430 - reconstruction_loss: 1.3658 - kl_loss: 4.9005 - lr: 2.5000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3408 - reconstruction_loss: 1.3634 - kl_loss: 4.8921 - lr: 2.5000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3490 - reconstruction_loss: 1.3762 - kl_loss: 4.8998 - lr: 2.5000e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3723 - reconstruction_loss: 1.3749 - kl_loss: 4.9094 - lr: 2.5000e-04\n",
      "Epoch 39/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.3681 - reconstruction_loss: 1.3733 - kl_loss: 4.9080\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3678 - reconstruction_loss: 1.3721 - kl_loss: 4.9092 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3553 - reconstruction_loss: 1.3554 - kl_loss: 4.9016 - lr: 1.2500e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3091 - reconstruction_loss: 1.3336 - kl_loss: 4.8759 - lr: 1.2500e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3065 - reconstruction_loss: 1.3310 - kl_loss: 4.9033 - lr: 1.2500e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3143 - reconstruction_loss: 1.3356 - kl_loss: 4.8985 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3107 - reconstruction_loss: 1.3252 - kl_loss: 4.9082 - lr: 1.2500e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3209 - reconstruction_loss: 1.3420 - kl_loss: 4.9023 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3280 - reconstruction_loss: 1.3373 - kl_loss: 4.9247 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3026 - reconstruction_loss: 1.3198 - kl_loss: 4.8929 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2952 - reconstruction_loss: 1.3189 - kl_loss: 4.8786 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3119 - reconstruction_loss: 1.3323 - kl_loss: 4.8957 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3061 - reconstruction_loss: 1.3283 - kl_loss: 4.9031 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "384/391 [============================>.] - ETA: 0s - loss: 2.3240 - reconstruction_loss: 1.3381 - kl_loss: 4.9040\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3239 - reconstruction_loss: 1.3364 - kl_loss: 4.9030 - lr: 1.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3161 - reconstruction_loss: 1.3265 - kl_loss: 4.9103 - lr: 6.2500e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2901 - reconstruction_loss: 1.3154 - kl_loss: 4.8854 - lr: 6.2500e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2989 - reconstruction_loss: 1.3119 - kl_loss: 4.8964 - lr: 6.2500e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2894 - reconstruction_loss: 1.3071 - kl_loss: 4.8867 - lr: 6.2500e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2760 - reconstruction_loss: 1.3045 - kl_loss: 4.8830 - lr: 6.2500e-05\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2806 - reconstruction_loss: 1.3014 - kl_loss: 4.8900 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2777 - reconstruction_loss: 1.3074 - kl_loss: 4.8782 - lr: 6.2500e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2781 - reconstruction_loss: 1.3034 - kl_loss: 4.8889 - lr: 6.2500e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2774 - reconstruction_loss: 1.2990 - kl_loss: 4.8884 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2753 - reconstruction_loss: 1.2999 - kl_loss: 4.8842 - lr: 6.2500e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2738 - reconstruction_loss: 1.3079 - kl_loss: 4.8866 - lr: 6.2500e-05\n",
      "Epoch 63/300\n",
      "384/391 [============================>.] - ETA: 0s - loss: 2.2811 - reconstruction_loss: 1.3072 - kl_loss: 4.8925\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2812 - reconstruction_loss: 1.3067 - kl_loss: 4.8923 - lr: 6.2500e-05\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2760 - reconstruction_loss: 1.3033 - kl_loss: 4.8869 - lr: 3.1250e-05\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2753 - reconstruction_loss: 1.2970 - kl_loss: 4.8877 - lr: 3.1250e-05\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2790 - reconstruction_loss: 1.3034 - kl_loss: 4.8781 - lr: 3.1250e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2870 - reconstruction_loss: 1.3028 - kl_loss: 4.8830 - lr: 3.1250e-05\n",
      "Epoch 68/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.2739 - reconstruction_loss: 1.3014 - kl_loss: 4.8883\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2740 - reconstruction_loss: 1.3006 - kl_loss: 4.8883 - lr: 3.1250e-05\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2771 - reconstruction_loss: 1.2982 - kl_loss: 4.8742 - lr: 1.5625e-05\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2702 - reconstruction_loss: 1.2964 - kl_loss: 4.8742 - lr: 1.5625e-05\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2714 - reconstruction_loss: 1.2915 - kl_loss: 4.8855 - lr: 1.5625e-05\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2659 - reconstruction_loss: 1.2903 - kl_loss: 4.8829 - lr: 1.5625e-05\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2671 - reconstruction_loss: 1.2925 - kl_loss: 4.8711 - lr: 1.5625e-05\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2679 - reconstruction_loss: 1.2906 - kl_loss: 4.8742 - lr: 1.5625e-05\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2688 - reconstruction_loss: 1.2957 - kl_loss: 4.8757 - lr: 1.5625e-05\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2716 - reconstruction_loss: 1.2909 - kl_loss: 4.8740 - lr: 1.5625e-05\n",
      "Epoch 77/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.2649 - reconstruction_loss: 1.2925 - kl_loss: 4.8865\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2650 - reconstruction_loss: 1.2911 - kl_loss: 4.8867 - lr: 1.5625e-05\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2632 - reconstruction_loss: 1.2891 - kl_loss: 4.8761 - lr: 7.8125e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2651 - reconstruction_loss: 1.2885 - kl_loss: 4.8773 - lr: 7.8125e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2632 - reconstruction_loss: 1.2869 - kl_loss: 4.8790 - lr: 7.8125e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2588 - reconstruction_loss: 1.2878 - kl_loss: 4.8769 - lr: 7.8125e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2673 - reconstruction_loss: 1.2885 - kl_loss: 4.8754 - lr: 7.8125e-06\n",
      "Epoch 83/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.2601 - reconstruction_loss: 1.2885 - kl_loss: 4.8768\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2601 - reconstruction_loss: 1.2877 - kl_loss: 4.8767 - lr: 7.8125e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2761 - reconstruction_loss: 1.2867 - kl_loss: 4.8758 - lr: 3.9063e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2657 - reconstruction_loss: 1.2878 - kl_loss: 4.8716 - lr: 3.9063e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2593 - reconstruction_loss: 1.2861 - kl_loss: 4.8707 - lr: 3.9063e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2658 - reconstruction_loss: 1.2858 - kl_loss: 4.8832 - lr: 3.9063e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2634 - reconstruction_loss: 1.2839 - kl_loss: 4.8876 - lr: 3.9063e-06\n",
      "Epoch 89/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.2633 - reconstruction_loss: 1.2848 - kl_loss: 4.8878\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2633 - reconstruction_loss: 1.2835 - kl_loss: 4.8884 - lr: 3.9063e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2540 - reconstruction_loss: 1.2811 - kl_loss: 4.8838 - lr: 1.9531e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2584 - reconstruction_loss: 1.2830 - kl_loss: 4.8794 - lr: 1.9531e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2621 - reconstruction_loss: 1.2824 - kl_loss: 4.8818 - lr: 1.9531e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2537 - reconstruction_loss: 1.2805 - kl_loss: 4.8779 - lr: 1.9531e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2623 - reconstruction_loss: 1.2840 - kl_loss: 4.8712 - lr: 1.9531e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2586 - reconstruction_loss: 1.2830 - kl_loss: 4.8756 - lr: 1.9531e-06\n",
      "Epoch 96/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.2641 - reconstruction_loss: 1.2838 - kl_loss: 4.8783\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2640 - reconstruction_loss: 1.2828 - kl_loss: 4.8777 - lr: 1.9531e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2549 - reconstruction_loss: 1.2818 - kl_loss: 4.8757 - lr: 1.0000e-06\n",
      "Epoch 98/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2583 - reconstruction_loss: 1.2830 - kl_loss: 4.8757 - lr: 1.0000e-06\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2662 - reconstruction_loss: 1.2830 - kl_loss: 4.8773 - lr: 1.0000e-06\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2568 - reconstruction_loss: 1.2822 - kl_loss: 4.8779 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2640 - reconstruction_loss: 1.2812 - kl_loss: 4.8785 - lr: 1.0000e-06\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2560 - reconstruction_loss: 1.2839 - kl_loss: 4.8757 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2615 - reconstruction_loss: 1.2820 - kl_loss: 4.8788 - lr: 1.0000e-06\n",
      "Epoch 103: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 10:27:02,935]\u001b[0m Trial 26 finished with value: 0.005013232295997356 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 32, 'encoder_units_l2': 130, 'encoder_units_l3': 227, 'encoder_units_l4': 255, 'decoder_layers': 6, 'decoder_units_l1': 207, 'decoder_units_l2': 57, 'decoder_units_l3': 394, 'decoder_units_l4': 62, 'decoder_units_l5': 104, 'decoder_units_l6': 1024, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 7ms/step - loss: 5.6587 - reconstruction_loss: 4.0263 - kl_loss: 3.3471 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.7413 - reconstruction_loss: 2.6551 - kl_loss: 4.0781 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.0869 - reconstruction_loss: 2.1767 - kl_loss: 4.3320 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.9364 - reconstruction_loss: 2.0225 - kl_loss: 4.4717 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.8419 - reconstruction_loss: 1.9006 - kl_loss: 4.5738 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.7550 - reconstruction_loss: 1.8117 - kl_loss: 4.6645 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.6894 - reconstruction_loss: 1.7457 - kl_loss: 4.7024 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.6735 - reconstruction_loss: 1.6923 - kl_loss: 4.7292 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5807 - reconstruction_loss: 1.6192 - kl_loss: 4.7683 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5675 - reconstruction_loss: 1.5858 - kl_loss: 4.7770 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5150 - reconstruction_loss: 1.5556 - kl_loss: 4.7951 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4977 - reconstruction_loss: 1.5393 - kl_loss: 4.8072 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4845 - reconstruction_loss: 1.5134 - kl_loss: 4.8031 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4753 - reconstruction_loss: 1.5064 - kl_loss: 4.8250 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4599 - reconstruction_loss: 1.4963 - kl_loss: 4.8093 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4453 - reconstruction_loss: 1.4832 - kl_loss: 4.7925 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4217 - reconstruction_loss: 1.4646 - kl_loss: 4.8069 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4772 - reconstruction_loss: 1.4733 - kl_loss: 4.8594 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4031 - reconstruction_loss: 1.4422 - kl_loss: 4.8304 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4148 - reconstruction_loss: 1.4409 - kl_loss: 4.8291 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4148 - reconstruction_loss: 1.4552 - kl_loss: 4.8611 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3897 - reconstruction_loss: 1.4122 - kl_loss: 4.8298 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3759 - reconstruction_loss: 1.4025 - kl_loss: 4.8277 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3909 - reconstruction_loss: 1.4194 - kl_loss: 4.8289 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3513 - reconstruction_loss: 1.3901 - kl_loss: 4.8179 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3622 - reconstruction_loss: 1.4254 - kl_loss: 4.8789 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4202 - reconstruction_loss: 1.4150 - kl_loss: 4.8732 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.4272 - reconstruction_loss: 1.4344 - kl_loss: 4.8780\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4269 - reconstruction_loss: 1.4332 - kl_loss: 4.8798 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3827 - reconstruction_loss: 1.3705 - kl_loss: 4.8767 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3230 - reconstruction_loss: 1.3429 - kl_loss: 4.8489 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2979 - reconstruction_loss: 1.3293 - kl_loss: 4.8146 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2864 - reconstruction_loss: 1.3317 - kl_loss: 4.7918 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2946 - reconstruction_loss: 1.3279 - kl_loss: 4.8189 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2899 - reconstruction_loss: 1.3254 - kl_loss: 4.8059 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2706 - reconstruction_loss: 1.3147 - kl_loss: 4.7963 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2873 - reconstruction_loss: 1.3216 - kl_loss: 4.8008 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2815 - reconstruction_loss: 1.3203 - kl_loss: 4.8125 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2853 - reconstruction_loss: 1.3196 - kl_loss: 4.8087\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2852 - reconstruction_loss: 1.3184 - kl_loss: 4.8069 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2591 - reconstruction_loss: 1.2976 - kl_loss: 4.7953 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2638 - reconstruction_loss: 1.2997 - kl_loss: 4.7918 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2601 - reconstruction_loss: 1.3077 - kl_loss: 4.7898 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2633 - reconstruction_loss: 1.3013 - kl_loss: 4.7968\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2633 - reconstruction_loss: 1.2997 - kl_loss: 4.7961 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2545 - reconstruction_loss: 1.2934 - kl_loss: 4.7911 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2466 - reconstruction_loss: 1.2901 - kl_loss: 4.7842 - lr: 1.2500e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2457 - reconstruction_loss: 1.2836 - kl_loss: 4.8005 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2432 - reconstruction_loss: 1.2856 - kl_loss: 4.7871 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2485 - reconstruction_loss: 1.2855 - kl_loss: 4.7629 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2392 - reconstruction_loss: 1.2857 - kl_loss: 4.7660 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2459 - reconstruction_loss: 1.2855 - kl_loss: 4.7722 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2370 - reconstruction_loss: 1.2866 - kl_loss: 4.7727\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2371 - reconstruction_loss: 1.2853 - kl_loss: 4.7723 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2325 - reconstruction_loss: 1.2799 - kl_loss: 4.7679 - lr: 6.2500e-05\n",
      "Epoch 52/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2305 - reconstruction_loss: 1.2825 - kl_loss: 4.7648 - lr: 6.2500e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2237 - reconstruction_loss: 1.2784 - kl_loss: 4.7695 - lr: 6.2500e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2275 - reconstruction_loss: 1.2781 - kl_loss: 4.7623 - lr: 6.2500e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2351 - reconstruction_loss: 1.2764 - kl_loss: 4.7752 - lr: 6.2500e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2342 - reconstruction_loss: 1.2786 - kl_loss: 4.7705 - lr: 6.2500e-05\n",
      "Epoch 57/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.2368 - reconstruction_loss: 1.2836 - kl_loss: 4.7722\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2368 - reconstruction_loss: 1.2820 - kl_loss: 4.7718 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2278 - reconstruction_loss: 1.2717 - kl_loss: 4.7776 - lr: 3.1250e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2251 - reconstruction_loss: 1.2727 - kl_loss: 4.7677 - lr: 3.1250e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2337 - reconstruction_loss: 1.2722 - kl_loss: 4.7745 - lr: 3.1250e-05\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2213 - reconstruction_loss: 1.2699 - kl_loss: 4.7675 - lr: 3.1250e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2243 - reconstruction_loss: 1.2706 - kl_loss: 4.7698 - lr: 3.1250e-05\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2239 - reconstruction_loss: 1.2729 - kl_loss: 4.7710 - lr: 3.1250e-05\n",
      "Epoch 64/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.2271 - reconstruction_loss: 1.2736 - kl_loss: 4.7704\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2271 - reconstruction_loss: 1.2725 - kl_loss: 4.7704 - lr: 3.1250e-05\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2171 - reconstruction_loss: 1.2698 - kl_loss: 4.7647 - lr: 1.5625e-05\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2260 - reconstruction_loss: 1.2687 - kl_loss: 4.7747 - lr: 1.5625e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2251 - reconstruction_loss: 1.2716 - kl_loss: 4.7746 - lr: 1.5625e-05\n",
      "Epoch 68/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2214 - reconstruction_loss: 1.2736 - kl_loss: 4.7666\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2215 - reconstruction_loss: 1.2720 - kl_loss: 4.7663 - lr: 1.5625e-05\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2279 - reconstruction_loss: 1.2676 - kl_loss: 4.7744 - lr: 7.8125e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2179 - reconstruction_loss: 1.2660 - kl_loss: 4.7761 - lr: 7.8125e-06\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2194 - reconstruction_loss: 1.2659 - kl_loss: 4.7781 - lr: 7.8125e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2245 - reconstruction_loss: 1.2686 - kl_loss: 4.7736 - lr: 7.8125e-06\n",
      "Epoch 73/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.2213 - reconstruction_loss: 1.2687 - kl_loss: 4.7768\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2214 - reconstruction_loss: 1.2674 - kl_loss: 4.7770 - lr: 7.8125e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2295 - reconstruction_loss: 1.2695 - kl_loss: 4.7739 - lr: 3.9063e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2185 - reconstruction_loss: 1.2675 - kl_loss: 4.7709 - lr: 3.9063e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2212 - reconstruction_loss: 1.2669 - kl_loss: 4.7664 - lr: 3.9063e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2176 - reconstruction_loss: 1.2670 - kl_loss: 4.7667 - lr: 3.9063e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2214 - reconstruction_loss: 1.2670 - kl_loss: 4.7761 - lr: 3.9063e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2279 - reconstruction_loss: 1.2653 - kl_loss: 4.7727 - lr: 3.9063e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2215 - reconstruction_loss: 1.2682 - kl_loss: 4.7629 - lr: 3.9063e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2186 - reconstruction_loss: 1.2659 - kl_loss: 4.7695 - lr: 3.9063e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2188 - reconstruction_loss: 1.2667 - kl_loss: 4.7750 - lr: 3.9063e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2240 - reconstruction_loss: 1.2658 - kl_loss: 4.7757 - lr: 3.9063e-06\n",
      "Epoch 84/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2202 - reconstruction_loss: 1.2671 - kl_loss: 4.7757\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2203 - reconstruction_loss: 1.2663 - kl_loss: 4.7756 - lr: 3.9063e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2254 - reconstruction_loss: 1.2674 - kl_loss: 4.7727 - lr: 1.9531e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2117 - reconstruction_loss: 1.2647 - kl_loss: 4.7749 - lr: 1.9531e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2232 - reconstruction_loss: 1.2643 - kl_loss: 4.7725 - lr: 1.9531e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2209 - reconstruction_loss: 1.2653 - kl_loss: 4.7703 - lr: 1.9531e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2203 - reconstruction_loss: 1.2655 - kl_loss: 4.7684 - lr: 1.9531e-06\n",
      "Epoch 90/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.2182 - reconstruction_loss: 1.2665 - kl_loss: 4.7718\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2182 - reconstruction_loss: 1.2657 - kl_loss: 4.7716 - lr: 1.9531e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2231 - reconstruction_loss: 1.2640 - kl_loss: 4.7704 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2191 - reconstruction_loss: 1.2653 - kl_loss: 4.7689 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2204 - reconstruction_loss: 1.2653 - kl_loss: 4.7686 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2171 - reconstruction_loss: 1.2651 - kl_loss: 4.7688 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2179 - reconstruction_loss: 1.2656 - kl_loss: 4.7690 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2216 - reconstruction_loss: 1.2658 - kl_loss: 4.7744 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2251 - reconstruction_loss: 1.2645 - kl_loss: 4.7748 - lr: 1.0000e-06\n",
      "Epoch 98/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2191 - reconstruction_loss: 1.2644 - kl_loss: 4.7709 - lr: 1.0000e-06\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2140 - reconstruction_loss: 1.2662 - kl_loss: 4.7719 - lr: 1.0000e-06\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2196 - reconstruction_loss: 1.2661 - kl_loss: 4.7733 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2266 - reconstruction_loss: 1.2652 - kl_loss: 4.7718 - lr: 1.0000e-06\n",
      "Epoch 101: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 10:31:50,727]\u001b[0m Trial 27 finished with value: 0.00494170356291645 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 80, 'encoder_units_l2': 559, 'encoder_units_l3': 104, 'decoder_layers': 4, 'decoder_units_l1': 95, 'decoder_units_l2': 147, 'decoder_units_l3': 653, 'decoder_units_l4': 253, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6515 - reconstruction_loss: 1.0122 - kl_loss: 2.2539 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2741 - reconstruction_loss: 0.7418 - kl_loss: 2.6340 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2554 - reconstruction_loss: 0.7110 - kl_loss: 2.7006 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2472 - reconstruction_loss: 0.6961 - kl_loss: 2.7430 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2302 - reconstruction_loss: 0.6750 - kl_loss: 2.7821 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2289 - reconstruction_loss: 0.6624 - kl_loss: 2.8236 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2239 - reconstruction_loss: 0.6467 - kl_loss: 2.8696 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2165 - reconstruction_loss: 0.6397 - kl_loss: 2.8869 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2155 - reconstruction_loss: 0.6370 - kl_loss: 2.9072 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2089 - reconstruction_loss: 0.6229 - kl_loss: 2.9286 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2089 - reconstruction_loss: 0.6195 - kl_loss: 2.9510 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2118 - reconstruction_loss: 0.6144 - kl_loss: 2.9935 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2035 - reconstruction_loss: 0.5985 - kl_loss: 3.0376 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2024 - reconstruction_loss: 0.5795 - kl_loss: 3.1093 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1992 - reconstruction_loss: 0.5713 - kl_loss: 3.1234 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1982 - reconstruction_loss: 0.5688 - kl_loss: 3.1410 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1937 - reconstruction_loss: 0.5651 - kl_loss: 3.1458 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1903 - reconstruction_loss: 0.5630 - kl_loss: 3.1469 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1878 - reconstruction_loss: 0.5600 - kl_loss: 3.1540 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1942 - reconstruction_loss: 0.5645 - kl_loss: 3.1427 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1906 - reconstruction_loss: 0.5564 - kl_loss: 3.1688 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1827 - reconstruction_loss: 0.5495 - kl_loss: 3.1817 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1894 - reconstruction_loss: 0.5443 - kl_loss: 3.2028 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1826 - reconstruction_loss: 0.5391 - kl_loss: 3.2167 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1793 - reconstruction_loss: 0.5339 - kl_loss: 3.2255 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1776 - reconstruction_loss: 0.5328 - kl_loss: 3.2493 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1916 - reconstruction_loss: 0.5470 - kl_loss: 3.2164 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "1555/1563 [============================>.] - ETA: 0s - loss: 1.1858 - reconstruction_loss: 0.5427 - kl_loss: 3.2192\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1858 - reconstruction_loss: 0.5425 - kl_loss: 3.2191 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1635 - reconstruction_loss: 0.5137 - kl_loss: 3.2446 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1623 - reconstruction_loss: 0.5095 - kl_loss: 3.2596 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1617 - reconstruction_loss: 0.5095 - kl_loss: 3.2624 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1641 - reconstruction_loss: 0.5119 - kl_loss: 3.2620 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 1.1667 - reconstruction_loss: 0.5168 - kl_loss: 3.2314\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1667 - reconstruction_loss: 0.5167 - kl_loss: 3.2316 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1569 - reconstruction_loss: 0.4997 - kl_loss: 3.2768 - lr: 2.5000e-04\n",
      "Epoch 35/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1526 - reconstruction_loss: 0.4966 - kl_loss: 3.2820 - lr: 2.5000e-04\n",
      "Epoch 36/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1529 - reconstruction_loss: 0.4941 - kl_loss: 3.2843 - lr: 2.5000e-04\n",
      "Epoch 37/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1578 - reconstruction_loss: 0.4974 - kl_loss: 3.2809 - lr: 2.5000e-04\n",
      "Epoch 38/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1525 - reconstruction_loss: 0.4961 - kl_loss: 3.2716 - lr: 2.5000e-04\n",
      "Epoch 39/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1521 - reconstruction_loss: 0.4962 - kl_loss: 3.2742 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1517 - reconstruction_loss: 0.4999 - kl_loss: 3.2755 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 1.1542 - reconstruction_loss: 0.4966 - kl_loss: 3.2726\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1542 - reconstruction_loss: 0.4965 - kl_loss: 3.2729 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1478 - reconstruction_loss: 0.4923 - kl_loss: 3.2700 - lr: 1.2500e-04\n",
      "Epoch 43/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1450 - reconstruction_loss: 0.4896 - kl_loss: 3.2778 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1436 - reconstruction_loss: 0.4855 - kl_loss: 3.2933 - lr: 1.2500e-04\n",
      "Epoch 45/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1458 - reconstruction_loss: 0.4864 - kl_loss: 3.2930 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1457 - reconstruction_loss: 0.4859 - kl_loss: 3.2894 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1425 - reconstruction_loss: 0.4856 - kl_loss: 3.2867 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1406 - reconstruction_loss: 0.4876 - kl_loss: 3.2763 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1453 - reconstruction_loss: 0.4883 - kl_loss: 3.2809 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1493 - reconstruction_loss: 0.4895 - kl_loss: 3.2830 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 1.1416 - reconstruction_loss: 0.4851 - kl_loss: 3.2923\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1416 - reconstruction_loss: 0.4849 - kl_loss: 3.2923 - lr: 1.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1432 - reconstruction_loss: 0.4841 - kl_loss: 3.2956 - lr: 6.2500e-05\n",
      "Epoch 53/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1428 - reconstruction_loss: 0.4824 - kl_loss: 3.3036 - lr: 6.2500e-05\n",
      "Epoch 54/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1438 - reconstruction_loss: 0.4814 - kl_loss: 3.3055 - lr: 6.2500e-05\n",
      "Epoch 55/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1423 - reconstruction_loss: 0.4832 - kl_loss: 3.2925 - lr: 6.2500e-05\n",
      "Epoch 56/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1416 - reconstruction_loss: 0.4817 - kl_loss: 3.2928 - lr: 6.2500e-05\n",
      "Epoch 57/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1452 - reconstruction_loss: 0.4821 - kl_loss: 3.2965 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1423 - reconstruction_loss: 0.4825 - kl_loss: 3.2972 - lr: 6.2500e-05\n",
      "Epoch 59/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 1.1429 - reconstruction_loss: 0.4836 - kl_loss: 3.2922\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1429 - reconstruction_loss: 0.4835 - kl_loss: 3.2922 - lr: 6.2500e-05\n",
      "Epoch 60/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1401 - reconstruction_loss: 0.4827 - kl_loss: 3.2996 - lr: 3.1250e-05\n",
      "Epoch 61/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1406 - reconstruction_loss: 0.4812 - kl_loss: 3.2933 - lr: 3.1250e-05\n",
      "Epoch 62/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1400 - reconstruction_loss: 0.4822 - kl_loss: 3.2864 - lr: 3.1250e-05\n",
      "Epoch 63/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1410 - reconstruction_loss: 0.4837 - kl_loss: 3.2904 - lr: 3.1250e-05\n",
      "Epoch 64/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1373 - reconstruction_loss: 0.4826 - kl_loss: 3.2807 - lr: 3.1250e-05\n",
      "Epoch 65/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1409 - reconstruction_loss: 0.4830 - kl_loss: 3.2860 - lr: 3.1250e-05\n",
      "Epoch 66/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1397 - reconstruction_loss: 0.4810 - kl_loss: 3.2865 - lr: 3.1250e-05\n",
      "Epoch 67/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1397 - reconstruction_loss: 0.4822 - kl_loss: 3.2921 - lr: 3.1250e-05\n",
      "Epoch 68/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1396 - reconstruction_loss: 0.4804 - kl_loss: 3.2915 - lr: 3.1250e-05\n",
      "Epoch 69/300\n",
      "1557/1563 [============================>.] - ETA: 0s - loss: 1.1400 - reconstruction_loss: 0.4827 - kl_loss: 3.2856\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1400 - reconstruction_loss: 0.4825 - kl_loss: 3.2857 - lr: 3.1250e-05\n",
      "Epoch 70/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1395 - reconstruction_loss: 0.4813 - kl_loss: 3.2929 - lr: 1.5625e-05\n",
      "Epoch 71/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1412 - reconstruction_loss: 0.4816 - kl_loss: 3.2871 - lr: 1.5625e-05\n",
      "Epoch 72/300\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 1.1399 - reconstruction_loss: 0.4806 - kl_loss: 3.2964\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1399 - reconstruction_loss: 0.4805 - kl_loss: 3.2963 - lr: 1.5625e-05\n",
      "Epoch 73/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1396 - reconstruction_loss: 0.4808 - kl_loss: 3.2923 - lr: 7.8125e-06\n",
      "Epoch 74/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1385 - reconstruction_loss: 0.4813 - kl_loss: 3.2912 - lr: 7.8125e-06\n",
      "Epoch 75/300\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 1.1383 - reconstruction_loss: 0.4806 - kl_loss: 3.2969\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1383 - reconstruction_loss: 0.4805 - kl_loss: 3.2973 - lr: 7.8125e-06\n",
      "Epoch 76/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1394 - reconstruction_loss: 0.4792 - kl_loss: 3.2949 - lr: 3.9063e-06\n",
      "Epoch 77/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1372 - reconstruction_loss: 0.4797 - kl_loss: 3.2877 - lr: 3.9063e-06\n",
      "Epoch 78/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1389 - reconstruction_loss: 0.4812 - kl_loss: 3.2912 - lr: 3.9063e-06\n",
      "Epoch 79/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1379 - reconstruction_loss: 0.4802 - kl_loss: 3.2928 - lr: 3.9063e-06\n",
      "Epoch 80/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 1.1410 - reconstruction_loss: 0.4808 - kl_loss: 3.2940\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1410 - reconstruction_loss: 0.4807 - kl_loss: 3.2939 - lr: 3.9063e-06\n",
      "Epoch 81/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1355 - reconstruction_loss: 0.4804 - kl_loss: 3.2904 - lr: 1.9531e-06\n",
      "Epoch 82/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1361 - reconstruction_loss: 0.4794 - kl_loss: 3.2884 - lr: 1.9531e-06\n",
      "Epoch 83/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1409 - reconstruction_loss: 0.4812 - kl_loss: 3.2891 - lr: 1.9531e-06\n",
      "Epoch 84/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1374 - reconstruction_loss: 0.4811 - kl_loss: 3.2916 - lr: 1.9531e-06\n",
      "Epoch 85/300\n",
      "1554/1563 [============================>.] - ETA: 0s - loss: 1.1395 - reconstruction_loss: 0.4801 - kl_loss: 3.2929\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1395 - reconstruction_loss: 0.4800 - kl_loss: 3.2928 - lr: 1.9531e-06\n",
      "Epoch 86/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1357 - reconstruction_loss: 0.4795 - kl_loss: 3.2914 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1387 - reconstruction_loss: 0.4806 - kl_loss: 3.2906 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1386 - reconstruction_loss: 0.4809 - kl_loss: 3.2909 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1376 - reconstruction_loss: 0.4810 - kl_loss: 3.2912 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1382 - reconstruction_loss: 0.4792 - kl_loss: 3.2929 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1391 - reconstruction_loss: 0.4791 - kl_loss: 3.2931 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1388 - reconstruction_loss: 0.4808 - kl_loss: 3.2937 - lr: 1.0000e-06\n",
      "Epoch 92: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 10:43:48,814]\u001b[0m Trial 28 finished with value: 0.00748916143417825 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 45, 'encoder_units_l2': 327, 'encoder_units_l3': 754, 'encoder_units_l4': 87, 'decoder_layers': 8, 'decoder_units_l1': 38, 'decoder_units_l2': 310, 'decoder_units_l3': 142, 'decoder_units_l4': 16, 'decoder_units_l5': 54, 'decoder_units_l6': 381, 'decoder_units_l7': 277, 'decoder_units_l8': 838, 'beta': 0.2, 'lr': 0.001, 'batch_size': 64}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 11ms/step - loss: 21.8737 - reconstruction_loss: 21.6625 - kl_loss: 0.0018 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7156 - reconstruction_loss: 21.6343 - kl_loss: 1.9418e-09 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7923 - reconstruction_loss: 21.6354 - kl_loss: 5.9443e-10 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7220 - reconstruction_loss: 21.6355 - kl_loss: 3.3321e-07 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 21.7150 - reconstruction_loss: 21.6373 - kl_loss: 2.3669e-10\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7146 - reconstruction_loss: 21.6373 - kl_loss: 2.3669e-10 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7183 - reconstruction_loss: 21.6331 - kl_loss: 1.2087e-10 - lr: 0.0050\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7550 - reconstruction_loss: 21.6330 - kl_loss: 9.0044e-11 - lr: 0.0050\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6840 - reconstruction_loss: 21.6328 - kl_loss: 7.3057e-11 - lr: 0.0050\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7406 - reconstruction_loss: 21.6336 - kl_loss: 6.5038e-11 - lr: 0.0050\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7273 - reconstruction_loss: 21.6326 - kl_loss: 5.6426e-11 - lr: 0.0050\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7035 - reconstruction_loss: 21.6328 - kl_loss: 5.1080e-11 - lr: 0.0050\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7096 - reconstruction_loss: 21.6335 - kl_loss: 4.4309e-11 - lr: 0.0050\n",
      "Epoch 13/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 21.7435 - reconstruction_loss: 21.7064 - kl_loss: 2.1333e-08\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7417 - reconstruction_loss: 21.6329 - kl_loss: 2.0791e-08 - lr: 0.0050\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6925 - reconstruction_loss: 21.6322 - kl_loss: 3.8310e-11 - lr: 0.0025\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7551 - reconstruction_loss: 21.6320 - kl_loss: 3.3262e-11 - lr: 0.0025\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7347 - reconstruction_loss: 21.6309 - kl_loss: 3.0292e-11 - lr: 0.0025\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6503 - reconstruction_loss: 21.6314 - kl_loss: 2.7025e-11 - lr: 0.0025\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6564 - reconstruction_loss: 21.6316 - kl_loss: 2.5837e-11 - lr: 0.0025\n",
      "Epoch 19/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 21.7006 - reconstruction_loss: 21.7055 - kl_loss: 2.3582e-11\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6999 - reconstruction_loss: 21.6314 - kl_loss: 2.3461e-11 - lr: 0.0025\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7065 - reconstruction_loss: 21.6310 - kl_loss: 2.1382e-11 - lr: 0.0012\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6670 - reconstruction_loss: 21.6311 - kl_loss: 2.0194e-11 - lr: 0.0012\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 21.7247 - reconstruction_loss: 21.6308 - kl_loss: 1.9304e-11\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7243 - reconstruction_loss: 21.6308 - kl_loss: 1.9304e-11 - lr: 0.0012\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6887 - reconstruction_loss: 21.6306 - kl_loss: 1.9304e-11 - lr: 6.2500e-04\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6425 - reconstruction_loss: 21.6305 - kl_loss: 1.8413e-11 - lr: 6.2500e-04\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7525 - reconstruction_loss: 21.6307 - kl_loss: 1.8116e-11 - lr: 6.2500e-04\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7574 - reconstruction_loss: 21.6307 - kl_loss: 1.8472e-11 - lr: 6.2500e-04\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 21.7486 - reconstruction_loss: 21.6306 - kl_loss: 1.7819e-11\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7480 - reconstruction_loss: 21.6306 - kl_loss: 1.7819e-11 - lr: 6.2500e-04\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7188 - reconstruction_loss: 21.6305 - kl_loss: 1.7522e-11 - lr: 3.1250e-04\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6988 - reconstruction_loss: 21.6303 - kl_loss: 1.7225e-11 - lr: 3.1250e-04\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6870 - reconstruction_loss: 21.6304 - kl_loss: 1.7522e-11 - lr: 3.1250e-04\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6929 - reconstruction_loss: 21.6304 - kl_loss: 1.6928e-11 - lr: 3.1250e-04\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 21.6683 - reconstruction_loss: 21.6304 - kl_loss: 1.6334e-11\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6681 - reconstruction_loss: 21.6304 - kl_loss: 1.6334e-11 - lr: 3.1250e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.5960 - reconstruction_loss: 21.6303 - kl_loss: 1.6631e-11 - lr: 1.5625e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7154 - reconstruction_loss: 21.6303 - kl_loss: 1.6334e-11 - lr: 1.5625e-04\n",
      "Epoch 35/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 21.7016 - reconstruction_loss: 21.7060 - kl_loss: 1.6119e-11\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7009 - reconstruction_loss: 21.6303 - kl_loss: 1.6037e-11 - lr: 1.5625e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 21.7100 - reconstruction_loss: 21.6302 - kl_loss: 1.6037e-11 - lr: 7.8125e-05\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7122 - reconstruction_loss: 21.6302 - kl_loss: 1.6334e-11 - lr: 7.8125e-05\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6850 - reconstruction_loss: 21.6302 - kl_loss: 1.6334e-11 - lr: 7.8125e-05\n",
      "Epoch 39/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 21.7296 - reconstruction_loss: 21.7117 - kl_loss: 1.5542e-11\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7282 - reconstruction_loss: 21.6302 - kl_loss: 1.5740e-11 - lr: 7.8125e-05\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 21.6596 - reconstruction_loss: 21.6302 - kl_loss: 1.6334e-11 - lr: 3.9062e-05\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 21.7009 - reconstruction_loss: 21.6302 - kl_loss: 1.5740e-11 - lr: 3.9062e-05\n",
      "Epoch 42/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 21.6676 - reconstruction_loss: 21.7043 - kl_loss: 1.5542e-11\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 21.6680 - reconstruction_loss: 21.6302 - kl_loss: 1.5146e-11 - lr: 3.9062e-05\n",
      "Epoch 43/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6893 - reconstruction_loss: 21.6302 - kl_loss: 1.5740e-11 - lr: 1.9531e-05\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7263 - reconstruction_loss: 21.6301 - kl_loss: 1.5740e-11 - lr: 1.9531e-05\n",
      "Epoch 45/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 21.7319 - reconstruction_loss: 21.7144 - kl_loss: 1.5847e-11\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7304 - reconstruction_loss: 21.6302 - kl_loss: 1.5740e-11 - lr: 1.9531e-05\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6963 - reconstruction_loss: 21.6301 - kl_loss: 1.6393e-11 - lr: 9.7656e-06\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7147 - reconstruction_loss: 21.6302 - kl_loss: 1.5740e-11 - lr: 9.7656e-06\n",
      "Epoch 48/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 21.7893 - reconstruction_loss: 21.7012 - kl_loss: 1.6152e-11\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7859 - reconstruction_loss: 21.6301 - kl_loss: 1.6037e-11 - lr: 9.7656e-06\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7336 - reconstruction_loss: 21.6301 - kl_loss: 1.5443e-11 - lr: 4.8828e-06\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7207 - reconstruction_loss: 21.6301 - kl_loss: 1.5443e-11 - lr: 4.8828e-06\n",
      "Epoch 51/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 21.6456 - reconstruction_loss: 21.7110 - kl_loss: 1.5238e-11\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6467 - reconstruction_loss: 21.6301 - kl_loss: 1.6096e-11 - lr: 4.8828e-06\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6789 - reconstruction_loss: 21.6301 - kl_loss: 1.5443e-11 - lr: 2.4414e-06\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6952 - reconstruction_loss: 21.6301 - kl_loss: 1.5443e-11 - lr: 2.4414e-06\n",
      "Epoch 54/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 21.6138 - reconstruction_loss: 21.7087 - kl_loss: 1.5542e-11\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6159 - reconstruction_loss: 21.6301 - kl_loss: 1.5740e-11 - lr: 2.4414e-06\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6748 - reconstruction_loss: 21.6301 - kl_loss: 1.5443e-11 - lr: 1.2207e-06\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 21.7307 - reconstruction_loss: 21.6301 - kl_loss: 1.5443e-11 - lr: 1.2207e-06\n",
      "Epoch 57/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 21.7309 - reconstruction_loss: 21.7041 - kl_loss: 1.5847e-11\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7294 - reconstruction_loss: 21.6301 - kl_loss: 1.5443e-11 - lr: 1.2207e-06\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.7238 - reconstruction_loss: 21.6301 - kl_loss: 1.5740e-11 - lr: 1.0000e-06\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 21.6880 - reconstruction_loss: 21.6301 - kl_loss: 1.5443e-11 - lr: 1.0000e-06\n",
      "Epoch 59: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 10:45:53,162]\u001b[0m Trial 29 finished with value: 0.04239506866859282 and parameters: {'encoder_layers': 5, 'encoder_units_l1': 19, 'encoder_units_l2': 226, 'encoder_units_l3': 476, 'encoder_units_l4': 455, 'encoder_units_l5': 373, 'decoder_layers': 7, 'decoder_units_l1': 35, 'decoder_units_l2': 91, 'decoder_units_l3': 94, 'decoder_units_l4': 106, 'decoder_units_l5': 200, 'decoder_units_l6': 209, 'decoder_units_l7': 72, 'beta': 5, 'lr': 0.01, 'batch_size': 512}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 11ms/step - loss: 19.8088 - reconstruction_loss: 16.0715 - kl_loss: 0.9198 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 11.0775 - reconstruction_loss: 8.1672 - kl_loss: 2.5175 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 10.5198 - reconstruction_loss: 7.9112 - kl_loss: 2.5783 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 10.4622 - reconstruction_loss: 7.8421 - kl_loss: 2.5809 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 10.3968 - reconstruction_loss: 7.7630 - kl_loss: 2.5904 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 10.2615 - reconstruction_loss: 7.4997 - kl_loss: 2.6608 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 9.8505 - reconstruction_loss: 6.8733 - kl_loss: 2.8668 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 9.4464 - reconstruction_loss: 6.3166 - kl_loss: 2.9944 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 9.0034 - reconstruction_loss: 5.8576 - kl_loss: 3.0905 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.8309 - reconstruction_loss: 5.6270 - kl_loss: 3.1358 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.6900 - reconstruction_loss: 5.4914 - kl_loss: 3.1759 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.6180 - reconstruction_loss: 5.3819 - kl_loss: 3.1950 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.5270 - reconstruction_loss: 5.3002 - kl_loss: 3.2067 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.4981 - reconstruction_loss: 5.2472 - kl_loss: 3.2130 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.4141 - reconstruction_loss: 5.1913 - kl_loss: 3.2154 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 8.4166 - reconstruction_loss: 5.1296 - kl_loss: 3.2431 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.3522 - reconstruction_loss: 5.0740 - kl_loss: 3.2428 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 8.2960 - reconstruction_loss: 5.0295 - kl_loss: 3.2539 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.2823 - reconstruction_loss: 4.9896 - kl_loss: 3.2555 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.2603 - reconstruction_loss: 4.9604 - kl_loss: 3.2799 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.2314 - reconstruction_loss: 4.9168 - kl_loss: 3.2760 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.2045 - reconstruction_loss: 4.9022 - kl_loss: 3.2796 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.1804 - reconstruction_loss: 4.8570 - kl_loss: 3.2980 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.1709 - reconstruction_loss: 4.8449 - kl_loss: 3.3065 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.1434 - reconstruction_loss: 4.8094 - kl_loss: 3.3109 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.1508 - reconstruction_loss: 4.7947 - kl_loss: 3.3126 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.1111 - reconstruction_loss: 4.7714 - kl_loss: 3.3184 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.0735 - reconstruction_loss: 4.7460 - kl_loss: 3.3269 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.0884 - reconstruction_loss: 4.7279 - kl_loss: 3.3395 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.0849 - reconstruction_loss: 4.7206 - kl_loss: 3.3341 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.0624 - reconstruction_loss: 4.6800 - kl_loss: 3.3525 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.0422 - reconstruction_loss: 4.6652 - kl_loss: 3.3522 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.0328 - reconstruction_loss: 4.6602 - kl_loss: 3.3466 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.0116 - reconstruction_loss: 4.6323 - kl_loss: 3.3599 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.0247 - reconstruction_loss: 4.6163 - kl_loss: 3.3765 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.9917 - reconstruction_loss: 4.6131 - kl_loss: 3.3645 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.9748 - reconstruction_loss: 4.5952 - kl_loss: 3.3797 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.9752 - reconstruction_loss: 4.5726 - kl_loss: 3.3809 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.9534 - reconstruction_loss: 4.5480 - kl_loss: 3.3850 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.9498 - reconstruction_loss: 4.5354 - kl_loss: 3.3966 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.9642 - reconstruction_loss: 4.5382 - kl_loss: 3.3973 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.9460 - reconstruction_loss: 4.5250 - kl_loss: 3.4018 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.9162 - reconstruction_loss: 4.4997 - kl_loss: 3.3963 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.9232 - reconstruction_loss: 4.4811 - kl_loss: 3.4120 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.9122 - reconstruction_loss: 4.4834 - kl_loss: 3.4151 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.8971 - reconstruction_loss: 4.4564 - kl_loss: 3.4191 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.8691 - reconstruction_loss: 4.4404 - kl_loss: 3.4225 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.8746 - reconstruction_loss: 4.4397 - kl_loss: 3.4329 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.8892 - reconstruction_loss: 4.4187 - kl_loss: 3.4295 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.8836 - reconstruction_loss: 4.4103 - kl_loss: 3.4444 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.8651 - reconstruction_loss: 4.3968 - kl_loss: 3.4444 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.8187 - reconstruction_loss: 4.3809 - kl_loss: 3.4448 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.8503 - reconstruction_loss: 4.3652 - kl_loss: 3.4492 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.8508 - reconstruction_loss: 4.3603 - kl_loss: 3.4610 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.8239 - reconstruction_loss: 4.3440 - kl_loss: 3.4666 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.8216 - reconstruction_loss: 4.3376 - kl_loss: 3.4673 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.8069 - reconstruction_loss: 4.3338 - kl_loss: 3.4700 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.8074 - reconstruction_loss: 4.3108 - kl_loss: 3.4780 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7999 - reconstruction_loss: 4.2923 - kl_loss: 3.4865 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7853 - reconstruction_loss: 4.2842 - kl_loss: 3.4793 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.8200 - reconstruction_loss: 4.2893 - kl_loss: 3.4882 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7850 - reconstruction_loss: 4.2646 - kl_loss: 3.4951 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7818 - reconstruction_loss: 4.2535 - kl_loss: 3.4995 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7623 - reconstruction_loss: 4.2392 - kl_loss: 3.5077 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7296 - reconstruction_loss: 4.2265 - kl_loss: 3.5039 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7374 - reconstruction_loss: 4.2148 - kl_loss: 3.5148 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7460 - reconstruction_loss: 4.2178 - kl_loss: 3.5151 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7376 - reconstruction_loss: 4.1978 - kl_loss: 3.5168 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7391 - reconstruction_loss: 4.2027 - kl_loss: 3.5189 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7125 - reconstruction_loss: 4.1787 - kl_loss: 3.5321 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7276 - reconstruction_loss: 4.1665 - kl_loss: 3.5323 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7251 - reconstruction_loss: 4.1500 - kl_loss: 3.5404 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6903 - reconstruction_loss: 4.1579 - kl_loss: 3.5367 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6809 - reconstruction_loss: 4.1398 - kl_loss: 3.5388 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7050 - reconstruction_loss: 4.1411 - kl_loss: 3.5406 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6893 - reconstruction_loss: 4.1302 - kl_loss: 3.5438 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6799 - reconstruction_loss: 4.1181 - kl_loss: 3.5499 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6585 - reconstruction_loss: 4.0998 - kl_loss: 3.5557 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6733 - reconstruction_loss: 4.0987 - kl_loss: 3.5509 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6682 - reconstruction_loss: 4.0984 - kl_loss: 3.5573 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6517 - reconstruction_loss: 4.0835 - kl_loss: 3.5677 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6662 - reconstruction_loss: 4.0624 - kl_loss: 3.5793 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6338 - reconstruction_loss: 4.0594 - kl_loss: 3.5642 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6439 - reconstruction_loss: 4.0628 - kl_loss: 3.5798 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6267 - reconstruction_loss: 4.0343 - kl_loss: 3.5855 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6338 - reconstruction_loss: 4.0285 - kl_loss: 3.5837 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6146 - reconstruction_loss: 4.0163 - kl_loss: 3.5953 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6268 - reconstruction_loss: 4.0017 - kl_loss: 3.5989 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6163 - reconstruction_loss: 3.9883 - kl_loss: 3.6019 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5825 - reconstruction_loss: 3.9823 - kl_loss: 3.6056 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6256 - reconstruction_loss: 3.9833 - kl_loss: 3.6024 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5870 - reconstruction_loss: 3.9734 - kl_loss: 3.6023 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5799 - reconstruction_loss: 3.9505 - kl_loss: 3.6111 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6077 - reconstruction_loss: 3.9550 - kl_loss: 3.6151 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5744 - reconstruction_loss: 3.9354 - kl_loss: 3.6132 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5798 - reconstruction_loss: 3.9306 - kl_loss: 3.6271 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5612 - reconstruction_loss: 3.9361 - kl_loss: 3.6185 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5662 - reconstruction_loss: 3.9311 - kl_loss: 3.6169 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5740 - reconstruction_loss: 3.9133 - kl_loss: 3.6269 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5551 - reconstruction_loss: 3.9197 - kl_loss: 3.6281 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5384 - reconstruction_loss: 3.8991 - kl_loss: 3.6223 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5181 - reconstruction_loss: 3.8944 - kl_loss: 3.6287 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5226 - reconstruction_loss: 3.8983 - kl_loss: 3.6354 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5368 - reconstruction_loss: 3.8680 - kl_loss: 3.6500 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5375 - reconstruction_loss: 3.8725 - kl_loss: 3.6423 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5149 - reconstruction_loss: 3.8589 - kl_loss: 3.6431 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5054 - reconstruction_loss: 3.8619 - kl_loss: 3.6450 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5080 - reconstruction_loss: 3.8486 - kl_loss: 3.6463 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5238 - reconstruction_loss: 3.8571 - kl_loss: 3.6456 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5101 - reconstruction_loss: 3.8379 - kl_loss: 3.6517 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5020 - reconstruction_loss: 3.8279 - kl_loss: 3.6628 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4902 - reconstruction_loss: 3.8357 - kl_loss: 3.6458 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4925 - reconstruction_loss: 3.8186 - kl_loss: 3.6584 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4949 - reconstruction_loss: 3.8057 - kl_loss: 3.6702 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4787 - reconstruction_loss: 3.8230 - kl_loss: 3.6591 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4955 - reconstruction_loss: 3.8129 - kl_loss: 3.6569 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4798 - reconstruction_loss: 3.7971 - kl_loss: 3.6587 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4631 - reconstruction_loss: 3.7852 - kl_loss: 3.6753 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4856 - reconstruction_loss: 3.7867 - kl_loss: 3.6661 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4728 - reconstruction_loss: 3.7858 - kl_loss: 3.6721 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4602 - reconstruction_loss: 3.7832 - kl_loss: 3.6740 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4572 - reconstruction_loss: 3.7706 - kl_loss: 3.6785 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4592 - reconstruction_loss: 3.7658 - kl_loss: 3.6810 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4629 - reconstruction_loss: 3.7599 - kl_loss: 3.6799 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4448 - reconstruction_loss: 3.7541 - kl_loss: 3.6828 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4660 - reconstruction_loss: 3.7559 - kl_loss: 3.6807 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4492 - reconstruction_loss: 3.7387 - kl_loss: 3.6874 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4477 - reconstruction_loss: 3.7494 - kl_loss: 3.6915 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4515 - reconstruction_loss: 3.7355 - kl_loss: 3.6892 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4267 - reconstruction_loss: 3.7312 - kl_loss: 3.6844 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4269 - reconstruction_loss: 3.7208 - kl_loss: 3.6927 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4241 - reconstruction_loss: 3.7208 - kl_loss: 3.6904 - lr: 1.0000e-04\n",
      "Epoch 133/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4185 - reconstruction_loss: 3.7139 - kl_loss: 3.6873 - lr: 1.0000e-04\n",
      "Epoch 134/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4245 - reconstruction_loss: 3.7184 - kl_loss: 3.6922 - lr: 1.0000e-04\n",
      "Epoch 135/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3982 - reconstruction_loss: 3.7088 - kl_loss: 3.6896 - lr: 1.0000e-04\n",
      "Epoch 136/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4230 - reconstruction_loss: 3.7136 - kl_loss: 3.6928 - lr: 1.0000e-04\n",
      "Epoch 137/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3973 - reconstruction_loss: 3.6924 - kl_loss: 3.6937 - lr: 1.0000e-04\n",
      "Epoch 138/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3996 - reconstruction_loss: 3.6740 - kl_loss: 3.7005 - lr: 1.0000e-04\n",
      "Epoch 139/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4011 - reconstruction_loss: 3.6800 - kl_loss: 3.7117 - lr: 1.0000e-04\n",
      "Epoch 140/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3902 - reconstruction_loss: 3.6719 - kl_loss: 3.7084 - lr: 1.0000e-04\n",
      "Epoch 141/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 7.3944 - reconstruction_loss: 3.6969 - kl_loss: 3.7057\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3944 - reconstruction_loss: 3.6840 - kl_loss: 3.7064 - lr: 1.0000e-04\n",
      "Epoch 142/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3653 - reconstruction_loss: 3.6473 - kl_loss: 3.7067 - lr: 5.0000e-05\n",
      "Epoch 143/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3522 - reconstruction_loss: 3.6420 - kl_loss: 3.7121 - lr: 5.0000e-05\n",
      "Epoch 144/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3893 - reconstruction_loss: 3.6373 - kl_loss: 3.7176 - lr: 5.0000e-05\n",
      "Epoch 145/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3595 - reconstruction_loss: 3.6296 - kl_loss: 3.7153 - lr: 5.0000e-05\n",
      "Epoch 146/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3642 - reconstruction_loss: 3.6268 - kl_loss: 3.7242 - lr: 5.0000e-05\n",
      "Epoch 147/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3618 - reconstruction_loss: 3.6272 - kl_loss: 3.7301 - lr: 5.0000e-05\n",
      "Epoch 148/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3595 - reconstruction_loss: 3.6139 - kl_loss: 3.7245 - lr: 5.0000e-05\n",
      "Epoch 149/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3631 - reconstruction_loss: 3.6220 - kl_loss: 3.7203 - lr: 5.0000e-05\n",
      "Epoch 150/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3548 - reconstruction_loss: 3.6227 - kl_loss: 3.7134 - lr: 5.0000e-05\n",
      "Epoch 151/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3564 - reconstruction_loss: 3.6191 - kl_loss: 3.7253 - lr: 5.0000e-05\n",
      "Epoch 152/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3498 - reconstruction_loss: 3.6224 - kl_loss: 3.7226 - lr: 5.0000e-05\n",
      "Epoch 153/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 7.3685 - reconstruction_loss: 3.6302 - kl_loss: 3.7236\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.3679 - reconstruction_loss: 3.6151 - kl_loss: 3.7237 - lr: 5.0000e-05\n",
      "Epoch 154/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.3112 - reconstruction_loss: 3.5904 - kl_loss: 3.7188 - lr: 2.5000e-05\n",
      "Epoch 155/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3205 - reconstruction_loss: 3.5910 - kl_loss: 3.7278 - lr: 2.5000e-05\n",
      "Epoch 156/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3185 - reconstruction_loss: 3.5866 - kl_loss: 3.7336 - lr: 2.5000e-05\n",
      "Epoch 157/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 7.3293 - reconstruction_loss: 3.6016 - kl_loss: 3.7277\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3292 - reconstruction_loss: 3.5892 - kl_loss: 3.7278 - lr: 2.5000e-05\n",
      "Epoch 158/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3275 - reconstruction_loss: 3.5747 - kl_loss: 3.7352 - lr: 1.2500e-05\n",
      "Epoch 159/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3336 - reconstruction_loss: 3.5758 - kl_loss: 3.7355 - lr: 1.2500e-05\n",
      "Epoch 160/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3157 - reconstruction_loss: 3.5730 - kl_loss: 3.7312 - lr: 1.2500e-05\n",
      "Epoch 161/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.2967 - reconstruction_loss: 3.5704 - kl_loss: 3.7348 - lr: 1.2500e-05\n",
      "Epoch 162/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3155 - reconstruction_loss: 3.5755 - kl_loss: 3.7355 - lr: 1.2500e-05\n",
      "Epoch 163/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 7.3227 - reconstruction_loss: 3.5756 - kl_loss: 3.7360\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3227 - reconstruction_loss: 3.5756 - kl_loss: 3.7360 - lr: 1.2500e-05\n",
      "Epoch 164/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3161 - reconstruction_loss: 3.5588 - kl_loss: 3.7428 - lr: 6.2500e-06\n",
      "Epoch 165/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.2989 - reconstruction_loss: 3.5681 - kl_loss: 3.7317 - lr: 6.2500e-06\n",
      "Epoch 166/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3036 - reconstruction_loss: 3.5690 - kl_loss: 3.7384 - lr: 6.2500e-06\n",
      "Epoch 167/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3179 - reconstruction_loss: 3.5605 - kl_loss: 3.7403 - lr: 6.2500e-06\n",
      "Epoch 168/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 7.3206 - reconstruction_loss: 3.5741 - kl_loss: 3.7375\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3203 - reconstruction_loss: 3.5623 - kl_loss: 3.7389 - lr: 6.2500e-06\n",
      "Epoch 169/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3224 - reconstruction_loss: 3.5612 - kl_loss: 3.7357 - lr: 3.1250e-06\n",
      "Epoch 170/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3062 - reconstruction_loss: 3.5661 - kl_loss: 3.7355 - lr: 3.1250e-06\n",
      "Epoch 171/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.3321 - reconstruction_loss: 3.5574 - kl_loss: 3.7398 - lr: 3.1250e-06\n",
      "Epoch 172/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 7.3120 - reconstruction_loss: 3.5595 - kl_loss: 3.7418\n",
      "Epoch 172: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3119 - reconstruction_loss: 3.5595 - kl_loss: 3.7418 - lr: 3.1250e-06\n",
      "Epoch 173/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3080 - reconstruction_loss: 3.5524 - kl_loss: 3.7382 - lr: 1.5625e-06\n",
      "Epoch 174/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3013 - reconstruction_loss: 3.5560 - kl_loss: 3.7407 - lr: 1.5625e-06\n",
      "Epoch 175/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.3176 - reconstruction_loss: 3.5515 - kl_loss: 3.7393 - lr: 1.5625e-06\n",
      "Epoch 176/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 7.3106 - reconstruction_loss: 3.5561 - kl_loss: 3.7393\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3105 - reconstruction_loss: 3.5561 - kl_loss: 3.7393 - lr: 1.5625e-06\n",
      "Epoch 177/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3035 - reconstruction_loss: 3.5628 - kl_loss: 3.7387 - lr: 1.0000e-06\n",
      "Epoch 178/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3019 - reconstruction_loss: 3.5566 - kl_loss: 3.7393 - lr: 1.0000e-06\n",
      "Epoch 179/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3044 - reconstruction_loss: 3.5611 - kl_loss: 3.7399 - lr: 1.0000e-06\n",
      "Epoch 180/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3058 - reconstruction_loss: 3.5615 - kl_loss: 3.7392 - lr: 1.0000e-06\n",
      "Epoch 181/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3039 - reconstruction_loss: 3.5502 - kl_loss: 3.7379 - lr: 1.0000e-06\n",
      "Epoch 182/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3243 - reconstruction_loss: 3.5620 - kl_loss: 3.7380 - lr: 1.0000e-06\n",
      "Epoch 183/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3043 - reconstruction_loss: 3.5554 - kl_loss: 3.7384 - lr: 1.0000e-06\n",
      "Epoch 184/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3017 - reconstruction_loss: 3.5522 - kl_loss: 3.7370 - lr: 1.0000e-06\n",
      "Epoch 185/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3062 - reconstruction_loss: 3.5582 - kl_loss: 3.7380 - lr: 1.0000e-06\n",
      "Epoch 186/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3015 - reconstruction_loss: 3.5567 - kl_loss: 3.7386 - lr: 1.0000e-06\n",
      "Epoch 187/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3154 - reconstruction_loss: 3.5555 - kl_loss: 3.7398 - lr: 1.0000e-06\n",
      "Epoch 188/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.2990 - reconstruction_loss: 3.5552 - kl_loss: 3.7396 - lr: 1.0000e-06\n",
      "Epoch 189/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3211 - reconstruction_loss: 3.5577 - kl_loss: 3.7428 - lr: 1.0000e-06\n",
      "Epoch 190/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3054 - reconstruction_loss: 3.5549 - kl_loss: 3.7424 - lr: 1.0000e-06\n",
      "Epoch 191/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3090 - reconstruction_loss: 3.5535 - kl_loss: 3.7425 - lr: 1.0000e-06\n",
      "Epoch 191: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 10:52:51,510]\u001b[0m Trial 30 finished with value: 0.0069525387766369105 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 240, 'encoder_units_l2': 131, 'encoder_units_l3': 324, 'decoder_layers': 7, 'decoder_units_l1': 735, 'decoder_units_l2': 52, 'decoder_units_l3': 230, 'decoder_units_l4': 453, 'decoder_units_l5': 88, 'decoder_units_l6': 583, 'decoder_units_l7': 28, 'beta': 1, 'lr': 0.0001, 'batch_size': 512}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 7.0853 - reconstruction_loss: 5.2342 - kl_loss: 2.5874 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 3.4305 - reconstruction_loss: 2.3903 - kl_loss: 4.1377 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 3.0319 - reconstruction_loss: 2.0868 - kl_loss: 4.3936 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.8124 - reconstruction_loss: 1.8700 - kl_loss: 4.5986 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.7173 - reconstruction_loss: 1.7377 - kl_loss: 4.7116 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.6073 - reconstruction_loss: 1.6124 - kl_loss: 4.8140 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.5297 - reconstruction_loss: 1.5413 - kl_loss: 4.8358 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.6771 - reconstruction_loss: 1.5819 - kl_loss: 4.8477 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4846 - reconstruction_loss: 1.5164 - kl_loss: 4.8922 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4291 - reconstruction_loss: 1.4508 - kl_loss: 4.8655 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4828 - reconstruction_loss: 1.5447 - kl_loss: 4.9336 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 3.0240 - reconstruction_loss: 2.0888 - kl_loss: 4.6946 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.8522 - reconstruction_loss: 1.8816 - kl_loss: 4.5810\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.8519 - reconstruction_loss: 1.8799 - kl_loss: 4.5813 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.6592 - reconstruction_loss: 1.6808 - kl_loss: 4.6910 - lr: 5.0000e-04\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5729 - reconstruction_loss: 1.5844 - kl_loss: 4.7665 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.5014 - reconstruction_loss: 1.5278 - kl_loss: 4.8151\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.5012 - reconstruction_loss: 1.5256 - kl_loss: 4.8148 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4135 - reconstruction_loss: 1.4398 - kl_loss: 4.8245 - lr: 2.5000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3721 - reconstruction_loss: 1.4097 - kl_loss: 4.8418 - lr: 2.5000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3676 - reconstruction_loss: 1.4027 - kl_loss: 4.8405 - lr: 2.5000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3818 - reconstruction_loss: 1.4033 - kl_loss: 4.8530 - lr: 2.5000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3663 - reconstruction_loss: 1.3938 - kl_loss: 4.8795 - lr: 2.5000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3677 - reconstruction_loss: 1.4038 - kl_loss: 4.9034 - lr: 2.5000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3554 - reconstruction_loss: 1.3789 - kl_loss: 4.8744 - lr: 2.5000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3564 - reconstruction_loss: 1.3658 - kl_loss: 4.9112 - lr: 2.5000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3466 - reconstruction_loss: 1.3717 - kl_loss: 4.8780 - lr: 2.5000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3430 - reconstruction_loss: 1.3753 - kl_loss: 4.9514 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3722 - reconstruction_loss: 1.3650 - kl_loss: 4.9008 - lr: 2.5000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 4s 12ms/step - loss: 2.3086 - reconstruction_loss: 1.3433 - kl_loss: 4.8235 - lr: 2.5000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3218 - reconstruction_loss: 1.3422 - kl_loss: 4.8074 - lr: 2.5000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2880 - reconstruction_loss: 1.3333 - kl_loss: 4.7906 - lr: 2.5000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2914 - reconstruction_loss: 1.3372 - kl_loss: 4.7992 - lr: 2.5000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2950 - reconstruction_loss: 1.3431 - kl_loss: 4.8103 - lr: 2.5000e-04\n",
      "Epoch 33/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.3231 - reconstruction_loss: 1.3530 - kl_loss: 4.8077\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3231 - reconstruction_loss: 1.3517 - kl_loss: 4.8076 - lr: 2.5000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2680 - reconstruction_loss: 1.3128 - kl_loss: 4.7797 - lr: 1.2500e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2726 - reconstruction_loss: 1.3052 - kl_loss: 4.7972 - lr: 1.2500e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2627 - reconstruction_loss: 1.3051 - kl_loss: 4.7896 - lr: 1.2500e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2530 - reconstruction_loss: 1.2962 - kl_loss: 4.7804 - lr: 1.2500e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2571 - reconstruction_loss: 1.3025 - kl_loss: 4.7956 - lr: 1.2500e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2509 - reconstruction_loss: 1.2974 - kl_loss: 4.7816 - lr: 1.2500e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2511 - reconstruction_loss: 1.3025 - kl_loss: 4.7644\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2511 - reconstruction_loss: 1.3025 - kl_loss: 4.7644 - lr: 1.2500e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2435 - reconstruction_loss: 1.2831 - kl_loss: 4.7726 - lr: 6.2500e-05\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2355 - reconstruction_loss: 1.2796 - kl_loss: 4.7584 - lr: 6.2500e-05\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2243 - reconstruction_loss: 1.2752 - kl_loss: 4.7571 - lr: 6.2500e-05\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2229 - reconstruction_loss: 1.2726 - kl_loss: 4.7541 - lr: 6.2500e-05\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2244 - reconstruction_loss: 1.2732 - kl_loss: 4.7647 - lr: 6.2500e-05\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2319 - reconstruction_loss: 1.2786 - kl_loss: 4.7575 - lr: 6.2500e-05\n",
      "Epoch 47/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2408 - reconstruction_loss: 1.2848 - kl_loss: 4.7702\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2408 - reconstruction_loss: 1.2834 - kl_loss: 4.7704 - lr: 6.2500e-05\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2294 - reconstruction_loss: 1.2732 - kl_loss: 4.7657 - lr: 3.1250e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2203 - reconstruction_loss: 1.2714 - kl_loss: 4.7595 - lr: 3.1250e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2238 - reconstruction_loss: 1.2702 - kl_loss: 4.7480 - lr: 3.1250e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2211 - reconstruction_loss: 1.2675 - kl_loss: 4.7515 - lr: 3.1250e-05\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2217 - reconstruction_loss: 1.2704 - kl_loss: 4.7633 - lr: 3.1250e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2245 - reconstruction_loss: 1.2699 - kl_loss: 4.7628 - lr: 3.1250e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2179 - reconstruction_loss: 1.2670 - kl_loss: 4.7613\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2179 - reconstruction_loss: 1.2670 - kl_loss: 4.7613 - lr: 3.1250e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2129 - reconstruction_loss: 1.2701 - kl_loss: 4.7561 - lr: 1.5625e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2184 - reconstruction_loss: 1.2661 - kl_loss: 4.7586 - lr: 1.5625e-05\n",
      "Epoch 57/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2217 - reconstruction_loss: 1.2683 - kl_loss: 4.7568\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2216 - reconstruction_loss: 1.2671 - kl_loss: 4.7566 - lr: 1.5625e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2131 - reconstruction_loss: 1.2657 - kl_loss: 4.7570 - lr: 7.8125e-06\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2103 - reconstruction_loss: 1.2650 - kl_loss: 4.7463 - lr: 7.8125e-06\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2171 - reconstruction_loss: 1.2649 - kl_loss: 4.7479 - lr: 7.8125e-06\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2213 - reconstruction_loss: 1.2643 - kl_loss: 4.7586 - lr: 7.8125e-06\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2130 - reconstruction_loss: 1.2647 - kl_loss: 4.7505\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2130 - reconstruction_loss: 1.2647 - kl_loss: 4.7505 - lr: 7.8125e-06\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2056 - reconstruction_loss: 1.2639 - kl_loss: 4.7435 - lr: 3.9063e-06\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2151 - reconstruction_loss: 1.2613 - kl_loss: 4.7478 - lr: 3.9063e-06\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2144 - reconstruction_loss: 1.2615 - kl_loss: 4.7537 - lr: 3.9063e-06\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2033 - reconstruction_loss: 1.2600 - kl_loss: 4.7481 - lr: 3.9063e-06\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2079 - reconstruction_loss: 1.2650 - kl_loss: 4.7462 - lr: 3.9063e-06\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2114 - reconstruction_loss: 1.2597 - kl_loss: 4.7474 - lr: 3.9063e-06\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2084 - reconstruction_loss: 1.2619 - kl_loss: 4.7419 - lr: 3.9063e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2140 - reconstruction_loss: 1.2621 - kl_loss: 4.7410 - lr: 3.9063e-06\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2103 - reconstruction_loss: 1.2621 - kl_loss: 4.7501\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2103 - reconstruction_loss: 1.2621 - kl_loss: 4.7501 - lr: 3.9063e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2049 - reconstruction_loss: 1.2598 - kl_loss: 4.7425 - lr: 1.9531e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2102 - reconstruction_loss: 1.2595 - kl_loss: 4.7429 - lr: 1.9531e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2124 - reconstruction_loss: 1.2591 - kl_loss: 4.7510 - lr: 1.9531e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2102 - reconstruction_loss: 1.2584 - kl_loss: 4.7524 - lr: 1.9531e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2166 - reconstruction_loss: 1.2596 - kl_loss: 4.7535\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2166 - reconstruction_loss: 1.2596 - kl_loss: 4.7535 - lr: 1.9531e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2117 - reconstruction_loss: 1.2587 - kl_loss: 4.7528 - lr: 1.0000e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2133 - reconstruction_loss: 1.2604 - kl_loss: 4.7469 - lr: 1.0000e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 4s 12ms/step - loss: 2.2114 - reconstruction_loss: 1.2600 - kl_loss: 4.7519 - lr: 1.0000e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2097 - reconstruction_loss: 1.2583 - kl_loss: 4.7514 - lr: 1.0000e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2091 - reconstruction_loss: 1.2578 - kl_loss: 4.7481 - lr: 1.0000e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1984 - reconstruction_loss: 1.2574 - kl_loss: 4.7513 - lr: 1.0000e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2117 - reconstruction_loss: 1.2582 - kl_loss: 4.7535 - lr: 1.0000e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2060 - reconstruction_loss: 1.2572 - kl_loss: 4.7459 - lr: 1.0000e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2049 - reconstruction_loss: 1.2588 - kl_loss: 4.7445 - lr: 1.0000e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2080 - reconstruction_loss: 1.2599 - kl_loss: 4.7458 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2049 - reconstruction_loss: 1.2572 - kl_loss: 4.7457 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2035 - reconstruction_loss: 1.2592 - kl_loss: 4.7465 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2052 - reconstruction_loss: 1.2583 - kl_loss: 4.7478 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2061 - reconstruction_loss: 1.2573 - kl_loss: 4.7493 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2052 - reconstruction_loss: 1.2584 - kl_loss: 4.7487 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2017 - reconstruction_loss: 1.2579 - kl_loss: 4.7467 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2099 - reconstruction_loss: 1.2578 - kl_loss: 4.7437 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2032 - reconstruction_loss: 1.2582 - kl_loss: 4.7460 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2087 - reconstruction_loss: 1.2593 - kl_loss: 4.7479 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2137 - reconstruction_loss: 1.2581 - kl_loss: 4.7493 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2019 - reconstruction_loss: 1.2570 - kl_loss: 4.7472 - lr: 1.0000e-06\n",
      "Epoch 97: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 11:00:13,556]\u001b[0m Trial 31 finished with value: 0.004917092947217735 and parameters: {'encoder_layers': 5, 'encoder_units_l1': 380, 'encoder_units_l2': 103, 'encoder_units_l3': 1013, 'encoder_units_l4': 996, 'encoder_units_l5': 69, 'decoder_layers': 8, 'decoder_units_l1': 175, 'decoder_units_l2': 497, 'decoder_units_l3': 74, 'decoder_units_l4': 466, 'decoder_units_l5': 64, 'decoder_units_l6': 317, 'decoder_units_l7': 45, 'decoder_units_l8': 71, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.9442 - reconstruction_loss: 4.0321 - kl_loss: 3.3985 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 3.2365 - reconstruction_loss: 2.3203 - kl_loss: 4.2378 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.9686 - reconstruction_loss: 2.0337 - kl_loss: 4.3629 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.8725 - reconstruction_loss: 1.9632 - kl_loss: 4.4709 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.8036 - reconstruction_loss: 1.8608 - kl_loss: 4.6458 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.7986 - reconstruction_loss: 1.8429 - kl_loss: 4.8036 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.6442 - reconstruction_loss: 1.6924 - kl_loss: 4.8219 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.6614 - reconstruction_loss: 1.6574 - kl_loss: 4.8784 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.7457 - reconstruction_loss: 1.8988 - kl_loss: 4.8970 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.9478 - reconstruction_loss: 2.0997 - kl_loss: 4.8073 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.7756 - reconstruction_loss: 1.8018 - kl_loss: 4.7157\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.7753 - reconstruction_loss: 1.8024 - kl_loss: 4.7159 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.7931 - reconstruction_loss: 1.6856 - kl_loss: 4.9077 - lr: 5.0000e-04\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.5003 - reconstruction_loss: 1.5135 - kl_loss: 5.0358 - lr: 5.0000e-04\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4412 - reconstruction_loss: 1.4839 - kl_loss: 5.0962 - lr: 5.0000e-04\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4804 - reconstruction_loss: 1.4498 - kl_loss: 4.9942 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.5997 - reconstruction_loss: 1.5701 - kl_loss: 5.0391 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4075 - reconstruction_loss: 1.4306 - kl_loss: 4.8705 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3840 - reconstruction_loss: 1.4891 - kl_loss: 4.9608 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4305 - reconstruction_loss: 1.4357 - kl_loss: 4.9576 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.4101 - reconstruction_loss: 1.4658 - kl_loss: 4.9919\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4106 - reconstruction_loss: 1.4636 - kl_loss: 4.9920 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3282 - reconstruction_loss: 1.3276 - kl_loss: 4.9248 - lr: 2.5000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2857 - reconstruction_loss: 1.3066 - kl_loss: 4.8882 - lr: 2.5000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2747 - reconstruction_loss: 1.3029 - kl_loss: 4.8605 - lr: 2.5000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2545 - reconstruction_loss: 1.2904 - kl_loss: 4.8328 - lr: 2.5000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2461 - reconstruction_loss: 1.2893 - kl_loss: 4.8057 - lr: 2.5000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2623 - reconstruction_loss: 1.2973 - kl_loss: 4.8238 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2612 - reconstruction_loss: 1.3029 - kl_loss: 4.8407 - lr: 2.5000e-04\n",
      "Epoch 28/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2499 - reconstruction_loss: 1.2925 - kl_loss: 4.7967\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2499 - reconstruction_loss: 1.2916 - kl_loss: 4.7962 - lr: 2.5000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2612 - reconstruction_loss: 1.2985 - kl_loss: 4.8051 - lr: 1.2500e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2379 - reconstruction_loss: 1.2770 - kl_loss: 4.7853 - lr: 1.2500e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2448 - reconstruction_loss: 1.2757 - kl_loss: 4.7780 - lr: 1.2500e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2247 - reconstruction_loss: 1.2749 - kl_loss: 4.7776 - lr: 1.2500e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2372 - reconstruction_loss: 1.2799 - kl_loss: 4.7838 - lr: 1.2500e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2327 - reconstruction_loss: 1.2744 - kl_loss: 4.7719 - lr: 1.2500e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2220 - reconstruction_loss: 1.2781 - kl_loss: 4.7737 - lr: 1.2500e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2336 - reconstruction_loss: 1.2718 - kl_loss: 4.7850 - lr: 1.2500e-04\n",
      "Epoch 37/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2202 - reconstruction_loss: 1.2766 - kl_loss: 4.7701\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2202 - reconstruction_loss: 1.2750 - kl_loss: 4.7700 - lr: 1.2500e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2167 - reconstruction_loss: 1.2648 - kl_loss: 4.7591 - lr: 6.2500e-05\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2227 - reconstruction_loss: 1.2649 - kl_loss: 4.7648 - lr: 6.2500e-05\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2108 - reconstruction_loss: 1.2613 - kl_loss: 4.7681 - lr: 6.2500e-05\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2161 - reconstruction_loss: 1.2602 - kl_loss: 4.7570 - lr: 6.2500e-05\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2175 - reconstruction_loss: 1.2694 - kl_loss: 4.7710 - lr: 6.2500e-05\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2127 - reconstruction_loss: 1.2640 - kl_loss: 4.7620 - lr: 6.2500e-05\n",
      "Epoch 44/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2165 - reconstruction_loss: 1.2623 - kl_loss: 4.7569\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2165 - reconstruction_loss: 1.2611 - kl_loss: 4.7570 - lr: 6.2500e-05\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2118 - reconstruction_loss: 1.2570 - kl_loss: 4.7643 - lr: 3.1250e-05\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2128 - reconstruction_loss: 1.2597 - kl_loss: 4.7595 - lr: 3.1250e-05\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2125 - reconstruction_loss: 1.2577 - kl_loss: 4.7603 - lr: 3.1250e-05\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2164 - reconstruction_loss: 1.2611 - kl_loss: 4.7543\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2164 - reconstruction_loss: 1.2611 - kl_loss: 4.7543 - lr: 3.1250e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2169 - reconstruction_loss: 1.2578 - kl_loss: 4.7571 - lr: 1.5625e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2053 - reconstruction_loss: 1.2531 - kl_loss: 4.7446 - lr: 1.5625e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2077 - reconstruction_loss: 1.2520 - kl_loss: 4.7530 - lr: 1.5625e-05\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1978 - reconstruction_loss: 1.2498 - kl_loss: 4.7472 - lr: 1.5625e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2050 - reconstruction_loss: 1.2514 - kl_loss: 4.7541 - lr: 1.5625e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1971 - reconstruction_loss: 1.2509 - kl_loss: 4.7417 - lr: 1.5625e-05\n",
      "Epoch 55/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2018 - reconstruction_loss: 1.2544 - kl_loss: 4.7445\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2018 - reconstruction_loss: 1.2533 - kl_loss: 4.7444 - lr: 1.5625e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2038 - reconstruction_loss: 1.2498 - kl_loss: 4.7555 - lr: 7.8125e-06\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1914 - reconstruction_loss: 1.2487 - kl_loss: 4.7414 - lr: 7.8125e-06\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1999 - reconstruction_loss: 1.2478 - kl_loss: 4.7473 - lr: 7.8125e-06\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1978 - reconstruction_loss: 1.2486 - kl_loss: 4.7471 - lr: 7.8125e-06\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1998 - reconstruction_loss: 1.2478 - kl_loss: 4.7424 - lr: 7.8125e-06\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1996 - reconstruction_loss: 1.2480 - kl_loss: 4.7509 - lr: 7.8125e-06\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1939 - reconstruction_loss: 1.2476 - kl_loss: 4.7480 - lr: 7.8125e-06\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1927 - reconstruction_loss: 1.2472 - kl_loss: 4.7443 - lr: 7.8125e-06\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1983 - reconstruction_loss: 1.2504 - kl_loss: 4.7485 - lr: 7.8125e-06\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1939 - reconstruction_loss: 1.2478 - kl_loss: 4.7412 - lr: 7.8125e-06\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1959 - reconstruction_loss: 1.2493 - kl_loss: 4.7339\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1959 - reconstruction_loss: 1.2493 - kl_loss: 4.7339 - lr: 7.8125e-06\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2018 - reconstruction_loss: 1.2500 - kl_loss: 4.7337 - lr: 3.9063e-06\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1937 - reconstruction_loss: 1.2472 - kl_loss: 4.7423 - lr: 3.9063e-06\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1961 - reconstruction_loss: 1.2464 - kl_loss: 4.7521 - lr: 3.9063e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1901 - reconstruction_loss: 1.2437 - kl_loss: 4.7534 - lr: 3.9063e-06\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1957 - reconstruction_loss: 1.2446 - kl_loss: 4.7499 - lr: 3.9063e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1983 - reconstruction_loss: 1.2451 - kl_loss: 4.7437 - lr: 3.9063e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1931 - reconstruction_loss: 1.2449 - kl_loss: 4.7478 - lr: 3.9063e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2021 - reconstruction_loss: 1.2465 - kl_loss: 4.7454 - lr: 3.9063e-06\n",
      "Epoch 75/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.1945 - reconstruction_loss: 1.2458 - kl_loss: 4.7464\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1945 - reconstruction_loss: 1.2447 - kl_loss: 4.7462 - lr: 3.9063e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2006 - reconstruction_loss: 1.2452 - kl_loss: 4.7433 - lr: 1.9531e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1922 - reconstruction_loss: 1.2459 - kl_loss: 4.7399 - lr: 1.9531e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1950 - reconstruction_loss: 1.2453 - kl_loss: 4.7431\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1950 - reconstruction_loss: 1.2453 - kl_loss: 4.7431 - lr: 1.9531e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1907 - reconstruction_loss: 1.2455 - kl_loss: 4.7397 - lr: 1.0000e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1975 - reconstruction_loss: 1.2450 - kl_loss: 4.7393 - lr: 1.0000e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1935 - reconstruction_loss: 1.2454 - kl_loss: 4.7420 - lr: 1.0000e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1936 - reconstruction_loss: 1.2448 - kl_loss: 4.7401 - lr: 1.0000e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1943 - reconstruction_loss: 1.2453 - kl_loss: 4.7374 - lr: 1.0000e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1922 - reconstruction_loss: 1.2451 - kl_loss: 4.7427 - lr: 1.0000e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1978 - reconstruction_loss: 1.2441 - kl_loss: 4.7457 - lr: 1.0000e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1978 - reconstruction_loss: 1.2449 - kl_loss: 4.7430 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2002 - reconstruction_loss: 1.2453 - kl_loss: 4.7433 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1959 - reconstruction_loss: 1.2460 - kl_loss: 4.7417 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1930 - reconstruction_loss: 1.2443 - kl_loss: 4.7397 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2009 - reconstruction_loss: 1.2473 - kl_loss: 4.7396 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1961 - reconstruction_loss: 1.2452 - kl_loss: 4.7414 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1946 - reconstruction_loss: 1.2456 - kl_loss: 4.7411 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1974 - reconstruction_loss: 1.2457 - kl_loss: 4.7436 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1932 - reconstruction_loss: 1.2440 - kl_loss: 4.7426 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2000 - reconstruction_loss: 1.2447 - kl_loss: 4.7434 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1908 - reconstruction_loss: 1.2452 - kl_loss: 4.7392 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1970 - reconstruction_loss: 1.2441 - kl_loss: 4.7381 - lr: 1.0000e-06\n",
      "Epoch 98/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1972 - reconstruction_loss: 1.2455 - kl_loss: 4.7372 - lr: 1.0000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1962 - reconstruction_loss: 1.2455 - kl_loss: 4.7411 - lr: 1.0000e-06\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1972 - reconstruction_loss: 1.2465 - kl_loss: 4.7401 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1988 - reconstruction_loss: 1.2463 - kl_loss: 4.7429 - lr: 1.0000e-06\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1958 - reconstruction_loss: 1.2448 - kl_loss: 4.7456 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2000 - reconstruction_loss: 1.2439 - kl_loss: 4.7419 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1984 - reconstruction_loss: 1.2445 - kl_loss: 4.7414 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1939 - reconstruction_loss: 1.2441 - kl_loss: 4.7425 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1912 - reconstruction_loss: 1.2444 - kl_loss: 4.7405 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1948 - reconstruction_loss: 1.2459 - kl_loss: 4.7381 - lr: 1.0000e-06\n",
      "Epoch 107: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 11:08:32,985]\u001b[0m Trial 32 finished with value: 0.004868062423182765 and parameters: {'encoder_layers': 6, 'encoder_units_l1': 553, 'encoder_units_l2': 85, 'encoder_units_l3': 786, 'encoder_units_l4': 753, 'encoder_units_l5': 38, 'encoder_units_l6': 405, 'decoder_layers': 8, 'decoder_units_l1': 94, 'decoder_units_l2': 749, 'decoder_units_l3': 61, 'decoder_units_l4': 700, 'decoder_units_l5': 38, 'decoder_units_l6': 343, 'decoder_units_l7': 47, 'decoder_units_l8': 16, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 5s 10ms/step - loss: 6.0094 - reconstruction_loss: 4.2014 - kl_loss: 3.2390 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 3.7971 - reconstruction_loss: 2.8043 - kl_loss: 4.0292 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 3.4456 - reconstruction_loss: 2.5639 - kl_loss: 4.4006 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 3.0794 - reconstruction_loss: 2.1525 - kl_loss: 4.3450 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.8958 - reconstruction_loss: 1.9659 - kl_loss: 4.4363 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.7891 - reconstruction_loss: 1.8055 - kl_loss: 4.6934 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.8676 - reconstruction_loss: 1.8341 - kl_loss: 4.8340 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6581 - reconstruction_loss: 1.7038 - kl_loss: 4.9559 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6898 - reconstruction_loss: 1.6288 - kl_loss: 4.9737 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5210 - reconstruction_loss: 1.5466 - kl_loss: 4.9696 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4998 - reconstruction_loss: 1.4935 - kl_loss: 4.9752 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5034 - reconstruction_loss: 1.6043 - kl_loss: 5.0313 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.9383 - reconstruction_loss: 1.8477 - kl_loss: 4.8900 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.6634 - reconstruction_loss: 1.7145 - kl_loss: 4.9255\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6637 - reconstruction_loss: 1.7117 - kl_loss: 4.9244 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4994 - reconstruction_loss: 1.5311 - kl_loss: 4.8822 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5472 - reconstruction_loss: 1.5033 - kl_loss: 4.9185 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4184 - reconstruction_loss: 1.4344 - kl_loss: 4.9297 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4158 - reconstruction_loss: 1.4299 - kl_loss: 4.9147 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3900 - reconstruction_loss: 1.4398 - kl_loss: 4.9305 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4385 - reconstruction_loss: 1.4104 - kl_loss: 4.9636 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3914 - reconstruction_loss: 1.4485 - kl_loss: 4.9410 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4924 - reconstruction_loss: 1.4796 - kl_loss: 4.9989 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.4565 - reconstruction_loss: 1.4349 - kl_loss: 4.9820\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4563 - reconstruction_loss: 1.4336 - kl_loss: 4.9818 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3612 - reconstruction_loss: 1.3692 - kl_loss: 4.9256 - lr: 2.5000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4080 - reconstruction_loss: 1.4222 - kl_loss: 5.0187 - lr: 2.5000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4205 - reconstruction_loss: 1.3906 - kl_loss: 5.0153 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3543 - reconstruction_loss: 1.3670 - kl_loss: 4.9516\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3543 - reconstruction_loss: 1.3670 - kl_loss: 4.9516 - lr: 2.5000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3148 - reconstruction_loss: 1.3259 - kl_loss: 4.9144 - lr: 1.2500e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3113 - reconstruction_loss: 1.3264 - kl_loss: 4.8946 - lr: 1.2500e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2908 - reconstruction_loss: 1.3215 - kl_loss: 4.8797 - lr: 1.2500e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3101 - reconstruction_loss: 1.3216 - kl_loss: 4.8794 - lr: 1.2500e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2987 - reconstruction_loss: 1.3200 - kl_loss: 4.9121 - lr: 1.2500e-04\n",
      "Epoch 33/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.3095 - reconstruction_loss: 1.3219 - kl_loss: 4.9360\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3095 - reconstruction_loss: 1.3205 - kl_loss: 4.9359 - lr: 1.2500e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2768 - reconstruction_loss: 1.2974 - kl_loss: 4.8797 - lr: 6.2500e-05\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2751 - reconstruction_loss: 1.3017 - kl_loss: 4.8559 - lr: 6.2500e-05\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2736 - reconstruction_loss: 1.2988 - kl_loss: 4.8631 - lr: 6.2500e-05\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2696 - reconstruction_loss: 1.2928 - kl_loss: 4.8647 - lr: 6.2500e-05\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2723 - reconstruction_loss: 1.2965 - kl_loss: 4.8528 - lr: 6.2500e-05\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2509 - reconstruction_loss: 1.2963 - kl_loss: 4.8678 - lr: 6.2500e-05\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2611 - reconstruction_loss: 1.2908 - kl_loss: 4.8485 - lr: 6.2500e-05\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2598 - reconstruction_loss: 1.2974 - kl_loss: 4.8311 - lr: 6.2500e-05\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2694 - reconstruction_loss: 1.3003 - kl_loss: 4.8468 - lr: 6.2500e-05\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2774 - reconstruction_loss: 1.3026 - kl_loss: 4.8461\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2774 - reconstruction_loss: 1.3026 - kl_loss: 4.8461 - lr: 6.2500e-05\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2958 - reconstruction_loss: 1.3190 - kl_loss: 4.8438 - lr: 3.1250e-05\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2798 - reconstruction_loss: 1.3022 - kl_loss: 4.8466 - lr: 3.1250e-05\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2661 - reconstruction_loss: 1.2911 - kl_loss: 4.8358 - lr: 3.1250e-05\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2581 - reconstruction_loss: 1.2935 - kl_loss: 4.8359 - lr: 3.1250e-05\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2610 - reconstruction_loss: 1.2937 - kl_loss: 4.8471 - lr: 3.1250e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2678 - reconstruction_loss: 1.2897 - kl_loss: 4.8431\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2678 - reconstruction_loss: 1.2897 - kl_loss: 4.8431 - lr: 3.1250e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2592 - reconstruction_loss: 1.2860 - kl_loss: 4.8499 - lr: 1.5625e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2529 - reconstruction_loss: 1.2819 - kl_loss: 4.8377 - lr: 1.5625e-05\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2542 - reconstruction_loss: 1.2782 - kl_loss: 4.8430 - lr: 1.5625e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2588 - reconstruction_loss: 1.2803 - kl_loss: 4.8397 - lr: 1.5625e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2533 - reconstruction_loss: 1.2793 - kl_loss: 4.8521 - lr: 1.5625e-05\n",
      "Epoch 55/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2397 - reconstruction_loss: 1.2789 - kl_loss: 4.8537\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2398 - reconstruction_loss: 1.2779 - kl_loss: 4.8539 - lr: 1.5625e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2469 - reconstruction_loss: 1.2789 - kl_loss: 4.8485 - lr: 7.8125e-06\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2478 - reconstruction_loss: 1.2746 - kl_loss: 4.8531 - lr: 7.8125e-06\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2446 - reconstruction_loss: 1.2752 - kl_loss: 4.8452 - lr: 7.8125e-06\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2386 - reconstruction_loss: 1.2767 - kl_loss: 4.8346 - lr: 7.8125e-06\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2521 - reconstruction_loss: 1.2798 - kl_loss: 4.8401 - lr: 7.8125e-06\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2555 - reconstruction_loss: 1.2810 - kl_loss: 4.8412 - lr: 7.8125e-06\n",
      "Epoch 62/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2495 - reconstruction_loss: 1.2772 - kl_loss: 4.8372\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2494 - reconstruction_loss: 1.2760 - kl_loss: 4.8375 - lr: 7.8125e-06\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2466 - reconstruction_loss: 1.2765 - kl_loss: 4.8494 - lr: 3.9063e-06\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2490 - reconstruction_loss: 1.2781 - kl_loss: 4.8416 - lr: 3.9063e-06\n",
      "Epoch 65/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2434 - reconstruction_loss: 1.2802 - kl_loss: 4.8353\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2434 - reconstruction_loss: 1.2789 - kl_loss: 4.8352 - lr: 3.9063e-06\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2443 - reconstruction_loss: 1.2780 - kl_loss: 4.8273 - lr: 1.9531e-06\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2433 - reconstruction_loss: 1.2770 - kl_loss: 4.8268 - lr: 1.9531e-06\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2442 - reconstruction_loss: 1.2757 - kl_loss: 4.8338 - lr: 1.9531e-06\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2529 - reconstruction_loss: 1.2777 - kl_loss: 4.8385 - lr: 1.9531e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2459 - reconstruction_loss: 1.2736 - kl_loss: 4.8369 - lr: 1.9531e-06\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2411 - reconstruction_loss: 1.2737 - kl_loss: 4.8359 - lr: 1.9531e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2400 - reconstruction_loss: 1.2751 - kl_loss: 4.8315 - lr: 1.9531e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2422 - reconstruction_loss: 1.2760 - kl_loss: 4.8322 - lr: 1.9531e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2438 - reconstruction_loss: 1.2769 - kl_loss: 4.8364\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2438 - reconstruction_loss: 1.2769 - kl_loss: 4.8364 - lr: 1.9531e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2498 - reconstruction_loss: 1.2763 - kl_loss: 4.8387 - lr: 1.0000e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2333 - reconstruction_loss: 1.2726 - kl_loss: 4.8376 - lr: 1.0000e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2404 - reconstruction_loss: 1.2725 - kl_loss: 4.8337 - lr: 1.0000e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2443 - reconstruction_loss: 1.2738 - kl_loss: 4.8347 - lr: 1.0000e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2490 - reconstruction_loss: 1.2751 - kl_loss: 4.8353 - lr: 1.0000e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2437 - reconstruction_loss: 1.2740 - kl_loss: 4.8328 - lr: 1.0000e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2471 - reconstruction_loss: 1.2735 - kl_loss: 4.8323 - lr: 1.0000e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2397 - reconstruction_loss: 1.2751 - kl_loss: 4.8304 - lr: 1.0000e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2390 - reconstruction_loss: 1.2744 - kl_loss: 4.8318 - lr: 1.0000e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2443 - reconstruction_loss: 1.2744 - kl_loss: 4.8350 - lr: 1.0000e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2441 - reconstruction_loss: 1.2717 - kl_loss: 4.8364 - lr: 1.0000e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2430 - reconstruction_loss: 1.2730 - kl_loss: 4.8388 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2434 - reconstruction_loss: 1.2720 - kl_loss: 4.8381 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2413 - reconstruction_loss: 1.2721 - kl_loss: 4.8363 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2389 - reconstruction_loss: 1.2726 - kl_loss: 4.8358 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2446 - reconstruction_loss: 1.2723 - kl_loss: 4.8355 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2374 - reconstruction_loss: 1.2716 - kl_loss: 4.8322 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2425 - reconstruction_loss: 1.2739 - kl_loss: 4.8325 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2394 - reconstruction_loss: 1.2726 - kl_loss: 4.8327 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2386 - reconstruction_loss: 1.2715 - kl_loss: 4.8319 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2391 - reconstruction_loss: 1.2737 - kl_loss: 4.8345 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2421 - reconstruction_loss: 1.2729 - kl_loss: 4.8343 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2468 - reconstruction_loss: 1.2722 - kl_loss: 4.8358 - lr: 1.0000e-06\n",
      "Epoch 98/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2453 - reconstruction_loss: 1.2722 - kl_loss: 4.8347 - lr: 1.0000e-06\n",
      "Epoch 99/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2365 - reconstruction_loss: 1.2729 - kl_loss: 4.8339 - lr: 1.0000e-06\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2420 - reconstruction_loss: 1.2716 - kl_loss: 4.8369 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2397 - reconstruction_loss: 1.2722 - kl_loss: 4.8382 - lr: 1.0000e-06\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2414 - reconstruction_loss: 1.2741 - kl_loss: 4.8414 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2375 - reconstruction_loss: 1.2711 - kl_loss: 4.8406 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2355 - reconstruction_loss: 1.2695 - kl_loss: 4.8365 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2401 - reconstruction_loss: 1.2710 - kl_loss: 4.8365 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2343 - reconstruction_loss: 1.2706 - kl_loss: 4.8362 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2319 - reconstruction_loss: 1.2727 - kl_loss: 4.8385 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2469 - reconstruction_loss: 1.2723 - kl_loss: 4.8414 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2371 - reconstruction_loss: 1.2698 - kl_loss: 4.8358 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2416 - reconstruction_loss: 1.2713 - kl_loss: 4.8363 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2345 - reconstruction_loss: 1.2702 - kl_loss: 4.8366 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2372 - reconstruction_loss: 1.2679 - kl_loss: 4.8361 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2391 - reconstruction_loss: 1.2711 - kl_loss: 4.8345 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2344 - reconstruction_loss: 1.2703 - kl_loss: 4.8325 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2364 - reconstruction_loss: 1.2706 - kl_loss: 4.8328 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2375 - reconstruction_loss: 1.2703 - kl_loss: 4.8333 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2359 - reconstruction_loss: 1.2714 - kl_loss: 4.8345 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2436 - reconstruction_loss: 1.2713 - kl_loss: 4.8356 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2341 - reconstruction_loss: 1.2701 - kl_loss: 4.8365 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2334 - reconstruction_loss: 1.2716 - kl_loss: 4.8366 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2367 - reconstruction_loss: 1.2708 - kl_loss: 4.8381 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2403 - reconstruction_loss: 1.2699 - kl_loss: 4.8377 - lr: 1.0000e-06\n",
      "Epoch 122: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 11:16:59,473]\u001b[0m Trial 33 finished with value: 0.004970580073847823 and parameters: {'encoder_layers': 5, 'encoder_units_l1': 319, 'encoder_units_l2': 81, 'encoder_units_l3': 575, 'encoder_units_l4': 447, 'encoder_units_l5': 71, 'decoder_layers': 8, 'decoder_units_l1': 135, 'decoder_units_l2': 460, 'decoder_units_l3': 41, 'decoder_units_l4': 430, 'decoder_units_l5': 63, 'decoder_units_l6': 163, 'decoder_units_l7': 23, 'decoder_units_l8': 189, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 6.8730 - reconstruction_loss: 4.7336 - kl_loss: 2.9414 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.9637 - reconstruction_loss: 2.8208 - kl_loss: 3.9657 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.0553 - reconstruction_loss: 2.1226 - kl_loss: 4.3582 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.8382 - reconstruction_loss: 1.8962 - kl_loss: 4.5560 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.7742 - reconstruction_loss: 1.7829 - kl_loss: 4.7313 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.6300 - reconstruction_loss: 1.6378 - kl_loss: 4.8428 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5738 - reconstruction_loss: 1.5960 - kl_loss: 4.9254 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5355 - reconstruction_loss: 1.5257 - kl_loss: 4.9379 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5529 - reconstruction_loss: 1.5833 - kl_loss: 4.9712 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5075 - reconstruction_loss: 1.5013 - kl_loss: 4.9628 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4929 - reconstruction_loss: 1.5002 - kl_loss: 4.9639 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4869 - reconstruction_loss: 1.5078 - kl_loss: 5.0035 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.5585 - reconstruction_loss: 1.5024 - kl_loss: 5.0179 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5060 - reconstruction_loss: 1.4897 - kl_loss: 5.0097 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5287 - reconstruction_loss: 1.4834 - kl_loss: 5.0175 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4880 - reconstruction_loss: 1.4579 - kl_loss: 4.9816 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4126 - reconstruction_loss: 1.4150 - kl_loss: 4.9482 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3957 - reconstruction_loss: 1.4047 - kl_loss: 4.9483 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4708 - reconstruction_loss: 1.5897 - kl_loss: 5.0418 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5486 - reconstruction_loss: 1.4693 - kl_loss: 4.9975 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3739 - reconstruction_loss: 1.4090 - kl_loss: 4.8821 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.3778 - reconstruction_loss: 1.3834 - kl_loss: 4.8745 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4683 - reconstruction_loss: 1.6032 - kl_loss: 4.9632 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5661 - reconstruction_loss: 1.5078 - kl_loss: 4.9977 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.4933 - reconstruction_loss: 1.4392 - kl_loss: 5.0721\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4931 - reconstruction_loss: 1.4381 - kl_loss: 5.0719 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.3372 - reconstruction_loss: 1.3326 - kl_loss: 4.9830 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3106 - reconstruction_loss: 1.3180 - kl_loss: 4.9366 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2926 - reconstruction_loss: 1.3099 - kl_loss: 4.8824 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2731 - reconstruction_loss: 1.3115 - kl_loss: 4.8688 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3345 - reconstruction_loss: 1.3498 - kl_loss: 4.8623 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2753 - reconstruction_loss: 1.3000 - kl_loss: 4.8436 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2805 - reconstruction_loss: 1.2956 - kl_loss: 4.8706 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2635 - reconstruction_loss: 1.2967 - kl_loss: 4.8382 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2577 - reconstruction_loss: 1.2922 - kl_loss: 4.8263 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2463 - reconstruction_loss: 1.2867 - kl_loss: 4.8131 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2713 - reconstruction_loss: 1.3003 - kl_loss: 4.8228 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2634 - reconstruction_loss: 1.2908 - kl_loss: 4.8137 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2716 - reconstruction_loss: 1.2962 - kl_loss: 4.8309\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2715 - reconstruction_loss: 1.2950 - kl_loss: 4.8302 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2269 - reconstruction_loss: 1.2694 - kl_loss: 4.7865 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2262 - reconstruction_loss: 1.2709 - kl_loss: 4.7847 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2225 - reconstruction_loss: 1.2660 - kl_loss: 4.7967 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2264 - reconstruction_loss: 1.2671 - kl_loss: 4.7882 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2261 - reconstruction_loss: 1.2688 - kl_loss: 4.8161 - lr: 2.5000e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2316 - reconstruction_loss: 1.2721 - kl_loss: 4.7905 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2301 - reconstruction_loss: 1.2714 - kl_loss: 4.7924\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2301 - reconstruction_loss: 1.2714 - kl_loss: 4.7924 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2192 - reconstruction_loss: 1.2557 - kl_loss: 4.7881 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2150 - reconstruction_loss: 1.2604 - kl_loss: 4.7906 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2302 - reconstruction_loss: 1.2612 - kl_loss: 4.7891 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2197 - reconstruction_loss: 1.2557 - kl_loss: 4.7957\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2197 - reconstruction_loss: 1.2549 - kl_loss: 4.7952 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2025 - reconstruction_loss: 1.2483 - kl_loss: 4.7713 - lr: 6.2500e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1997 - reconstruction_loss: 1.2472 - kl_loss: 4.7720 - lr: 6.2500e-05\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2043 - reconstruction_loss: 1.2488 - kl_loss: 4.7799 - lr: 6.2500e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2067 - reconstruction_loss: 1.2494 - kl_loss: 4.7623 - lr: 6.2500e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1975 - reconstruction_loss: 1.2477 - kl_loss: 4.7539 - lr: 6.2500e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2025 - reconstruction_loss: 1.2493 - kl_loss: 4.7557 - lr: 6.2500e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2046 - reconstruction_loss: 1.2477 - kl_loss: 4.7604 - lr: 6.2500e-05\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2003 - reconstruction_loss: 1.2471 - kl_loss: 4.7671\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2003 - reconstruction_loss: 1.2471 - kl_loss: 4.7671 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2012 - reconstruction_loss: 1.2458 - kl_loss: 4.7682 - lr: 3.1250e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1963 - reconstruction_loss: 1.2431 - kl_loss: 4.7601 - lr: 3.1250e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1940 - reconstruction_loss: 1.2419 - kl_loss: 4.7713 - lr: 3.1250e-05\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1991 - reconstruction_loss: 1.2445 - kl_loss: 4.7644 - lr: 3.1250e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2044 - reconstruction_loss: 1.2449 - kl_loss: 4.7579\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2044 - reconstruction_loss: 1.2449 - kl_loss: 4.7579 - lr: 3.1250e-05\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1974 - reconstruction_loss: 1.2429 - kl_loss: 4.7565 - lr: 1.5625e-05\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1970 - reconstruction_loss: 1.2438 - kl_loss: 4.7504 - lr: 1.5625e-05\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1959 - reconstruction_loss: 1.2430 - kl_loss: 4.7700 - lr: 1.5625e-05\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1948 - reconstruction_loss: 1.2431 - kl_loss: 4.7535 - lr: 1.5625e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1981 - reconstruction_loss: 1.2433 - kl_loss: 4.7691 - lr: 1.5625e-05\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1965 - reconstruction_loss: 1.2441 - kl_loss: 4.7539 - lr: 1.5625e-05\n",
      "Epoch 69/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.1959 - reconstruction_loss: 1.2416 - kl_loss: 4.7681\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1959 - reconstruction_loss: 1.2406 - kl_loss: 4.7681 - lr: 1.5625e-05\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1938 - reconstruction_loss: 1.2390 - kl_loss: 4.7627 - lr: 7.8125e-06\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1961 - reconstruction_loss: 1.2384 - kl_loss: 4.7689 - lr: 7.8125e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1956 - reconstruction_loss: 1.2420 - kl_loss: 4.7553 - lr: 7.8125e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1948 - reconstruction_loss: 1.2409 - kl_loss: 4.7523 - lr: 7.8125e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1935 - reconstruction_loss: 1.2386 - kl_loss: 4.7672 - lr: 7.8125e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1901 - reconstruction_loss: 1.2406 - kl_loss: 4.7569 - lr: 7.8125e-06\n",
      "Epoch 76/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.1875 - reconstruction_loss: 1.2416 - kl_loss: 4.7559\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1876 - reconstruction_loss: 1.2403 - kl_loss: 4.7558 - lr: 7.8125e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1944 - reconstruction_loss: 1.2400 - kl_loss: 4.7636 - lr: 3.9063e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1909 - reconstruction_loss: 1.2381 - kl_loss: 4.7599 - lr: 3.9063e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1915 - reconstruction_loss: 1.2394 - kl_loss: 4.7546 - lr: 3.9063e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1929 - reconstruction_loss: 1.2392 - kl_loss: 4.7611 - lr: 3.9063e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1921 - reconstruction_loss: 1.2387 - kl_loss: 4.7537 - lr: 3.9063e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1917 - reconstruction_loss: 1.2391 - kl_loss: 4.7534 - lr: 3.9063e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1922 - reconstruction_loss: 1.2376 - kl_loss: 4.7612 - lr: 3.9063e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1894 - reconstruction_loss: 1.2382 - kl_loss: 4.7527 - lr: 3.9063e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1957 - reconstruction_loss: 1.2401 - kl_loss: 4.7566 - lr: 3.9063e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1973 - reconstruction_loss: 1.2383 - kl_loss: 4.7637 - lr: 3.9063e-06\n",
      "Epoch 87/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.1901 - reconstruction_loss: 1.2388 - kl_loss: 4.7605\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1901 - reconstruction_loss: 1.2373 - kl_loss: 4.7606 - lr: 3.9063e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1928 - reconstruction_loss: 1.2382 - kl_loss: 4.7603 - lr: 1.9531e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1913 - reconstruction_loss: 1.2372 - kl_loss: 4.7602 - lr: 1.9531e-06\n",
      "Epoch 90/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.1920 - reconstruction_loss: 1.2399 - kl_loss: 4.7560\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1920 - reconstruction_loss: 1.2384 - kl_loss: 4.7561 - lr: 1.9531e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1896 - reconstruction_loss: 1.2392 - kl_loss: 4.7543 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1908 - reconstruction_loss: 1.2394 - kl_loss: 4.7553 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1882 - reconstruction_loss: 1.2392 - kl_loss: 4.7561 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1940 - reconstruction_loss: 1.2382 - kl_loss: 4.7559 - lr: 1.0000e-06\n",
      "Epoch 94: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 11:26:50,421]\u001b[0m Trial 34 finished with value: 0.004832818535252309 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 156, 'encoder_units_l2': 149, 'encoder_units_l3': 863, 'encoder_units_l4': 747, 'decoder_layers': 8, 'decoder_units_l1': 61, 'decoder_units_l2': 411, 'decoder_units_l3': 27, 'decoder_units_l4': 178, 'decoder_units_l5': 131, 'decoder_units_l6': 259, 'decoder_units_l7': 38, 'decoder_units_l8': 47, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 5s 11ms/step - loss: 8.1329 - reconstruction_loss: 5.7436 - kl_loss: 2.3267 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 4.3504 - reconstruction_loss: 3.6224 - kl_loss: 3.5463 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 4.1982 - reconstruction_loss: 3.3172 - kl_loss: 3.7402 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 3.6219 - reconstruction_loss: 2.6798 - kl_loss: 4.1660 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 3.2893 - reconstruction_loss: 2.4013 - kl_loss: 4.2972 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 3.1888 - reconstruction_loss: 2.2910 - kl_loss: 4.3432 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 3.1082 - reconstruction_loss: 2.2220 - kl_loss: 4.3762 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 3.0603 - reconstruction_loss: 2.1582 - kl_loss: 4.4216 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 3.0108 - reconstruction_loss: 2.1107 - kl_loss: 4.4554 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.9679 - reconstruction_loss: 2.0612 - kl_loss: 4.4912 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.9403 - reconstruction_loss: 2.0191 - kl_loss: 4.5277 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.9048 - reconstruction_loss: 1.9787 - kl_loss: 4.5693 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.8667 - reconstruction_loss: 1.9470 - kl_loss: 4.6297 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.8458 - reconstruction_loss: 1.9110 - kl_loss: 4.6437 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.8305 - reconstruction_loss: 1.8810 - kl_loss: 4.6813 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.7913 - reconstruction_loss: 1.8482 - kl_loss: 4.7004 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.7723 - reconstruction_loss: 1.8283 - kl_loss: 4.7146 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.7708 - reconstruction_loss: 1.8164 - kl_loss: 4.7367 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.7528 - reconstruction_loss: 1.7989 - kl_loss: 4.7454 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.7342 - reconstruction_loss: 1.7801 - kl_loss: 4.7591 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.7107 - reconstruction_loss: 1.7617 - kl_loss: 4.7580 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.7083 - reconstruction_loss: 1.7410 - kl_loss: 4.7747 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.6959 - reconstruction_loss: 1.7326 - kl_loss: 4.8003 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.6843 - reconstruction_loss: 1.7133 - kl_loss: 4.8049 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.6675 - reconstruction_loss: 1.6948 - kl_loss: 4.8241 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.6483 - reconstruction_loss: 1.6816 - kl_loss: 4.8289 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.6525 - reconstruction_loss: 1.6727 - kl_loss: 4.8366 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.6344 - reconstruction_loss: 1.6638 - kl_loss: 4.8286 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.6118 - reconstruction_loss: 1.6480 - kl_loss: 4.8326 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.6059 - reconstruction_loss: 1.6400 - kl_loss: 4.8502 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5895 - reconstruction_loss: 1.6190 - kl_loss: 4.8645 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5871 - reconstruction_loss: 1.6091 - kl_loss: 4.8651 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5701 - reconstruction_loss: 1.5945 - kl_loss: 4.8531 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5589 - reconstruction_loss: 1.5860 - kl_loss: 4.8622 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5627 - reconstruction_loss: 1.5804 - kl_loss: 4.8738 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5478 - reconstruction_loss: 1.5641 - kl_loss: 4.8704 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5437 - reconstruction_loss: 1.5616 - kl_loss: 4.8690 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5310 - reconstruction_loss: 1.5490 - kl_loss: 4.8743 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5182 - reconstruction_loss: 1.5482 - kl_loss: 4.8663 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4981 - reconstruction_loss: 1.5222 - kl_loss: 4.8753 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4950 - reconstruction_loss: 1.5087 - kl_loss: 4.8734 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4976 - reconstruction_loss: 1.5138 - kl_loss: 4.8744 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5010 - reconstruction_loss: 1.5184 - kl_loss: 4.8761 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4747 - reconstruction_loss: 1.4960 - kl_loss: 4.8804 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4696 - reconstruction_loss: 1.4942 - kl_loss: 4.8807 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4576 - reconstruction_loss: 1.4803 - kl_loss: 4.8796 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4477 - reconstruction_loss: 1.4718 - kl_loss: 4.8813 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4540 - reconstruction_loss: 1.4749 - kl_loss: 4.8638 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4376 - reconstruction_loss: 1.4744 - kl_loss: 4.8668 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4392 - reconstruction_loss: 1.4608 - kl_loss: 4.8755 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4341 - reconstruction_loss: 1.4600 - kl_loss: 4.8698 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4223 - reconstruction_loss: 1.4486 - kl_loss: 4.8589 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4218 - reconstruction_loss: 1.4478 - kl_loss: 4.8873 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4239 - reconstruction_loss: 1.4365 - kl_loss: 4.8838 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4279 - reconstruction_loss: 1.4455 - kl_loss: 4.8852 - lr: 1.0000e-04\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4389 - reconstruction_loss: 1.4609 - kl_loss: 4.9233 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.4184 - reconstruction_loss: 1.4352 - kl_loss: 4.8929\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4183 - reconstruction_loss: 1.4352 - kl_loss: 4.8929 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3802 - reconstruction_loss: 1.4038 - kl_loss: 4.8887 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3805 - reconstruction_loss: 1.4035 - kl_loss: 4.8884 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3789 - reconstruction_loss: 1.3982 - kl_loss: 4.8836 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3793 - reconstruction_loss: 1.3977 - kl_loss: 4.8823 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3859 - reconstruction_loss: 1.3988 - kl_loss: 4.8726 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3704 - reconstruction_loss: 1.3931 - kl_loss: 4.8700 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3703 - reconstruction_loss: 1.3961 - kl_loss: 4.8720 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3524 - reconstruction_loss: 1.3876 - kl_loss: 4.8652 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3626 - reconstruction_loss: 1.3874 - kl_loss: 4.8888 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3624 - reconstruction_loss: 1.3912 - kl_loss: 4.8731 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.3664 - reconstruction_loss: 1.3972 - kl_loss: 4.8709\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3665 - reconstruction_loss: 1.3963 - kl_loss: 4.8716 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3483 - reconstruction_loss: 1.3759 - kl_loss: 4.8769 - lr: 2.5000e-05\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3516 - reconstruction_loss: 1.3766 - kl_loss: 4.8762 - lr: 2.5000e-05\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3482 - reconstruction_loss: 1.3694 - kl_loss: 4.8803 - lr: 2.5000e-05\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3503 - reconstruction_loss: 1.3670 - kl_loss: 4.8861 - lr: 2.5000e-05\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3425 - reconstruction_loss: 1.3669 - kl_loss: 4.8729 - lr: 2.5000e-05\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3435 - reconstruction_loss: 1.3678 - kl_loss: 4.8789 - lr: 2.5000e-05\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3395 - reconstruction_loss: 1.3660 - kl_loss: 4.8771 - lr: 2.5000e-05\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3344 - reconstruction_loss: 1.3629 - kl_loss: 4.8856 - lr: 2.5000e-05\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3363 - reconstruction_loss: 1.3591 - kl_loss: 4.8953 - lr: 2.5000e-05\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3513 - reconstruction_loss: 1.3657 - kl_loss: 4.8879 - lr: 2.5000e-05\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3364 - reconstruction_loss: 1.3657 - kl_loss: 4.8777 - lr: 2.5000e-05\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3402 - reconstruction_loss: 1.3638 - kl_loss: 4.8718\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3402 - reconstruction_loss: 1.3638 - kl_loss: 4.8718 - lr: 2.5000e-05\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3358 - reconstruction_loss: 1.3559 - kl_loss: 4.8808 - lr: 1.2500e-05\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3322 - reconstruction_loss: 1.3514 - kl_loss: 4.8860 - lr: 1.2500e-05\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3294 - reconstruction_loss: 1.3510 - kl_loss: 4.8863 - lr: 1.2500e-05\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3204 - reconstruction_loss: 1.3465 - kl_loss: 4.8862 - lr: 1.2500e-05\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3224 - reconstruction_loss: 1.3466 - kl_loss: 4.8905 - lr: 1.2500e-05\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3130 - reconstruction_loss: 1.3468 - kl_loss: 4.8890 - lr: 1.2500e-05\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3273 - reconstruction_loss: 1.3508 - kl_loss: 4.8889\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3273 - reconstruction_loss: 1.3508 - kl_loss: 4.8889 - lr: 1.2500e-05\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3226 - reconstruction_loss: 1.3414 - kl_loss: 4.8929 - lr: 6.2500e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3181 - reconstruction_loss: 1.3404 - kl_loss: 4.8947 - lr: 6.2500e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3193 - reconstruction_loss: 1.3389 - kl_loss: 4.8945 - lr: 6.2500e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3198 - reconstruction_loss: 1.3392 - kl_loss: 4.8988 - lr: 6.2500e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3203 - reconstruction_loss: 1.3389 - kl_loss: 4.8876 - lr: 6.2500e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3185 - reconstruction_loss: 1.3375 - kl_loss: 4.9030 - lr: 6.2500e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3216 - reconstruction_loss: 1.3385 - kl_loss: 4.8942 - lr: 6.2500e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3170 - reconstruction_loss: 1.3396 - kl_loss: 4.8910\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3170 - reconstruction_loss: 1.3396 - kl_loss: 4.8910 - lr: 6.2500e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3172 - reconstruction_loss: 1.3349 - kl_loss: 4.8875 - lr: 3.1250e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3129 - reconstruction_loss: 1.3354 - kl_loss: 4.8932 - lr: 3.1250e-06\n",
      "Epoch 98/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3132 - reconstruction_loss: 1.3353 - kl_loss: 4.8907 - lr: 3.1250e-06\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3050 - reconstruction_loss: 1.3333 - kl_loss: 4.8847 - lr: 3.1250e-06\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3232 - reconstruction_loss: 1.3352 - kl_loss: 4.8928 - lr: 3.1250e-06\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3146 - reconstruction_loss: 1.3324 - kl_loss: 4.8924 - lr: 3.1250e-06\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3141 - reconstruction_loss: 1.3327 - kl_loss: 4.8921\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3141 - reconstruction_loss: 1.3327 - kl_loss: 4.8921 - lr: 3.1250e-06\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3045 - reconstruction_loss: 1.3299 - kl_loss: 4.8938 - lr: 1.5625e-06\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3092 - reconstruction_loss: 1.3307 - kl_loss: 4.8948 - lr: 1.5625e-06\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3033 - reconstruction_loss: 1.3313 - kl_loss: 4.8964 - lr: 1.5625e-06\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3115 - reconstruction_loss: 1.3313 - kl_loss: 4.8971\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3115 - reconstruction_loss: 1.3313 - kl_loss: 4.8971 - lr: 1.5625e-06\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3023 - reconstruction_loss: 1.3294 - kl_loss: 4.8925 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3229 - reconstruction_loss: 1.3302 - kl_loss: 4.8899 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3101 - reconstruction_loss: 1.3320 - kl_loss: 4.8937 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3075 - reconstruction_loss: 1.3302 - kl_loss: 4.8960 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3145 - reconstruction_loss: 1.3319 - kl_loss: 4.8949 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3090 - reconstruction_loss: 1.3311 - kl_loss: 4.8961 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3115 - reconstruction_loss: 1.3299 - kl_loss: 4.8953 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3093 - reconstruction_loss: 1.3300 - kl_loss: 4.8976 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3171 - reconstruction_loss: 1.3295 - kl_loss: 4.8985 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3104 - reconstruction_loss: 1.3289 - kl_loss: 4.8952 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3061 - reconstruction_loss: 1.3297 - kl_loss: 4.8947 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3091 - reconstruction_loss: 1.3305 - kl_loss: 4.8960 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3083 - reconstruction_loss: 1.3292 - kl_loss: 4.8948 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3078 - reconstruction_loss: 1.3283 - kl_loss: 4.8959 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3103 - reconstruction_loss: 1.3289 - kl_loss: 4.8943 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3100 - reconstruction_loss: 1.3293 - kl_loss: 4.8963 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3036 - reconstruction_loss: 1.3295 - kl_loss: 4.8938 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3011 - reconstruction_loss: 1.3290 - kl_loss: 4.8952 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3181 - reconstruction_loss: 1.3284 - kl_loss: 4.8949 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3053 - reconstruction_loss: 1.3299 - kl_loss: 4.8939 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3060 - reconstruction_loss: 1.3283 - kl_loss: 4.8924 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3029 - reconstruction_loss: 1.3286 - kl_loss: 4.8884 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3160 - reconstruction_loss: 1.3297 - kl_loss: 4.8867 - lr: 1.0000e-06\n",
      "Epoch 130/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3113 - reconstruction_loss: 1.3302 - kl_loss: 4.8940 - lr: 1.0000e-06\n",
      "Epoch 131/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3123 - reconstruction_loss: 1.3299 - kl_loss: 4.8940 - lr: 1.0000e-06\n",
      "Epoch 132/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3032 - reconstruction_loss: 1.3278 - kl_loss: 4.8975 - lr: 1.0000e-06\n",
      "Epoch 133/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3126 - reconstruction_loss: 1.3292 - kl_loss: 4.8994 - lr: 1.0000e-06\n",
      "Epoch 134/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3061 - reconstruction_loss: 1.3278 - kl_loss: 4.8977 - lr: 1.0000e-06\n",
      "Epoch 135/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3067 - reconstruction_loss: 1.3289 - kl_loss: 4.8947 - lr: 1.0000e-06\n",
      "Epoch 136/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3132 - reconstruction_loss: 1.3289 - kl_loss: 4.8943 - lr: 1.0000e-06\n",
      "Epoch 137/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3043 - reconstruction_loss: 1.3289 - kl_loss: 4.8912 - lr: 1.0000e-06\n",
      "Epoch 138/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3035 - reconstruction_loss: 1.3312 - kl_loss: 4.8949 - lr: 1.0000e-06\n",
      "Epoch 138: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 11:36:27,158]\u001b[0m Trial 35 finished with value: 0.005182537113881049 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 144, 'encoder_units_l2': 147, 'encoder_units_l3': 516, 'encoder_units_l4': 201, 'decoder_layers': 7, 'decoder_units_l1': 76, 'decoder_units_l2': 382, 'decoder_units_l3': 28, 'decoder_units_l4': 161, 'decoder_units_l5': 131, 'decoder_units_l6': 233, 'decoder_units_l7': 36, 'beta': 0.2, 'lr': 0.0001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 5s 10ms/step - loss: 7.1641 - reconstruction_loss: 5.2682 - kl_loss: 2.5733 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 4.1211 - reconstruction_loss: 3.0901 - kl_loss: 3.8554 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 3.3115 - reconstruction_loss: 2.3083 - kl_loss: 4.3208 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.8780 - reconstruction_loss: 1.8609 - kl_loss: 4.7217 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6720 - reconstruction_loss: 1.6480 - kl_loss: 4.8396 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5500 - reconstruction_loss: 1.5712 - kl_loss: 4.8752 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5065 - reconstruction_loss: 1.5422 - kl_loss: 4.8832 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4857 - reconstruction_loss: 1.5031 - kl_loss: 4.9344 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4326 - reconstruction_loss: 1.4572 - kl_loss: 4.9333 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4583 - reconstruction_loss: 1.4605 - kl_loss: 4.9447 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4506 - reconstruction_loss: 1.4588 - kl_loss: 4.9484 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.5216 - reconstruction_loss: 1.5211 - kl_loss: 5.0110\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5216 - reconstruction_loss: 1.5195 - kl_loss: 5.0110 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3630 - reconstruction_loss: 1.3653 - kl_loss: 4.9191 - lr: 5.0000e-04\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3300 - reconstruction_loss: 1.3345 - kl_loss: 4.9077 - lr: 5.0000e-04\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3243 - reconstruction_loss: 1.3410 - kl_loss: 4.9004 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3043 - reconstruction_loss: 1.3313 - kl_loss: 4.8866 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3380 - reconstruction_loss: 1.3370 - kl_loss: 4.8817 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3092 - reconstruction_loss: 1.3186 - kl_loss: 4.8753 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2902 - reconstruction_loss: 1.3196 - kl_loss: 4.8606 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2996 - reconstruction_loss: 1.3355 - kl_loss: 4.9165 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3668 - reconstruction_loss: 1.3685 - kl_loss: 4.9653 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.3212 - reconstruction_loss: 1.3341 - kl_loss: 4.8954\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3212 - reconstruction_loss: 1.3327 - kl_loss: 4.8953 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2803 - reconstruction_loss: 1.2997 - kl_loss: 4.8807 - lr: 2.5000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2812 - reconstruction_loss: 1.3044 - kl_loss: 4.8725 - lr: 2.5000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2869 - reconstruction_loss: 1.2978 - kl_loss: 4.8992 - lr: 2.5000e-04\n",
      "Epoch 26/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2837 - reconstruction_loss: 1.3063 - kl_loss: 4.9189\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2838 - reconstruction_loss: 1.3050 - kl_loss: 4.9191 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2743 - reconstruction_loss: 1.2844 - kl_loss: 4.9040 - lr: 1.2500e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2644 - reconstruction_loss: 1.2803 - kl_loss: 4.8863 - lr: 1.2500e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2499 - reconstruction_loss: 1.2776 - kl_loss: 4.8813 - lr: 1.2500e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2581 - reconstruction_loss: 1.2784 - kl_loss: 4.8773 - lr: 1.2500e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2546 - reconstruction_loss: 1.2763 - kl_loss: 4.8810 - lr: 1.2500e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2510 - reconstruction_loss: 1.2771 - kl_loss: 4.8653 - lr: 1.2500e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2480 - reconstruction_loss: 1.2787 - kl_loss: 4.8557 - lr: 1.2500e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2499 - reconstruction_loss: 1.2758 - kl_loss: 4.8765 - lr: 1.2500e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2436 - reconstruction_loss: 1.2755 - kl_loss: 4.8503 - lr: 1.2500e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2515 - reconstruction_loss: 1.2768 - kl_loss: 4.8495 - lr: 1.2500e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2547 - reconstruction_loss: 1.2731 - kl_loss: 4.8496 - lr: 1.2500e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2390 - reconstruction_loss: 1.2687 - kl_loss: 4.8459 - lr: 1.2500e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2331 - reconstruction_loss: 1.2647 - kl_loss: 4.8343 - lr: 1.2500e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2415 - reconstruction_loss: 1.2717 - kl_loss: 4.8467 - lr: 1.2500e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2480 - reconstruction_loss: 1.2723 - kl_loss: 4.8515 - lr: 1.2500e-04\n",
      "Epoch 42/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2380 - reconstruction_loss: 1.2706 - kl_loss: 4.8482\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2380 - reconstruction_loss: 1.2694 - kl_loss: 4.8482 - lr: 1.2500e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2323 - reconstruction_loss: 1.2642 - kl_loss: 4.8349 - lr: 6.2500e-05\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2335 - reconstruction_loss: 1.2654 - kl_loss: 4.8250 - lr: 6.2500e-05\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2227 - reconstruction_loss: 1.2621 - kl_loss: 4.8323 - lr: 6.2500e-05\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2287 - reconstruction_loss: 1.2605 - kl_loss: 4.8368 - lr: 6.2500e-05\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2359 - reconstruction_loss: 1.2616 - kl_loss: 4.8336 - lr: 6.2500e-05\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2236 - reconstruction_loss: 1.2599 - kl_loss: 4.8221 - lr: 6.2500e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2373 - reconstruction_loss: 1.2631 - kl_loss: 4.8367 - lr: 6.2500e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2255 - reconstruction_loss: 1.2606 - kl_loss: 4.8461 - lr: 6.2500e-05\n",
      "Epoch 51/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2374 - reconstruction_loss: 1.2617 - kl_loss: 4.8411\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2373 - reconstruction_loss: 1.2604 - kl_loss: 4.8416 - lr: 6.2500e-05\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2298 - reconstruction_loss: 1.2551 - kl_loss: 4.8405 - lr: 3.1250e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2186 - reconstruction_loss: 1.2588 - kl_loss: 4.8160 - lr: 3.1250e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2229 - reconstruction_loss: 1.2561 - kl_loss: 4.8295 - lr: 3.1250e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2188 - reconstruction_loss: 1.2551 - kl_loss: 4.8155 - lr: 3.1250e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2198 - reconstruction_loss: 1.2563 - kl_loss: 4.8269 - lr: 3.1250e-05\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2241 - reconstruction_loss: 1.2565 - kl_loss: 4.8289 - lr: 3.1250e-05\n",
      "Epoch 58/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2277 - reconstruction_loss: 1.2572 - kl_loss: 4.8311\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2276 - reconstruction_loss: 1.2563 - kl_loss: 4.8314 - lr: 3.1250e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2221 - reconstruction_loss: 1.2510 - kl_loss: 4.8269 - lr: 1.5625e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2175 - reconstruction_loss: 1.2520 - kl_loss: 4.8190 - lr: 1.5625e-05\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2164 - reconstruction_loss: 1.2514 - kl_loss: 4.8252 - lr: 1.5625e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2168 - reconstruction_loss: 1.2495 - kl_loss: 4.8302 - lr: 1.5625e-05\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2080 - reconstruction_loss: 1.2499 - kl_loss: 4.8203 - lr: 1.5625e-05\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2233 - reconstruction_loss: 1.2510 - kl_loss: 4.8310 - lr: 1.5625e-05\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2202 - reconstruction_loss: 1.2507 - kl_loss: 4.8184 - lr: 1.5625e-05\n",
      "Epoch 66/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.2218 - reconstruction_loss: 1.2529 - kl_loss: 4.8244\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2218 - reconstruction_loss: 1.2525 - kl_loss: 4.8247 - lr: 1.5625e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2158 - reconstruction_loss: 1.2516 - kl_loss: 4.8217 - lr: 7.8125e-06\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2173 - reconstruction_loss: 1.2520 - kl_loss: 4.8162 - lr: 7.8125e-06\n",
      "Epoch 69/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.2173 - reconstruction_loss: 1.2541 - kl_loss: 4.8227\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2173 - reconstruction_loss: 1.2527 - kl_loss: 4.8231 - lr: 7.8125e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2171 - reconstruction_loss: 1.2502 - kl_loss: 4.8218 - lr: 3.9063e-06\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2213 - reconstruction_loss: 1.2501 - kl_loss: 4.8264 - lr: 3.9063e-06\n",
      "Epoch 72/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2192 - reconstruction_loss: 1.2533 - kl_loss: 4.8238\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2192 - reconstruction_loss: 1.2520 - kl_loss: 4.8240 - lr: 3.9063e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2162 - reconstruction_loss: 1.2502 - kl_loss: 4.8231 - lr: 1.9531e-06\n",
      "Epoch 73: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 11:41:14,520]\u001b[0m Trial 36 finished with value: 0.004878543683904388 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 63, 'encoder_units_l2': 207, 'encoder_units_l3': 795, 'encoder_units_l4': 730, 'decoder_layers': 8, 'decoder_units_l1': 55, 'decoder_units_l2': 190, 'decoder_units_l3': 366, 'decoder_units_l4': 120, 'decoder_units_l5': 174, 'decoder_units_l6': 156, 'decoder_units_l7': 63, 'decoder_units_l8': 93, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.5021 - reconstruction_loss: 2.0180 - kl_loss: 0.4304 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4258 - reconstruction_loss: 1.9708 - kl_loss: 0.4562 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.4267 - reconstruction_loss: 1.9633 - kl_loss: 0.4609 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.4203 - reconstruction_loss: 1.9588 - kl_loss: 0.4587 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4137 - reconstruction_loss: 1.9601 - kl_loss: 0.4592 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4162 - reconstruction_loss: 1.9584 - kl_loss: 0.4574 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.4190 - reconstruction_loss: 1.9593 - kl_loss: 0.4592 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4099 - reconstruction_loss: 1.9545 - kl_loss: 0.4587 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4123 - reconstruction_loss: 1.9561 - kl_loss: 0.4604 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4132 - reconstruction_loss: 1.9564 - kl_loss: 0.4598 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "1555/1563 [============================>.] - ETA: 0s - loss: 2.4188 - reconstruction_loss: 1.9548 - kl_loss: 0.4594\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4187 - reconstruction_loss: 1.9541 - kl_loss: 0.4595 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.4153 - reconstruction_loss: 1.9510 - kl_loss: 0.4569 - lr: 5.0000e-04\n",
      "Epoch 13/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4074 - reconstruction_loss: 1.9513 - kl_loss: 0.4589 - lr: 5.0000e-04\n",
      "Epoch 14/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4238 - reconstruction_loss: 1.9541 - kl_loss: 0.4579 - lr: 5.0000e-04\n",
      "Epoch 15/300\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 2.4162 - reconstruction_loss: 1.9520 - kl_loss: 0.4619\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4162 - reconstruction_loss: 1.9520 - kl_loss: 0.4619 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4182 - reconstruction_loss: 1.9488 - kl_loss: 0.4647 - lr: 2.5000e-04\n",
      "Epoch 17/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4139 - reconstruction_loss: 1.9505 - kl_loss: 0.4617 - lr: 2.5000e-04\n",
      "Epoch 18/300\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 2.4004 - reconstruction_loss: 1.9495 - kl_loss: 0.4592\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4004 - reconstruction_loss: 1.9495 - kl_loss: 0.4592 - lr: 2.5000e-04\n",
      "Epoch 19/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4081 - reconstruction_loss: 1.9480 - kl_loss: 0.4594 - lr: 1.2500e-04\n",
      "Epoch 20/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4128 - reconstruction_loss: 1.9481 - kl_loss: 0.4628 - lr: 1.2500e-04\n",
      "Epoch 21/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4147 - reconstruction_loss: 1.9479 - kl_loss: 0.4641 - lr: 1.2500e-04\n",
      "Epoch 22/300\n",
      "1551/1563 [============================>.] - ETA: 0s - loss: 2.4070 - reconstruction_loss: 1.9482 - kl_loss: 0.4607\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4071 - reconstruction_loss: 1.9479 - kl_loss: 0.4609 - lr: 1.2500e-04\n",
      "Epoch 23/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4024 - reconstruction_loss: 1.9429 - kl_loss: 0.4626 - lr: 6.2500e-05\n",
      "Epoch 24/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4071 - reconstruction_loss: 1.9463 - kl_loss: 0.4615 - lr: 6.2500e-05\n",
      "Epoch 25/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4130 - reconstruction_loss: 1.9479 - kl_loss: 0.4604 - lr: 6.2500e-05\n",
      "Epoch 26/300\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 2.3959 - reconstruction_loss: 1.9466 - kl_loss: 0.4592\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3959 - reconstruction_loss: 1.9463 - kl_loss: 0.4594 - lr: 6.2500e-05\n",
      "Epoch 27/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4060 - reconstruction_loss: 1.9467 - kl_loss: 0.4636 - lr: 3.1250e-05\n",
      "Epoch 28/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4173 - reconstruction_loss: 1.9475 - kl_loss: 0.4627 - lr: 3.1250e-05\n",
      "Epoch 29/300\n",
      "1552/1563 [============================>.] - ETA: 0s - loss: 2.4072 - reconstruction_loss: 1.9482 - kl_loss: 0.4627\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4072 - reconstruction_loss: 1.9480 - kl_loss: 0.4628 - lr: 3.1250e-05\n",
      "Epoch 30/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4102 - reconstruction_loss: 1.9462 - kl_loss: 0.4623 - lr: 1.5625e-05\n",
      "Epoch 31/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4079 - reconstruction_loss: 1.9448 - kl_loss: 0.4628 - lr: 1.5625e-05\n",
      "Epoch 32/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 2.4056 - reconstruction_loss: 1.9481 - kl_loss: 0.4627\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4056 - reconstruction_loss: 1.9475 - kl_loss: 0.4627 - lr: 1.5625e-05\n",
      "Epoch 33/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4184 - reconstruction_loss: 1.9461 - kl_loss: 0.4649 - lr: 7.8125e-06\n",
      "Epoch 33: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 11:44:43,986]\u001b[0m Trial 37 finished with value: 0.030426585237693458 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 111, 'encoder_units_l2': 349, 'encoder_units_l3': 42, 'encoder_units_l4': 130, 'decoder_layers': 6, 'decoder_units_l1': 49, 'decoder_units_l2': 279, 'decoder_units_l3': 157, 'decoder_units_l4': 270, 'decoder_units_l5': 341, 'decoder_units_l6': 417, 'beta': 1, 'lr': 0.001, 'batch_size': 64}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 8.0736 - reconstruction_loss: 5.5026 - kl_loss: 2.5153 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 4.4069 - reconstruction_loss: 3.6554 - kl_loss: 3.5578 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 4.1262 - reconstruction_loss: 3.2103 - kl_loss: 3.9198 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 3.7249 - reconstruction_loss: 2.8135 - kl_loss: 4.2204 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 3.4717 - reconstruction_loss: 2.5662 - kl_loss: 4.3255 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.3363 - reconstruction_loss: 2.4185 - kl_loss: 4.4200 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.2365 - reconstruction_loss: 2.3235 - kl_loss: 4.4668 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.1892 - reconstruction_loss: 2.2587 - kl_loss: 4.4971 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.1227 - reconstruction_loss: 2.1779 - kl_loss: 4.5848 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.0635 - reconstruction_loss: 2.1218 - kl_loss: 4.6168 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.0009 - reconstruction_loss: 2.0661 - kl_loss: 4.6582 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.9599 - reconstruction_loss: 2.0290 - kl_loss: 4.6797 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.9411 - reconstruction_loss: 1.9921 - kl_loss: 4.7016 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.9144 - reconstruction_loss: 1.9650 - kl_loss: 4.7323 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.8991 - reconstruction_loss: 1.9375 - kl_loss: 4.7608 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.8706 - reconstruction_loss: 1.9086 - kl_loss: 4.7822 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.8605 - reconstruction_loss: 1.8837 - kl_loss: 4.8061 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.8309 - reconstruction_loss: 1.8534 - kl_loss: 4.8240 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.8113 - reconstruction_loss: 1.8357 - kl_loss: 4.8139 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.7856 - reconstruction_loss: 1.8127 - kl_loss: 4.8514 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.7617 - reconstruction_loss: 1.7933 - kl_loss: 4.8526 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.7468 - reconstruction_loss: 1.7686 - kl_loss: 4.8805 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.7372 - reconstruction_loss: 1.7570 - kl_loss: 4.8940 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.7235 - reconstruction_loss: 1.7416 - kl_loss: 4.8965 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.7127 - reconstruction_loss: 1.7297 - kl_loss: 4.8912 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.6996 - reconstruction_loss: 1.7145 - kl_loss: 4.8935 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.6873 - reconstruction_loss: 1.7062 - kl_loss: 4.9041 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6727 - reconstruction_loss: 1.6936 - kl_loss: 4.9090 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.6727 - reconstruction_loss: 1.6895 - kl_loss: 4.9203 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.6734 - reconstruction_loss: 1.6792 - kl_loss: 4.9159 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.6447 - reconstruction_loss: 1.6603 - kl_loss: 4.9264 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.6469 - reconstruction_loss: 1.6511 - kl_loss: 4.9305 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.6284 - reconstruction_loss: 1.6410 - kl_loss: 4.9277 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.6287 - reconstruction_loss: 1.6265 - kl_loss: 4.9436 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.6020 - reconstruction_loss: 1.6157 - kl_loss: 4.9427 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5933 - reconstruction_loss: 1.6045 - kl_loss: 4.9354 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5944 - reconstruction_loss: 1.5972 - kl_loss: 4.9693 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5888 - reconstruction_loss: 1.5869 - kl_loss: 4.9683 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5817 - reconstruction_loss: 1.5838 - kl_loss: 4.9641 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5755 - reconstruction_loss: 1.5852 - kl_loss: 4.9726 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5670 - reconstruction_loss: 1.5715 - kl_loss: 4.9615 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5649 - reconstruction_loss: 1.5699 - kl_loss: 4.9515 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5607 - reconstruction_loss: 1.5596 - kl_loss: 4.9585 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5478 - reconstruction_loss: 1.5540 - kl_loss: 4.9658 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5452 - reconstruction_loss: 1.5558 - kl_loss: 4.9538 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5396 - reconstruction_loss: 1.5493 - kl_loss: 4.9577 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5403 - reconstruction_loss: 1.5388 - kl_loss: 4.9728 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5296 - reconstruction_loss: 1.5337 - kl_loss: 4.9759 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5237 - reconstruction_loss: 1.5270 - kl_loss: 4.9746 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5201 - reconstruction_loss: 1.5214 - kl_loss: 4.9650 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5135 - reconstruction_loss: 1.5146 - kl_loss: 4.9822 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5055 - reconstruction_loss: 1.5026 - kl_loss: 4.9865 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5090 - reconstruction_loss: 1.5083 - kl_loss: 4.9591 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5013 - reconstruction_loss: 1.5033 - kl_loss: 4.9794 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4859 - reconstruction_loss: 1.4906 - kl_loss: 4.9876 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4909 - reconstruction_loss: 1.4949 - kl_loss: 4.9827 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4878 - reconstruction_loss: 1.4938 - kl_loss: 4.9863 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4902 - reconstruction_loss: 1.4896 - kl_loss: 4.9786 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4777 - reconstruction_loss: 1.4771 - kl_loss: 4.9850 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4697 - reconstruction_loss: 1.4761 - kl_loss: 4.9775 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4638 - reconstruction_loss: 1.4712 - kl_loss: 4.9796 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4678 - reconstruction_loss: 1.4676 - kl_loss: 4.9653 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4578 - reconstruction_loss: 1.4624 - kl_loss: 4.9767 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4594 - reconstruction_loss: 1.4589 - kl_loss: 4.9783 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4661 - reconstruction_loss: 1.4570 - kl_loss: 4.9792 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4507 - reconstruction_loss: 1.4627 - kl_loss: 4.9708 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4547 - reconstruction_loss: 1.4610 - kl_loss: 4.9618 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4461 - reconstruction_loss: 1.4587 - kl_loss: 4.9650 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4564 - reconstruction_loss: 1.4577 - kl_loss: 4.9821 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4604 - reconstruction_loss: 1.4591 - kl_loss: 4.9699 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4435 - reconstruction_loss: 1.4473 - kl_loss: 4.9604 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4433 - reconstruction_loss: 1.4441 - kl_loss: 4.9609 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4253 - reconstruction_loss: 1.4402 - kl_loss: 4.9636 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4425 - reconstruction_loss: 1.4437 - kl_loss: 4.9752 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4315 - reconstruction_loss: 1.4363 - kl_loss: 4.9513 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4271 - reconstruction_loss: 1.4327 - kl_loss: 4.9551 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4223 - reconstruction_loss: 1.4286 - kl_loss: 4.9602 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4229 - reconstruction_loss: 1.4272 - kl_loss: 4.9559 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4145 - reconstruction_loss: 1.4296 - kl_loss: 4.9609 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4099 - reconstruction_loss: 1.4223 - kl_loss: 4.9540 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4171 - reconstruction_loss: 1.4250 - kl_loss: 4.9507 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4019 - reconstruction_loss: 1.4183 - kl_loss: 4.9678 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4163 - reconstruction_loss: 1.4169 - kl_loss: 4.9635 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4059 - reconstruction_loss: 1.4163 - kl_loss: 4.9543 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4067 - reconstruction_loss: 1.4099 - kl_loss: 4.9611 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3829 - reconstruction_loss: 1.4077 - kl_loss: 4.9533 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3971 - reconstruction_loss: 1.4057 - kl_loss: 4.9534 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3985 - reconstruction_loss: 1.4066 - kl_loss: 4.9641 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4017 - reconstruction_loss: 1.4158 - kl_loss: 4.9575 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.3978 - reconstruction_loss: 1.4075 - kl_loss: 4.9592\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3978 - reconstruction_loss: 1.4059 - kl_loss: 4.9588 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3792 - reconstruction_loss: 1.3856 - kl_loss: 4.9539 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3798 - reconstruction_loss: 1.3787 - kl_loss: 4.9585 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3740 - reconstruction_loss: 1.3759 - kl_loss: 4.9636 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3707 - reconstruction_loss: 1.3776 - kl_loss: 4.9726 - lr: 5.0000e-05\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3711 - reconstruction_loss: 1.3736 - kl_loss: 4.9670 - lr: 5.0000e-05\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3731 - reconstruction_loss: 1.3755 - kl_loss: 4.9636 - lr: 5.0000e-05\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3667 - reconstruction_loss: 1.3725 - kl_loss: 4.9678 - lr: 5.0000e-05\n",
      "Epoch 98/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3718 - reconstruction_loss: 1.3747 - kl_loss: 4.9684 - lr: 5.0000e-05\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3679 - reconstruction_loss: 1.3738 - kl_loss: 4.9559 - lr: 5.0000e-05\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3686 - reconstruction_loss: 1.3708 - kl_loss: 4.9597 - lr: 5.0000e-05\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3620 - reconstruction_loss: 1.3720 - kl_loss: 4.9557 - lr: 5.0000e-05\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3680 - reconstruction_loss: 1.3737 - kl_loss: 4.9562 - lr: 5.0000e-05\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3565 - reconstruction_loss: 1.3675 - kl_loss: 4.9598 - lr: 5.0000e-05\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3538 - reconstruction_loss: 1.3692 - kl_loss: 4.9582 - lr: 5.0000e-05\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3545 - reconstruction_loss: 1.3660 - kl_loss: 4.9512 - lr: 5.0000e-05\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3648 - reconstruction_loss: 1.3704 - kl_loss: 4.9538 - lr: 5.0000e-05\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3703 - reconstruction_loss: 1.3710 - kl_loss: 4.9539 - lr: 5.0000e-05\n",
      "Epoch 108/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.3678 - reconstruction_loss: 1.3714 - kl_loss: 4.9534\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3678 - reconstruction_loss: 1.3703 - kl_loss: 4.9533 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3409 - reconstruction_loss: 1.3578 - kl_loss: 4.9566 - lr: 2.5000e-05\n",
      "Epoch 110/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3457 - reconstruction_loss: 1.3580 - kl_loss: 4.9605 - lr: 2.5000e-05\n",
      "Epoch 111/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3497 - reconstruction_loss: 1.3590 - kl_loss: 4.9519 - lr: 2.5000e-05\n",
      "Epoch 112/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3476 - reconstruction_loss: 1.3560 - kl_loss: 4.9577 - lr: 2.5000e-05\n",
      "Epoch 113/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3454 - reconstruction_loss: 1.3550 - kl_loss: 4.9532 - lr: 2.5000e-05\n",
      "Epoch 114/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3617 - reconstruction_loss: 1.3526 - kl_loss: 4.9731 - lr: 2.5000e-05\n",
      "Epoch 115/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3444 - reconstruction_loss: 1.3528 - kl_loss: 4.9546 - lr: 2.5000e-05\n",
      "Epoch 116/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3383 - reconstruction_loss: 1.3510 - kl_loss: 4.9600 - lr: 2.5000e-05\n",
      "Epoch 117/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3397 - reconstruction_loss: 1.3490 - kl_loss: 4.9643 - lr: 2.5000e-05\n",
      "Epoch 118/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3419 - reconstruction_loss: 1.3499 - kl_loss: 4.9554 - lr: 2.5000e-05\n",
      "Epoch 119/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3403 - reconstruction_loss: 1.3469 - kl_loss: 4.9666 - lr: 2.5000e-05\n",
      "Epoch 120/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3345 - reconstruction_loss: 1.3477 - kl_loss: 4.9590 - lr: 2.5000e-05\n",
      "Epoch 121/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3488 - reconstruction_loss: 1.3532 - kl_loss: 4.9582 - lr: 2.5000e-05\n",
      "Epoch 122/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3423 - reconstruction_loss: 1.3485 - kl_loss: 4.9581 - lr: 2.5000e-05\n",
      "Epoch 123/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.3397 - reconstruction_loss: 1.3489 - kl_loss: 4.9673\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3397 - reconstruction_loss: 1.3478 - kl_loss: 4.9669 - lr: 2.5000e-05\n",
      "Epoch 124/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3370 - reconstruction_loss: 1.3462 - kl_loss: 4.9492 - lr: 1.2500e-05\n",
      "Epoch 125/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3316 - reconstruction_loss: 1.3455 - kl_loss: 4.9579 - lr: 1.2500e-05\n",
      "Epoch 126/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3372 - reconstruction_loss: 1.3406 - kl_loss: 4.9619 - lr: 1.2500e-05\n",
      "Epoch 127/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3345 - reconstruction_loss: 1.3417 - kl_loss: 4.9557 - lr: 1.2500e-05\n",
      "Epoch 128/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3292 - reconstruction_loss: 1.3391 - kl_loss: 4.9729 - lr: 1.2500e-05\n",
      "Epoch 129/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3332 - reconstruction_loss: 1.3408 - kl_loss: 4.9608 - lr: 1.2500e-05\n",
      "Epoch 130/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3245 - reconstruction_loss: 1.3393 - kl_loss: 4.9612 - lr: 1.2500e-05\n",
      "Epoch 131/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3275 - reconstruction_loss: 1.3396 - kl_loss: 4.9681 - lr: 1.2500e-05\n",
      "Epoch 132/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3271 - reconstruction_loss: 1.3379 - kl_loss: 4.9746 - lr: 1.2500e-05\n",
      "Epoch 133/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3306 - reconstruction_loss: 1.3373 - kl_loss: 4.9657 - lr: 1.2500e-05\n",
      "Epoch 134/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3324 - reconstruction_loss: 1.3354 - kl_loss: 4.9623 - lr: 1.2500e-05\n",
      "Epoch 135/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3343 - reconstruction_loss: 1.3374 - kl_loss: 4.9736 - lr: 1.2500e-05\n",
      "Epoch 136/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3234 - reconstruction_loss: 1.3338 - kl_loss: 4.9642 - lr: 1.2500e-05\n",
      "Epoch 137/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3360 - reconstruction_loss: 1.3355 - kl_loss: 4.9690 - lr: 1.2500e-05\n",
      "Epoch 138/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3296 - reconstruction_loss: 1.3318 - kl_loss: 4.9755 - lr: 1.2500e-05\n",
      "Epoch 139/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3223 - reconstruction_loss: 1.3338 - kl_loss: 4.9612 - lr: 1.2500e-05\n",
      "Epoch 140/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3311 - reconstruction_loss: 1.3331 - kl_loss: 4.9671 - lr: 1.2500e-05\n",
      "Epoch 141/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3228 - reconstruction_loss: 1.3313 - kl_loss: 4.9659 - lr: 1.2500e-05\n",
      "Epoch 142/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3258 - reconstruction_loss: 1.3322 - kl_loss: 4.9707 - lr: 1.2500e-05\n",
      "Epoch 143/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3275 - reconstruction_loss: 1.3318 - kl_loss: 4.9746 - lr: 1.2500e-05\n",
      "Epoch 144/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.3271 - reconstruction_loss: 1.3329 - kl_loss: 4.9738\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3271 - reconstruction_loss: 1.3311 - kl_loss: 4.9738 - lr: 1.2500e-05\n",
      "Epoch 145/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3232 - reconstruction_loss: 1.3281 - kl_loss: 4.9743 - lr: 6.2500e-06\n",
      "Epoch 146/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3109 - reconstruction_loss: 1.3285 - kl_loss: 4.9578 - lr: 6.2500e-06\n",
      "Epoch 147/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3207 - reconstruction_loss: 1.3267 - kl_loss: 4.9734 - lr: 6.2500e-06\n",
      "Epoch 148/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3199 - reconstruction_loss: 1.3266 - kl_loss: 4.9681 - lr: 6.2500e-06\n",
      "Epoch 149/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.3241 - reconstruction_loss: 1.3289 - kl_loss: 4.9695\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3241 - reconstruction_loss: 1.3276 - kl_loss: 4.9693 - lr: 6.2500e-06\n",
      "Epoch 150/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3215 - reconstruction_loss: 1.3231 - kl_loss: 4.9718 - lr: 3.1250e-06\n",
      "Epoch 151/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3335 - reconstruction_loss: 1.3261 - kl_loss: 4.9736 - lr: 3.1250e-06\n",
      "Epoch 152/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3172 - reconstruction_loss: 1.3247 - kl_loss: 4.9779 - lr: 3.1250e-06\n",
      "Epoch 153/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3234 - reconstruction_loss: 1.3236 - kl_loss: 4.9778\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3234 - reconstruction_loss: 1.3236 - kl_loss: 4.9778 - lr: 3.1250e-06\n",
      "Epoch 154/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3197 - reconstruction_loss: 1.3258 - kl_loss: 4.9694 - lr: 1.5625e-06\n",
      "Epoch 155/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3155 - reconstruction_loss: 1.3241 - kl_loss: 4.9708 - lr: 1.5625e-06\n",
      "Epoch 156/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3210 - reconstruction_loss: 1.3226 - kl_loss: 4.9732 - lr: 1.5625e-06\n",
      "Epoch 157/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3227 - reconstruction_loss: 1.3252 - kl_loss: 4.9693 - lr: 1.5625e-06\n",
      "Epoch 158/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3213 - reconstruction_loss: 1.3233 - kl_loss: 4.9734 - lr: 1.5625e-06\n",
      "Epoch 159/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.3160 - reconstruction_loss: 1.3241 - kl_loss: 4.9710\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3160 - reconstruction_loss: 1.3233 - kl_loss: 4.9710 - lr: 1.5625e-06\n",
      "Epoch 160/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3123 - reconstruction_loss: 1.3227 - kl_loss: 4.9667 - lr: 1.0000e-06\n",
      "Epoch 161/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3185 - reconstruction_loss: 1.3261 - kl_loss: 4.9679 - lr: 1.0000e-06\n",
      "Epoch 162/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3274 - reconstruction_loss: 1.3231 - kl_loss: 4.9681 - lr: 1.0000e-06\n",
      "Epoch 163/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3240 - reconstruction_loss: 1.3244 - kl_loss: 4.9694 - lr: 1.0000e-06\n",
      "Epoch 164/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3243 - reconstruction_loss: 1.3217 - kl_loss: 4.9730 - lr: 1.0000e-06\n",
      "Epoch 165/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3195 - reconstruction_loss: 1.3228 - kl_loss: 4.9712 - lr: 1.0000e-06\n",
      "Epoch 166/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3327 - reconstruction_loss: 1.3231 - kl_loss: 4.9735 - lr: 1.0000e-06\n",
      "Epoch 167/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3143 - reconstruction_loss: 1.3220 - kl_loss: 4.9730 - lr: 1.0000e-06\n",
      "Epoch 168/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3175 - reconstruction_loss: 1.3232 - kl_loss: 4.9719 - lr: 1.0000e-06\n",
      "Epoch 169/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3234 - reconstruction_loss: 1.3233 - kl_loss: 4.9709 - lr: 1.0000e-06\n",
      "Epoch 170/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3141 - reconstruction_loss: 1.3209 - kl_loss: 4.9704 - lr: 1.0000e-06\n",
      "Epoch 171/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3166 - reconstruction_loss: 1.3237 - kl_loss: 4.9683 - lr: 1.0000e-06\n",
      "Epoch 172/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3179 - reconstruction_loss: 1.3229 - kl_loss: 4.9715 - lr: 1.0000e-06\n",
      "Epoch 173/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3134 - reconstruction_loss: 1.3220 - kl_loss: 4.9714 - lr: 1.0000e-06\n",
      "Epoch 174/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3173 - reconstruction_loss: 1.3213 - kl_loss: 4.9723 - lr: 1.0000e-06\n",
      "Epoch 175/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3154 - reconstruction_loss: 1.3225 - kl_loss: 4.9711 - lr: 1.0000e-06\n",
      "Epoch 176/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3135 - reconstruction_loss: 1.3240 - kl_loss: 4.9733 - lr: 1.0000e-06\n",
      "Epoch 177/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3229 - reconstruction_loss: 1.3223 - kl_loss: 4.9763 - lr: 1.0000e-06\n",
      "Epoch 178/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3210 - reconstruction_loss: 1.3238 - kl_loss: 4.9771 - lr: 1.0000e-06\n",
      "Epoch 179/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3207 - reconstruction_loss: 1.3220 - kl_loss: 4.9790 - lr: 1.0000e-06\n",
      "Epoch 180/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3216 - reconstruction_loss: 1.3229 - kl_loss: 4.9737 - lr: 1.0000e-06\n",
      "Epoch 180: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 11:55:49,205]\u001b[0m Trial 38 finished with value: 0.005178869894801146 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 231, 'encoder_units_l2': 407, 'encoder_units_l3': 176, 'decoder_layers': 7, 'decoder_units_l1': 341, 'decoder_units_l2': 653, 'decoder_units_l3': 24, 'decoder_units_l4': 85, 'decoder_units_l5': 108, 'decoder_units_l6': 88, 'decoder_units_l7': 108, 'beta': 0.2, 'lr': 0.0001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 7ms/step - loss: 21.9479 - reconstruction_loss: 21.6815 - kl_loss: 0.0056 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6784 - reconstruction_loss: 21.6345 - kl_loss: 3.2001e-06 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6845 - reconstruction_loss: 21.6353 - kl_loss: 1.2349e-06 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6355 - reconstruction_loss: 21.6339 - kl_loss: 1.2572e-06 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.6793 - reconstruction_loss: 21.6357 - kl_loss: 1.6060e-06 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.6417 - reconstruction_loss: 21.6347 - kl_loss: 2.8380e-06 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 21.7330 - reconstruction_loss: 21.7048 - kl_loss: 1.5582e-07\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7315 - reconstruction_loss: 21.6356 - kl_loss: 2.6152e-07 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7018 - reconstruction_loss: 21.6329 - kl_loss: 3.1728e-07 - lr: 0.0050\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6945 - reconstruction_loss: 21.6331 - kl_loss: 5.8771e-08 - lr: 0.0050\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7562 - reconstruction_loss: 21.6320 - kl_loss: 5.2933e-08 - lr: 0.0050\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7006 - reconstruction_loss: 21.6326 - kl_loss: 4.6801e-08 - lr: 0.0050\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6479 - reconstruction_loss: 21.6327 - kl_loss: 4.3406e-08 - lr: 0.0050\n",
      "Epoch 13/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 21.7051 - reconstruction_loss: 21.7028 - kl_loss: 4.3930e-08\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7044 - reconstruction_loss: 21.6333 - kl_loss: 4.7760e-08 - lr: 0.0050\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7921 - reconstruction_loss: 21.6318 - kl_loss: 4.4911e-08 - lr: 0.0025\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6564 - reconstruction_loss: 21.6318 - kl_loss: 3.2036e-08 - lr: 0.0025\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7046 - reconstruction_loss: 21.6311 - kl_loss: 3.0522e-08 - lr: 0.0025\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.7134 - reconstruction_loss: 21.6319 - kl_loss: 2.8988e-08 - lr: 0.0025\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6830 - reconstruction_loss: 21.6315 - kl_loss: 2.7612e-08 - lr: 0.0025\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7138 - reconstruction_loss: 21.6310 - kl_loss: 2.6325e-08 - lr: 0.0025\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7553 - reconstruction_loss: 21.6317 - kl_loss: 1.7248e-07 - lr: 0.0025\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6964 - reconstruction_loss: 21.6324 - kl_loss: 5.0629e-07 - lr: 0.0025\n",
      "Epoch 22/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 21.7072 - reconstruction_loss: 21.7144 - kl_loss: 3.6876e-07\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7064 - reconstruction_loss: 21.6317 - kl_loss: 3.6007e-07 - lr: 0.0025\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6963 - reconstruction_loss: 21.6310 - kl_loss: 2.1060e-08 - lr: 0.0012\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.6745 - reconstruction_loss: 21.6309 - kl_loss: 1.9790e-08 - lr: 0.0012\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6777 - reconstruction_loss: 21.6308 - kl_loss: 1.9024e-08 - lr: 0.0012\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 21.7495 - reconstruction_loss: 21.6314 - kl_loss: 1.8168e-08 - lr: 0.0012\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.7619 - reconstruction_loss: 21.6308 - kl_loss: 1.7407e-08 - lr: 0.0012\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 21.7208 - reconstruction_loss: 21.6311 - kl_loss: 1.6771e-08\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 21.7204 - reconstruction_loss: 21.6311 - kl_loss: 1.6771e-08 - lr: 0.0012\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.6857 - reconstruction_loss: 21.6307 - kl_loss: 1.5723e-08 - lr: 6.2500e-04\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7403 - reconstruction_loss: 21.6307 - kl_loss: 1.5288e-08 - lr: 6.2500e-04\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6809 - reconstruction_loss: 21.6306 - kl_loss: 1.4845e-08 - lr: 6.2500e-04\n",
      "Epoch 32/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 21.7172 - reconstruction_loss: 21.7099 - kl_loss: 1.4481e-08\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7162 - reconstruction_loss: 21.6306 - kl_loss: 1.4491e-08 - lr: 6.2500e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6521 - reconstruction_loss: 21.6305 - kl_loss: 1.3985e-08 - lr: 3.1250e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6626 - reconstruction_loss: 21.6305 - kl_loss: 1.3685e-08 - lr: 3.1250e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6850 - reconstruction_loss: 21.6305 - kl_loss: 1.3484e-08 - lr: 3.1250e-04\n",
      "Epoch 36/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 21.7264 - reconstruction_loss: 21.7026 - kl_loss: 1.3190e-08\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7249 - reconstruction_loss: 21.6304 - kl_loss: 1.3231e-08 - lr: 3.1250e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7153 - reconstruction_loss: 21.6303 - kl_loss: 1.2935e-08 - lr: 1.5625e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6685 - reconstruction_loss: 21.6303 - kl_loss: 1.2801e-08 - lr: 1.5625e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.6919 - reconstruction_loss: 21.6303 - kl_loss: 1.2596e-08 - lr: 1.5625e-04\n",
      "Epoch 40/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 21.6996 - reconstruction_loss: 21.7158 - kl_loss: 1.2449e-08\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6992 - reconstruction_loss: 21.6303 - kl_loss: 1.2436e-08 - lr: 1.5625e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7705 - reconstruction_loss: 21.6302 - kl_loss: 1.2276e-08 - lr: 7.8125e-05\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7051 - reconstruction_loss: 21.6302 - kl_loss: 1.2154e-08 - lr: 7.8125e-05\n",
      "Epoch 43/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 21.6538 - reconstruction_loss: 21.7091 - kl_loss: 1.2049e-08\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6550 - reconstruction_loss: 21.6302 - kl_loss: 1.2071e-08 - lr: 7.8125e-05\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7252 - reconstruction_loss: 21.6302 - kl_loss: 1.1934e-08 - lr: 3.9062e-05\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7341 - reconstruction_loss: 21.6302 - kl_loss: 1.1868e-08 - lr: 3.9062e-05\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6501 - reconstruction_loss: 21.6302 - kl_loss: 1.1834e-08 - lr: 3.9062e-05\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 21.6843 - reconstruction_loss: 21.6302 - kl_loss: 1.1750e-08\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.6841 - reconstruction_loss: 21.6302 - kl_loss: 1.1750e-08 - lr: 3.9062e-05\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.6690 - reconstruction_loss: 21.6301 - kl_loss: 1.1684e-08 - lr: 1.9531e-05\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.6398 - reconstruction_loss: 21.6302 - kl_loss: 1.1609e-08 - lr: 1.9531e-05\n",
      "Epoch 50/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 21.7270 - reconstruction_loss: 21.7074 - kl_loss: 1.1559e-08\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.7256 - reconstruction_loss: 21.6302 - kl_loss: 1.1551e-08 - lr: 1.9531e-05\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6917 - reconstruction_loss: 21.6302 - kl_loss: 1.1548e-08 - lr: 9.7656e-06\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6564 - reconstruction_loss: 21.6301 - kl_loss: 1.1483e-08 - lr: 9.7656e-06\n",
      "Epoch 53/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 21.7073 - reconstruction_loss: 21.6976 - kl_loss: 1.1449e-08\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.7064 - reconstruction_loss: 21.6301 - kl_loss: 1.1466e-08 - lr: 9.7656e-06\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7516 - reconstruction_loss: 21.6302 - kl_loss: 1.1439e-08 - lr: 4.8828e-06\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7151 - reconstruction_loss: 21.6301 - kl_loss: 1.1418e-08 - lr: 4.8828e-06\n",
      "Epoch 56/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 21.6470 - reconstruction_loss: 21.6981 - kl_loss: 1.1395e-08\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6484 - reconstruction_loss: 21.6301 - kl_loss: 1.1376e-08 - lr: 4.8828e-06\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6806 - reconstruction_loss: 21.6301 - kl_loss: 1.1380e-08 - lr: 2.4414e-06\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7110 - reconstruction_loss: 21.6301 - kl_loss: 1.1391e-08 - lr: 2.4414e-06\n",
      "Epoch 59/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 21.7516 - reconstruction_loss: 21.7023 - kl_loss: 1.1373e-08\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7491 - reconstruction_loss: 21.6301 - kl_loss: 1.1380e-08 - lr: 2.4414e-06\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7113 - reconstruction_loss: 21.6301 - kl_loss: 1.1347e-08 - lr: 1.2207e-06\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.7788 - reconstruction_loss: 21.6301 - kl_loss: 1.1312e-08 - lr: 1.2207e-06\n",
      "Epoch 62/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 21.6550 - reconstruction_loss: 21.6965 - kl_loss: 1.1327e-08\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6558 - reconstruction_loss: 21.6301 - kl_loss: 1.1310e-08 - lr: 1.2207e-06\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7597 - reconstruction_loss: 21.6301 - kl_loss: 1.1322e-08 - lr: 1.0000e-06\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6444 - reconstruction_loss: 21.6301 - kl_loss: 1.1297e-08 - lr: 1.0000e-06\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6519 - reconstruction_loss: 21.6301 - kl_loss: 1.1316e-08 - lr: 1.0000e-06\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.6749 - reconstruction_loss: 21.6301 - kl_loss: 1.1319e-08 - lr: 1.0000e-06\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.6565 - reconstruction_loss: 21.6301 - kl_loss: 1.1290e-08 - lr: 1.0000e-06\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 21.6346 - reconstruction_loss: 21.6301 - kl_loss: 1.1273e-08 - lr: 1.0000e-06\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.7054 - reconstruction_loss: 21.6301 - kl_loss: 1.1253e-08 - lr: 1.0000e-06\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 21.6788 - reconstruction_loss: 21.6301 - kl_loss: 1.1264e-08 - lr: 1.0000e-06\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 21.7531 - reconstruction_loss: 21.6301 - kl_loss: 1.1210e-08 - lr: 1.0000e-06\n",
      "Epoch 71: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 11:57:33,842]\u001b[0m Trial 39 finished with value: 0.042395068698840126 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 153, 'encoder_units_l2': 23, 'encoder_units_l3': 422, 'decoder_layers': 6, 'decoder_units_l1': 28, 'decoder_units_l2': 122, 'decoder_units_l3': 96, 'decoder_units_l4': 192, 'decoder_units_l5': 648, 'decoder_units_l6': 677, 'beta': 1, 'lr': 0.01, 'batch_size': 512}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 6.1180 - reconstruction_loss: 4.0842 - kl_loss: 3.4560 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 3.6008 - reconstruction_loss: 2.5641 - kl_loss: 4.2210 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 3.0518 - reconstruction_loss: 2.1257 - kl_loss: 4.3676 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.8722 - reconstruction_loss: 1.9382 - kl_loss: 4.5629 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.7409 - reconstruction_loss: 1.7976 - kl_loss: 4.7222 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.7856 - reconstruction_loss: 1.7368 - kl_loss: 4.8521 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.6069 - reconstruction_loss: 1.6257 - kl_loss: 4.8975 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.5508 - reconstruction_loss: 1.5592 - kl_loss: 4.9391 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.5425 - reconstruction_loss: 1.5358 - kl_loss: 4.9702 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.5595 - reconstruction_loss: 1.5151 - kl_loss: 4.9505 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.4836 - reconstruction_loss: 1.5311 - kl_loss: 4.9739 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.5068 - reconstruction_loss: 1.4891 - kl_loss: 4.9600 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.4345 - reconstruction_loss: 1.4493 - kl_loss: 4.9893 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.4277 - reconstruction_loss: 1.4244 - kl_loss: 4.9614 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.4581 - reconstruction_loss: 1.4419 - kl_loss: 4.9997 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.4262 - reconstruction_loss: 1.4775 - kl_loss: 5.0072 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.3897 - reconstruction_loss: 1.3847 - kl_loss: 4.9691 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.3568 - reconstruction_loss: 1.3764 - kl_loss: 4.9255 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.4134 - reconstruction_loss: 1.3994 - kl_loss: 4.9646 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.3928 - reconstruction_loss: 1.3943 - kl_loss: 4.9824 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.3332 - reconstruction_loss: 1.3651 - kl_loss: 4.9458 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.3393 - reconstruction_loss: 1.3637 - kl_loss: 4.9336 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.5521 - reconstruction_loss: 1.4822 - kl_loss: 5.0113 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.3502 - reconstruction_loss: 1.3619 - kl_loss: 4.9210 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.3807 - reconstruction_loss: 1.3714 - kl_loss: 4.9534 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.3649 - reconstruction_loss: 1.3727 - kl_loss: 4.9534 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.4146 - reconstruction_loss: 1.3997 - kl_loss: 5.0045\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.4145 - reconstruction_loss: 1.3997 - kl_loss: 5.0045 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.3501 - reconstruction_loss: 1.3328 - kl_loss: 4.9795 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.3465 - reconstruction_loss: 1.3631 - kl_loss: 5.0144 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.4424 - reconstruction_loss: 1.3854 - kl_loss: 5.0850 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.3290 - reconstruction_loss: 1.3213 - kl_loss: 4.9753 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.3331 - reconstruction_loss: 1.3203 - kl_loss: 4.9536 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2700 - reconstruction_loss: 1.3003 - kl_loss: 4.8696 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2721 - reconstruction_loss: 1.2984 - kl_loss: 4.8607 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2627 - reconstruction_loss: 1.2965 - kl_loss: 4.8564 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2827 - reconstruction_loss: 1.3040 - kl_loss: 4.8414 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2740 - reconstruction_loss: 1.2989 - kl_loss: 4.8469 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2669 - reconstruction_loss: 1.3036 - kl_loss: 4.8844\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2670 - reconstruction_loss: 1.3036 - kl_loss: 4.8844 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2840 - reconstruction_loss: 1.2887 - kl_loss: 4.8791 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2516 - reconstruction_loss: 1.2741 - kl_loss: 4.8682 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2674 - reconstruction_loss: 1.2854 - kl_loss: 4.8669 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2457 - reconstruction_loss: 1.2777 - kl_loss: 4.8468 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2467 - reconstruction_loss: 1.2795 - kl_loss: 4.8408 - lr: 2.5000e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2512 - reconstruction_loss: 1.2774 - kl_loss: 4.8552 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2434 - reconstruction_loss: 1.2775 - kl_loss: 4.8491\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2434 - reconstruction_loss: 1.2775 - kl_loss: 4.8491 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2490 - reconstruction_loss: 1.2687 - kl_loss: 4.8411 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2418 - reconstruction_loss: 1.2716 - kl_loss: 4.8369 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2238 - reconstruction_loss: 1.2624 - kl_loss: 4.8271 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2237 - reconstruction_loss: 1.2611 - kl_loss: 4.8152 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2231 - reconstruction_loss: 1.2628 - kl_loss: 4.8220 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2211 - reconstruction_loss: 1.2604 - kl_loss: 4.8098 - lr: 1.2500e-04\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2294 - reconstruction_loss: 1.2635 - kl_loss: 4.8177 - lr: 1.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2306 - reconstruction_loss: 1.2615 - kl_loss: 4.8019 - lr: 1.2500e-04\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2233 - reconstruction_loss: 1.2609 - kl_loss: 4.7969 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.2194 - reconstruction_loss: 1.2582 - kl_loss: 4.8014 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2139 - reconstruction_loss: 1.2561 - kl_loss: 4.7959 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2217 - reconstruction_loss: 1.2582 - kl_loss: 4.7995 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2183 - reconstruction_loss: 1.2595 - kl_loss: 4.8000 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2120 - reconstruction_loss: 1.2541 - kl_loss: 4.8093\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2120 - reconstruction_loss: 1.2541 - kl_loss: 4.8093 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2199 - reconstruction_loss: 1.2550 - kl_loss: 4.7899 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2078 - reconstruction_loss: 1.2520 - kl_loss: 4.7812 - lr: 6.2500e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2118 - reconstruction_loss: 1.2530 - kl_loss: 4.7778 - lr: 6.2500e-05\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2129 - reconstruction_loss: 1.2513 - kl_loss: 4.7905 - lr: 6.2500e-05\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2088 - reconstruction_loss: 1.2504 - kl_loss: 4.7898\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2088 - reconstruction_loss: 1.2504 - kl_loss: 4.7898 - lr: 6.2500e-05\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2094 - reconstruction_loss: 1.2494 - kl_loss: 4.7938 - lr: 3.1250e-05\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2022 - reconstruction_loss: 1.2444 - kl_loss: 4.7892 - lr: 3.1250e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2036 - reconstruction_loss: 1.2463 - kl_loss: 4.7818 - lr: 3.1250e-05\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2037 - reconstruction_loss: 1.2470 - kl_loss: 4.7867 - lr: 3.1250e-05\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2023 - reconstruction_loss: 1.2443 - kl_loss: 4.7848 - lr: 3.1250e-05\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2050 - reconstruction_loss: 1.2452 - kl_loss: 4.7867 - lr: 3.1250e-05\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2050 - reconstruction_loss: 1.2475 - kl_loss: 4.7847 - lr: 3.1250e-05\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2091 - reconstruction_loss: 1.2457 - kl_loss: 4.7862\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2091 - reconstruction_loss: 1.2457 - kl_loss: 4.7862 - lr: 3.1250e-05\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2120 - reconstruction_loss: 1.2444 - kl_loss: 4.7864 - lr: 1.5625e-05\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2072 - reconstruction_loss: 1.2459 - kl_loss: 4.7841 - lr: 1.5625e-05\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2109 - reconstruction_loss: 1.2437 - kl_loss: 4.7896\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2108 - reconstruction_loss: 1.2437 - kl_loss: 4.7896 - lr: 1.5625e-05\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.1959 - reconstruction_loss: 1.2428 - kl_loss: 4.7854 - lr: 7.8125e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.1943 - reconstruction_loss: 1.2437 - kl_loss: 4.7869 - lr: 7.8125e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2090 - reconstruction_loss: 1.2424 - kl_loss: 4.7852 - lr: 7.8125e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.1975 - reconstruction_loss: 1.2431 - kl_loss: 4.7861 - lr: 7.8125e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2126 - reconstruction_loss: 1.2409 - kl_loss: 4.7953 - lr: 7.8125e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2007 - reconstruction_loss: 1.2411 - kl_loss: 4.7852 - lr: 7.8125e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2032 - reconstruction_loss: 1.2445 - kl_loss: 4.7838 - lr: 7.8125e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2058 - reconstruction_loss: 1.2429 - kl_loss: 4.7916 - lr: 7.8125e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1987 - reconstruction_loss: 1.2437 - kl_loss: 4.7854\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.1987 - reconstruction_loss: 1.2437 - kl_loss: 4.7854 - lr: 7.8125e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2057 - reconstruction_loss: 1.2411 - kl_loss: 4.7907 - lr: 3.9063e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2005 - reconstruction_loss: 1.2415 - kl_loss: 4.7903 - lr: 3.9063e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2032 - reconstruction_loss: 1.2421 - kl_loss: 4.7863\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2032 - reconstruction_loss: 1.2421 - kl_loss: 4.7863 - lr: 3.9063e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2027 - reconstruction_loss: 1.2427 - kl_loss: 4.7800 - lr: 1.9531e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2023 - reconstruction_loss: 1.2430 - kl_loss: 4.7801 - lr: 1.9531e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1993 - reconstruction_loss: 1.2426 - kl_loss: 4.7793\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.1993 - reconstruction_loss: 1.2426 - kl_loss: 4.7793 - lr: 1.9531e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2033 - reconstruction_loss: 1.2424 - kl_loss: 4.7767 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2013 - reconstruction_loss: 1.2432 - kl_loss: 4.7769 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.1984 - reconstruction_loss: 1.2416 - kl_loss: 4.7779 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2058 - reconstruction_loss: 1.2425 - kl_loss: 4.7810 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.1974 - reconstruction_loss: 1.2430 - kl_loss: 4.7825 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2045 - reconstruction_loss: 1.2411 - kl_loss: 4.7844 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.1976 - reconstruction_loss: 1.2411 - kl_loss: 4.7837 - lr: 1.0000e-06\n",
      "Epoch 98/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.1986 - reconstruction_loss: 1.2411 - kl_loss: 4.7826 - lr: 1.0000e-06\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2016 - reconstruction_loss: 1.2413 - kl_loss: 4.7818 - lr: 1.0000e-06\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.1989 - reconstruction_loss: 1.2411 - kl_loss: 4.7815 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.1922 - reconstruction_loss: 1.2399 - kl_loss: 4.7787 - lr: 1.0000e-06\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.1914 - reconstruction_loss: 1.2405 - kl_loss: 4.7764 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.1930 - reconstruction_loss: 1.2426 - kl_loss: 4.7748 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.1994 - reconstruction_loss: 1.2440 - kl_loss: 4.7763 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.1932 - reconstruction_loss: 1.2399 - kl_loss: 4.7764 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.1914 - reconstruction_loss: 1.2428 - kl_loss: 4.7722 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2051 - reconstruction_loss: 1.2429 - kl_loss: 4.7756 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.1989 - reconstruction_loss: 1.2420 - kl_loss: 4.7782 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2032 - reconstruction_loss: 1.2413 - kl_loss: 4.7811 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.1998 - reconstruction_loss: 1.2413 - kl_loss: 4.7823 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.1980 - reconstruction_loss: 1.2392 - kl_loss: 4.7809 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.2020 - reconstruction_loss: 1.2417 - kl_loss: 4.7768 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2005 - reconstruction_loss: 1.2432 - kl_loss: 4.7795 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.1947 - reconstruction_loss: 1.2411 - kl_loss: 4.7783 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2008 - reconstruction_loss: 1.2402 - kl_loss: 4.7788 - lr: 1.0000e-06\n",
      "Epoch 115: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 12:12:06,803]\u001b[0m Trial 40 finished with value: 0.0048500028948246865 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 73, 'encoder_units_l2': 510, 'encoder_units_l3': 622, 'encoder_units_l4': 318, 'decoder_layers': 8, 'decoder_units_l1': 841, 'decoder_units_l2': 407, 'decoder_units_l3': 229, 'decoder_units_l4': 354, 'decoder_units_l5': 94, 'decoder_units_l6': 189, 'decoder_units_l7': 21, 'decoder_units_l8': 366, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 6.6187 - reconstruction_loss: 4.4244 - kl_loss: 3.1338 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.2398 - reconstruction_loss: 2.3024 - kl_loss: 4.1934 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.9504 - reconstruction_loss: 2.0408 - kl_loss: 4.3246 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.8407 - reconstruction_loss: 1.9149 - kl_loss: 4.4882 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.7071 - reconstruction_loss: 1.7658 - kl_loss: 4.6694 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.6194 - reconstruction_loss: 1.6461 - kl_loss: 4.7853 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5281 - reconstruction_loss: 1.5513 - kl_loss: 4.8242 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5511 - reconstruction_loss: 1.5743 - kl_loss: 4.8521 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4697 - reconstruction_loss: 1.4852 - kl_loss: 4.8576 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4743 - reconstruction_loss: 1.4873 - kl_loss: 4.8785 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4798 - reconstruction_loss: 1.4676 - kl_loss: 4.8677 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4454 - reconstruction_loss: 1.4966 - kl_loss: 4.8807 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4363 - reconstruction_loss: 1.4592 - kl_loss: 4.8550 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4036 - reconstruction_loss: 1.4328 - kl_loss: 4.8705 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.4888 - reconstruction_loss: 1.4636 - kl_loss: 4.8993 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4406 - reconstruction_loss: 1.4918 - kl_loss: 4.9842 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.4582 - reconstruction_loss: 1.4607 - kl_loss: 4.9380\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.4581 - reconstruction_loss: 1.4647 - kl_loss: 4.9384 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.6690 - reconstruction_loss: 1.5776 - kl_loss: 4.8240 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.4473 - reconstruction_loss: 1.4626 - kl_loss: 4.8736 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.3769 - reconstruction_loss: 1.4015 - kl_loss: 4.8802 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.4304 - reconstruction_loss: 1.4051 - kl_loss: 4.9226 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.3761 - reconstruction_loss: 1.3861 - kl_loss: 4.8587 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.3366 - reconstruction_loss: 1.3707 - kl_loss: 4.8288 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3928 - reconstruction_loss: 1.4309 - kl_loss: 4.9072 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.3597 - reconstruction_loss: 1.3814 - kl_loss: 4.8447 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.3228 - reconstruction_loss: 1.3613 - kl_loss: 4.8368 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3167 - reconstruction_loss: 1.3387 - kl_loss: 4.8363 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3001 - reconstruction_loss: 1.3293 - kl_loss: 4.8396 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2958 - reconstruction_loss: 1.3337 - kl_loss: 4.8535 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.3179 - reconstruction_loss: 1.3453 - kl_loss: 4.8625 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2886 - reconstruction_loss: 1.3164 - kl_loss: 4.8547 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2891 - reconstruction_loss: 1.3203 - kl_loss: 4.8326 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2985 - reconstruction_loss: 1.3338 - kl_loss: 4.8776 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.3092 - reconstruction_loss: 1.3344 - kl_loss: 4.8581 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2930 - reconstruction_loss: 1.3246 - kl_loss: 4.8551\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2930 - reconstruction_loss: 1.3229 - kl_loss: 4.8557 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3082 - reconstruction_loss: 1.3252 - kl_loss: 4.8798 - lr: 2.5000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2813 - reconstruction_loss: 1.3056 - kl_loss: 4.8422 - lr: 2.5000e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2699 - reconstruction_loss: 1.2990 - kl_loss: 4.8224 - lr: 2.5000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2619 - reconstruction_loss: 1.2926 - kl_loss: 4.8429 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2550 - reconstruction_loss: 1.2926 - kl_loss: 4.8193 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2534 - reconstruction_loss: 1.2902 - kl_loss: 4.8085 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2534 - reconstruction_loss: 1.2870 - kl_loss: 4.8237 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2643 - reconstruction_loss: 1.2900 - kl_loss: 4.8289 - lr: 2.5000e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2526 - reconstruction_loss: 1.2880 - kl_loss: 4.8147 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2508 - reconstruction_loss: 1.2888 - kl_loss: 4.8145 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2578 - reconstruction_loss: 1.2872 - kl_loss: 4.8295 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2585 - reconstruction_loss: 1.2875 - kl_loss: 4.8095 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2524 - reconstruction_loss: 1.2828 - kl_loss: 4.8283 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2525 - reconstruction_loss: 1.2867 - kl_loss: 4.8373 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2525 - reconstruction_loss: 1.2836 - kl_loss: 4.8336 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2492 - reconstruction_loss: 1.2866 - kl_loss: 4.8356\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2492 - reconstruction_loss: 1.2858 - kl_loss: 4.8349 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2302 - reconstruction_loss: 1.2666 - kl_loss: 4.8091 - lr: 1.2500e-04\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2309 - reconstruction_loss: 1.2665 - kl_loss: 4.8024 - lr: 1.2500e-04\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2390 - reconstruction_loss: 1.2695 - kl_loss: 4.8135 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2369 - reconstruction_loss: 1.2708 - kl_loss: 4.8299 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2298 - reconstruction_loss: 1.2668 - kl_loss: 4.8160\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2298 - reconstruction_loss: 1.2656 - kl_loss: 4.8161 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2294 - reconstruction_loss: 1.2641 - kl_loss: 4.8015 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2289 - reconstruction_loss: 1.2659 - kl_loss: 4.8154 - lr: 6.2500e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2204 - reconstruction_loss: 1.2594 - kl_loss: 4.8014 - lr: 6.2500e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2208 - reconstruction_loss: 1.2587 - kl_loss: 4.8128 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2199 - reconstruction_loss: 1.2578 - kl_loss: 4.8043 - lr: 6.2500e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2263 - reconstruction_loss: 1.2600 - kl_loss: 4.8034 - lr: 6.2500e-05\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2259 - reconstruction_loss: 1.2647 - kl_loss: 4.8017 - lr: 6.2500e-05\n",
      "Epoch 64/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2249 - reconstruction_loss: 1.2617 - kl_loss: 4.8087\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2249 - reconstruction_loss: 1.2605 - kl_loss: 4.8093 - lr: 6.2500e-05\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2126 - reconstruction_loss: 1.2545 - kl_loss: 4.8117 - lr: 3.1250e-05\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2138 - reconstruction_loss: 1.2532 - kl_loss: 4.8155 - lr: 3.1250e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2130 - reconstruction_loss: 1.2556 - kl_loss: 4.7988 - lr: 3.1250e-05\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2240 - reconstruction_loss: 1.2568 - kl_loss: 4.8239 - lr: 3.1250e-05\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2166 - reconstruction_loss: 1.2561 - kl_loss: 4.8086 - lr: 3.1250e-05\n",
      "Epoch 70/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2230 - reconstruction_loss: 1.2588 - kl_loss: 4.8069\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2230 - reconstruction_loss: 1.2581 - kl_loss: 4.8067 - lr: 3.1250e-05\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2204 - reconstruction_loss: 1.2517 - kl_loss: 4.8133 - lr: 1.5625e-05\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2183 - reconstruction_loss: 1.2553 - kl_loss: 4.8038 - lr: 1.5625e-05\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2109 - reconstruction_loss: 1.2505 - kl_loss: 4.8106 - lr: 1.5625e-05\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2133 - reconstruction_loss: 1.2499 - kl_loss: 4.8198 - lr: 1.5625e-05\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2156 - reconstruction_loss: 1.2519 - kl_loss: 4.8195 - lr: 1.5625e-05\n",
      "Epoch 76/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2083 - reconstruction_loss: 1.2512 - kl_loss: 4.8207\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2083 - reconstruction_loss: 1.2500 - kl_loss: 4.8206 - lr: 1.5625e-05\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2136 - reconstruction_loss: 1.2506 - kl_loss: 4.8087 - lr: 7.8125e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2119 - reconstruction_loss: 1.2518 - kl_loss: 4.8043 - lr: 7.8125e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2067 - reconstruction_loss: 1.2486 - kl_loss: 4.8070 - lr: 7.8125e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2118 - reconstruction_loss: 1.2499 - kl_loss: 4.8029 - lr: 7.8125e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2138 - reconstruction_loss: 1.2499 - kl_loss: 4.8151 - lr: 7.8125e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2117 - reconstruction_loss: 1.2482 - kl_loss: 4.8080 - lr: 7.8125e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2076 - reconstruction_loss: 1.2500 - kl_loss: 4.8022 - lr: 7.8125e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.1984 - reconstruction_loss: 1.2471 - kl_loss: 4.8073 - lr: 7.8125e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2083 - reconstruction_loss: 1.2482 - kl_loss: 4.8100 - lr: 7.8125e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2084 - reconstruction_loss: 1.2471 - kl_loss: 4.8097 - lr: 7.8125e-06\n",
      "Epoch 87/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2100 - reconstruction_loss: 1.2478 - kl_loss: 4.8118\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2100 - reconstruction_loss: 1.2468 - kl_loss: 4.8118 - lr: 7.8125e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2140 - reconstruction_loss: 1.2473 - kl_loss: 4.8074 - lr: 3.9063e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2122 - reconstruction_loss: 1.2456 - kl_loss: 4.8044 - lr: 3.9063e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2079 - reconstruction_loss: 1.2462 - kl_loss: 4.8027 - lr: 3.9063e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2122 - reconstruction_loss: 1.2468 - kl_loss: 4.8038 - lr: 3.9063e-06\n",
      "Epoch 92/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2081 - reconstruction_loss: 1.2458 - kl_loss: 4.8072\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2080 - reconstruction_loss: 1.2451 - kl_loss: 4.8075 - lr: 3.9063e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2016 - reconstruction_loss: 1.2449 - kl_loss: 4.8107 - lr: 1.9531e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2033 - reconstruction_loss: 1.2448 - kl_loss: 4.8074 - lr: 1.9531e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2117 - reconstruction_loss: 1.2453 - kl_loss: 4.8044 - lr: 1.9531e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2101 - reconstruction_loss: 1.2455 - kl_loss: 4.8082 - lr: 1.9531e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2082 - reconstruction_loss: 1.2438 - kl_loss: 4.8109 - lr: 1.9531e-06\n",
      "Epoch 98/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2128 - reconstruction_loss: 1.2456 - kl_loss: 4.8098 - lr: 1.9531e-06\n",
      "Epoch 99/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2119 - reconstruction_loss: 1.2449 - kl_loss: 4.8069 - lr: 1.9531e-06\n",
      "Epoch 100/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2027 - reconstruction_loss: 1.2453 - kl_loss: 4.8096\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2027 - reconstruction_loss: 1.2441 - kl_loss: 4.8099 - lr: 1.9531e-06\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2111 - reconstruction_loss: 1.2444 - kl_loss: 4.8152 - lr: 1.0000e-06\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2018 - reconstruction_loss: 1.2412 - kl_loss: 4.8145 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2018 - reconstruction_loss: 1.2434 - kl_loss: 4.8085 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2013 - reconstruction_loss: 1.2434 - kl_loss: 4.8039 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2016 - reconstruction_loss: 1.2440 - kl_loss: 4.8074 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.1990 - reconstruction_loss: 1.2441 - kl_loss: 4.8084 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2052 - reconstruction_loss: 1.2437 - kl_loss: 4.8107 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2048 - reconstruction_loss: 1.2412 - kl_loss: 4.8089 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2033 - reconstruction_loss: 1.2417 - kl_loss: 4.8058 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2096 - reconstruction_loss: 1.2449 - kl_loss: 4.8027 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2076 - reconstruction_loss: 1.2442 - kl_loss: 4.8037 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2086 - reconstruction_loss: 1.2434 - kl_loss: 4.8061 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2044 - reconstruction_loss: 1.2441 - kl_loss: 4.8078 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2090 - reconstruction_loss: 1.2423 - kl_loss: 4.8108 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2113 - reconstruction_loss: 1.2437 - kl_loss: 4.8077 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.1998 - reconstruction_loss: 1.2414 - kl_loss: 4.8074 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2045 - reconstruction_loss: 1.2422 - kl_loss: 4.8061 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2095 - reconstruction_loss: 1.2439 - kl_loss: 4.8066 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2016 - reconstruction_loss: 1.2424 - kl_loss: 4.8093 - lr: 1.0000e-06\n",
      "Epoch 119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 12:25:44,020]\u001b[0m Trial 41 finished with value: 0.0048577128703372 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 75, 'encoder_units_l2': 499, 'encoder_units_l3': 620, 'encoder_units_l4': 372, 'decoder_layers': 8, 'decoder_units_l1': 837, 'decoder_units_l2': 229, 'decoder_units_l3': 246, 'decoder_units_l4': 343, 'decoder_units_l5': 86, 'decoder_units_l6': 189, 'decoder_units_l7': 22, 'decoder_units_l8': 298, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 6.3935 - reconstruction_loss: 4.3381 - kl_loss: 3.2337 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 3.3995 - reconstruction_loss: 2.4151 - kl_loss: 4.1507 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 3.0053 - reconstruction_loss: 2.1062 - kl_loss: 4.3234 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.8698 - reconstruction_loss: 1.9482 - kl_loss: 4.4901 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.7607 - reconstruction_loss: 1.7950 - kl_loss: 4.6581 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.7192 - reconstruction_loss: 1.7286 - kl_loss: 4.7804 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5797 - reconstruction_loss: 1.6081 - kl_loss: 4.8341 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.6410 - reconstruction_loss: 1.6363 - kl_loss: 4.9056 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5367 - reconstruction_loss: 1.5166 - kl_loss: 4.9360 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4785 - reconstruction_loss: 1.4697 - kl_loss: 4.8810 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.5753 - reconstruction_loss: 1.5610 - kl_loss: 4.9285 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4419 - reconstruction_loss: 1.4459 - kl_loss: 4.9147 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4803 - reconstruction_loss: 1.4665 - kl_loss: 4.9050 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3810 - reconstruction_loss: 1.4283 - kl_loss: 4.8853 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3557 - reconstruction_loss: 1.3779 - kl_loss: 4.8302 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3416 - reconstruction_loss: 1.3962 - kl_loss: 4.8562 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3082 - reconstruction_loss: 1.3628 - kl_loss: 4.8277 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4082 - reconstruction_loss: 1.4010 - kl_loss: 4.8497 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3381 - reconstruction_loss: 1.3956 - kl_loss: 4.8533 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.4833 - reconstruction_loss: 1.5491 - kl_loss: 4.8715\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4834 - reconstruction_loss: 1.5491 - kl_loss: 4.8715 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4349 - reconstruction_loss: 1.3946 - kl_loss: 5.0137 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3130 - reconstruction_loss: 1.3403 - kl_loss: 4.8716 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3839 - reconstruction_loss: 1.3514 - kl_loss: 4.8366 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3358 - reconstruction_loss: 1.3576 - kl_loss: 4.9040 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2750 - reconstruction_loss: 1.3081 - kl_loss: 4.8031 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2685 - reconstruction_loss: 1.3145 - kl_loss: 4.7895 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2913 - reconstruction_loss: 1.3200 - kl_loss: 4.8040 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2586 - reconstruction_loss: 1.3007 - kl_loss: 4.7722 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3128 - reconstruction_loss: 1.3433 - kl_loss: 4.8900 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2704 - reconstruction_loss: 1.3052 - kl_loss: 4.7907 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2622 - reconstruction_loss: 1.2998 - kl_loss: 4.7758 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2514 - reconstruction_loss: 1.3041 - kl_loss: 4.7803 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3136 - reconstruction_loss: 1.3337 - kl_loss: 4.8420 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3408 - reconstruction_loss: 1.3481 - kl_loss: 4.9619\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3408 - reconstruction_loss: 1.3481 - kl_loss: 4.9619 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3037 - reconstruction_loss: 1.3019 - kl_loss: 4.9391 - lr: 2.5000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2805 - reconstruction_loss: 1.2956 - kl_loss: 4.8919 - lr: 2.5000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2572 - reconstruction_loss: 1.2834 - kl_loss: 4.8463 - lr: 2.5000e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2600 - reconstruction_loss: 1.2953 - kl_loss: 4.8836 - lr: 2.5000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2647 - reconstruction_loss: 1.2878 - kl_loss: 4.8876 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2720 - reconstruction_loss: 1.2873 - kl_loss: 4.8521\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2719 - reconstruction_loss: 1.2860 - kl_loss: 4.8520 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2344 - reconstruction_loss: 1.2699 - kl_loss: 4.8103 - lr: 1.2500e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2317 - reconstruction_loss: 1.2744 - kl_loss: 4.7824 - lr: 1.2500e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2310 - reconstruction_loss: 1.2722 - kl_loss: 4.7886 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2299 - reconstruction_loss: 1.2694 - kl_loss: 4.7975 - lr: 1.2500e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2324 - reconstruction_loss: 1.2710 - kl_loss: 4.8159 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2253 - reconstruction_loss: 1.2651 - kl_loss: 4.8061 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2268 - reconstruction_loss: 1.2676 - kl_loss: 4.8001 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2232 - reconstruction_loss: 1.2652 - kl_loss: 4.7966 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2301 - reconstruction_loss: 1.2697 - kl_loss: 4.7803 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2273 - reconstruction_loss: 1.2670 - kl_loss: 4.7923 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2268 - reconstruction_loss: 1.2711 - kl_loss: 4.7780\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2268 - reconstruction_loss: 1.2700 - kl_loss: 4.7778 - lr: 1.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2215 - reconstruction_loss: 1.2634 - kl_loss: 4.7750 - lr: 6.2500e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2222 - reconstruction_loss: 1.2638 - kl_loss: 4.7684 - lr: 6.2500e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2157 - reconstruction_loss: 1.2573 - kl_loss: 4.7754 - lr: 6.2500e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2153 - reconstruction_loss: 1.2546 - kl_loss: 4.7755 - lr: 6.2500e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2134 - reconstruction_loss: 1.2591 - kl_loss: 4.7609 - lr: 6.2500e-05\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2175 - reconstruction_loss: 1.2659 - kl_loss: 4.7511 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2098 - reconstruction_loss: 1.2616 - kl_loss: 4.7487\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2098 - reconstruction_loss: 1.2616 - kl_loss: 4.7487 - lr: 6.2500e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2113 - reconstruction_loss: 1.2588 - kl_loss: 4.7588 - lr: 3.1250e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2088 - reconstruction_loss: 1.2574 - kl_loss: 4.7652 - lr: 3.1250e-05\n",
      "Epoch 61/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2076 - reconstruction_loss: 1.2611 - kl_loss: 4.7588\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2076 - reconstruction_loss: 1.2598 - kl_loss: 4.7588 - lr: 3.1250e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2111 - reconstruction_loss: 1.2550 - kl_loss: 4.7691 - lr: 1.5625e-05\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2098 - reconstruction_loss: 1.2562 - kl_loss: 4.7484 - lr: 1.5625e-05\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2062 - reconstruction_loss: 1.2558 - kl_loss: 4.7586 - lr: 1.5625e-05\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2032 - reconstruction_loss: 1.2546 - kl_loss: 4.7602 - lr: 1.5625e-05\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2075 - reconstruction_loss: 1.2521 - kl_loss: 4.7577 - lr: 1.5625e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1993 - reconstruction_loss: 1.2536 - kl_loss: 4.7422 - lr: 1.5625e-05\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2044 - reconstruction_loss: 1.2549 - kl_loss: 4.7407 - lr: 1.5625e-05\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2051 - reconstruction_loss: 1.2540 - kl_loss: 4.7476 - lr: 1.5625e-05\n",
      "Epoch 70/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2068 - reconstruction_loss: 1.2542 - kl_loss: 4.7584\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2068 - reconstruction_loss: 1.2528 - kl_loss: 4.7586 - lr: 1.5625e-05\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2065 - reconstruction_loss: 1.2520 - kl_loss: 4.7636 - lr: 7.8125e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1992 - reconstruction_loss: 1.2504 - kl_loss: 4.7556 - lr: 7.8125e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2042 - reconstruction_loss: 1.2509 - kl_loss: 4.7619 - lr: 7.8125e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2023 - reconstruction_loss: 1.2519 - kl_loss: 4.7499 - lr: 7.8125e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1955 - reconstruction_loss: 1.2490 - kl_loss: 4.7489 - lr: 7.8125e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2047 - reconstruction_loss: 1.2501 - kl_loss: 4.7529 - lr: 7.8125e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2020 - reconstruction_loss: 1.2502 - kl_loss: 4.7549 - lr: 7.8125e-06\n",
      "Epoch 78/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2002 - reconstruction_loss: 1.2489 - kl_loss: 4.7662\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2002 - reconstruction_loss: 1.2474 - kl_loss: 4.7665 - lr: 7.8125e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2083 - reconstruction_loss: 1.2492 - kl_loss: 4.7730 - lr: 3.9063e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2077 - reconstruction_loss: 1.2506 - kl_loss: 4.7639 - lr: 3.9063e-06\n",
      "Epoch 81/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.1999 - reconstruction_loss: 1.2523 - kl_loss: 4.7470\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1999 - reconstruction_loss: 1.2508 - kl_loss: 4.7472 - lr: 3.9063e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1992 - reconstruction_loss: 1.2507 - kl_loss: 4.7498 - lr: 1.9531e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 4s 12ms/step - loss: 2.2032 - reconstruction_loss: 1.2526 - kl_loss: 4.7484 - lr: 1.9531e-06\n",
      "Epoch 84/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.1943 - reconstruction_loss: 1.2515 - kl_loss: 4.7490\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.1944 - reconstruction_loss: 1.2501 - kl_loss: 4.7490 - lr: 1.9531e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1981 - reconstruction_loss: 1.2499 - kl_loss: 4.7512 - lr: 1.0000e-06\n",
      "Epoch 85: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 12:31:59,925]\u001b[0m Trial 42 finished with value: 0.004879500938070406 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 50, 'encoder_units_l2': 756, 'encoder_units_l3': 820, 'encoder_units_l4': 544, 'decoder_layers': 8, 'decoder_units_l1': 571, 'decoder_units_l2': 379, 'decoder_units_l3': 368, 'decoder_units_l4': 162, 'decoder_units_l5': 129, 'decoder_units_l6': 147, 'decoder_units_l7': 22, 'decoder_units_l8': 398, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 8ms/step - loss: 6.1707 - reconstruction_loss: 4.1565 - kl_loss: 3.2932 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.2364 - reconstruction_loss: 2.2803 - kl_loss: 4.2272 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.9533 - reconstruction_loss: 2.0571 - kl_loss: 4.3543 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.9054 - reconstruction_loss: 1.9587 - kl_loss: 4.5468 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.7519 - reconstruction_loss: 1.7762 - kl_loss: 4.6842 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.6456 - reconstruction_loss: 1.6529 - kl_loss: 4.7725 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.6072 - reconstruction_loss: 1.5992 - kl_loss: 4.8380 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5808 - reconstruction_loss: 1.5303 - kl_loss: 4.8974 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4592 - reconstruction_loss: 1.5097 - kl_loss: 4.8555 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5292 - reconstruction_loss: 1.5829 - kl_loss: 4.9237 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4698 - reconstruction_loss: 1.4764 - kl_loss: 4.9140 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4502 - reconstruction_loss: 1.5120 - kl_loss: 4.9021 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4466 - reconstruction_loss: 1.4572 - kl_loss: 4.9210 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4160 - reconstruction_loss: 1.4179 - kl_loss: 4.8850 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4294 - reconstruction_loss: 1.4027 - kl_loss: 4.9103 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3888 - reconstruction_loss: 1.4219 - kl_loss: 4.8855 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.6242 - reconstruction_loss: 1.5827 - kl_loss: 4.8698 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.4259 - reconstruction_loss: 1.5205 - kl_loss: 4.9103\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4265 - reconstruction_loss: 1.5185 - kl_loss: 4.9108 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3484 - reconstruction_loss: 1.3594 - kl_loss: 4.8521 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3075 - reconstruction_loss: 1.3496 - kl_loss: 4.8302 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3039 - reconstruction_loss: 1.3305 - kl_loss: 4.8200 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3086 - reconstruction_loss: 1.3613 - kl_loss: 4.8377 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3200 - reconstruction_loss: 1.3540 - kl_loss: 4.8371 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3426 - reconstruction_loss: 1.3782 - kl_loss: 4.8928\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3426 - reconstruction_loss: 1.3782 - kl_loss: 4.8928 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3425 - reconstruction_loss: 1.3562 - kl_loss: 5.0035 - lr: 2.5000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.3075 - reconstruction_loss: 1.3228 - kl_loss: 4.8959 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.3122 - reconstruction_loss: 1.3226 - kl_loss: 4.9058\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3121 - reconstruction_loss: 1.3213 - kl_loss: 4.9056 - lr: 2.5000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2521 - reconstruction_loss: 1.2938 - kl_loss: 4.8155 - lr: 1.2500e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2783 - reconstruction_loss: 1.2990 - kl_loss: 4.7959 - lr: 1.2500e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2446 - reconstruction_loss: 1.2914 - kl_loss: 4.7974 - lr: 1.2500e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2448 - reconstruction_loss: 1.2917 - kl_loss: 4.7789 - lr: 1.2500e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2444 - reconstruction_loss: 1.2886 - kl_loss: 4.7780 - lr: 1.2500e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2579 - reconstruction_loss: 1.2965 - kl_loss: 4.7962 - lr: 1.2500e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2471 - reconstruction_loss: 1.2884 - kl_loss: 4.7823 - lr: 1.2500e-04\n",
      "Epoch 35/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.2727 - reconstruction_loss: 1.3121 - kl_loss: 4.8633\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2729 - reconstruction_loss: 1.3118 - kl_loss: 4.8629 - lr: 1.2500e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2915 - reconstruction_loss: 1.3094 - kl_loss: 4.8524 - lr: 6.2500e-05\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2661 - reconstruction_loss: 1.2994 - kl_loss: 4.8354 - lr: 6.2500e-05\n",
      "Epoch 38/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.2784 - reconstruction_loss: 1.3007 - kl_loss: 4.8330\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2782 - reconstruction_loss: 1.2991 - kl_loss: 4.8331 - lr: 6.2500e-05\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2491 - reconstruction_loss: 1.2851 - kl_loss: 4.8210 - lr: 3.1250e-05\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2484 - reconstruction_loss: 1.2841 - kl_loss: 4.8196 - lr: 3.1250e-05\n",
      "Epoch 41/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2447 - reconstruction_loss: 1.2859 - kl_loss: 4.7987\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2447 - reconstruction_loss: 1.2847 - kl_loss: 4.7988 - lr: 3.1250e-05\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2366 - reconstruction_loss: 1.2829 - kl_loss: 4.8003 - lr: 1.5625e-05\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2372 - reconstruction_loss: 1.2828 - kl_loss: 4.7776 - lr: 1.5625e-05\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2441 - reconstruction_loss: 1.2767 - kl_loss: 4.8021 - lr: 1.5625e-05\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2447 - reconstruction_loss: 1.2828 - kl_loss: 4.7971 - lr: 1.5625e-05\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2408 - reconstruction_loss: 1.2787 - kl_loss: 4.7883 - lr: 1.5625e-05\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2367 - reconstruction_loss: 1.2789 - kl_loss: 4.7977 - lr: 1.5625e-05\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2424 - reconstruction_loss: 1.2791 - kl_loss: 4.7793 - lr: 1.5625e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2331 - reconstruction_loss: 1.2771 - kl_loss: 4.7760 - lr: 1.5625e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2385 - reconstruction_loss: 1.2826 - kl_loss: 4.7852 - lr: 1.5625e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2371 - reconstruction_loss: 1.2827 - kl_loss: 4.7844 - lr: 1.5625e-05\n",
      "Epoch 52/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2448 - reconstruction_loss: 1.2830 - kl_loss: 4.7978\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2447 - reconstruction_loss: 1.2828 - kl_loss: 4.7978 - lr: 1.5625e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2444 - reconstruction_loss: 1.2837 - kl_loss: 4.7968 - lr: 7.8125e-06\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2393 - reconstruction_loss: 1.2817 - kl_loss: 4.7985 - lr: 7.8125e-06\n",
      "Epoch 55/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2429 - reconstruction_loss: 1.2813 - kl_loss: 4.7989\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2429 - reconstruction_loss: 1.2798 - kl_loss: 4.7993 - lr: 7.8125e-06\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2436 - reconstruction_loss: 1.2833 - kl_loss: 4.7981 - lr: 3.9063e-06\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2385 - reconstruction_loss: 1.2822 - kl_loss: 4.7960 - lr: 3.9063e-06\n",
      "Epoch 58/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2468 - reconstruction_loss: 1.2825 - kl_loss: 4.7978\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2468 - reconstruction_loss: 1.2809 - kl_loss: 4.7979 - lr: 3.9063e-06\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2439 - reconstruction_loss: 1.2798 - kl_loss: 4.7957 - lr: 1.9531e-06\n",
      "Epoch 59: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 12:35:10,184]\u001b[0m Trial 43 finished with value: 0.005004152756378872 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 100, 'encoder_units_l2': 483, 'encoder_units_l3': 603, 'encoder_units_l4': 295, 'decoder_layers': 8, 'decoder_units_l1': 118, 'decoder_units_l2': 736, 'decoder_units_l3': 509, 'decoder_units_l4': 672, 'decoder_units_l5': 204, 'decoder_units_l6': 215, 'decoder_units_l7': 35, 'decoder_units_l8': 131, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 3.5751 - reconstruction_loss: 2.8780 - kl_loss: 1.8102 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 2.3305 - reconstruction_loss: 1.5576 - kl_loss: 3.2993 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 2.0386 - reconstruction_loss: 1.2879 - kl_loss: 3.6722 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.9712 - reconstruction_loss: 1.1817 - kl_loss: 3.8408 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.9797 - reconstruction_loss: 1.2257 - kl_loss: 3.7958 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.9087 - reconstruction_loss: 1.1206 - kl_loss: 3.8044 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.9314 - reconstruction_loss: 1.1658 - kl_loss: 3.8508 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7933 - reconstruction_loss: 0.9546 - kl_loss: 4.0476 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 1.6744 - reconstruction_loss: 0.8597 - kl_loss: 4.0512 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 1.6484 - reconstruction_loss: 0.8795 - kl_loss: 4.0251 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 1.6937 - reconstruction_loss: 0.8480 - kl_loss: 4.0341 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6374 - reconstruction_loss: 0.8370 - kl_loss: 4.0206 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6361 - reconstruction_loss: 0.9741 - kl_loss: 4.0012 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.9423 - reconstruction_loss: 1.1774 - kl_loss: 3.7252 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.8475 - reconstruction_loss: 1.1108 - kl_loss: 3.6490\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.8474 - reconstruction_loss: 1.1096 - kl_loss: 3.6496 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7689 - reconstruction_loss: 1.0004 - kl_loss: 3.7755 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7432 - reconstruction_loss: 0.9676 - kl_loss: 3.8579 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.7579 - reconstruction_loss: 0.9433 - kl_loss: 3.9462\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7578 - reconstruction_loss: 0.9421 - kl_loss: 3.9457 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6518 - reconstruction_loss: 0.8471 - kl_loss: 3.9855 - lr: 2.5000e-04\n",
      "Epoch 20/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6414 - reconstruction_loss: 0.8389 - kl_loss: 4.0087 - lr: 2.5000e-04\n",
      "Epoch 21/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6256 - reconstruction_loss: 0.8253 - kl_loss: 4.0077 - lr: 2.5000e-04\n",
      "Epoch 22/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6285 - reconstruction_loss: 0.8285 - kl_loss: 3.9989 - lr: 2.5000e-04\n",
      "Epoch 23/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6299 - reconstruction_loss: 0.8250 - kl_loss: 4.0096 - lr: 2.5000e-04\n",
      "Epoch 24/300\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.6443 - reconstruction_loss: 0.8397 - kl_loss: 3.9949\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6443 - reconstruction_loss: 0.8387 - kl_loss: 3.9944 - lr: 2.5000e-04\n",
      "Epoch 25/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6133 - reconstruction_loss: 0.8135 - kl_loss: 3.9959 - lr: 1.2500e-04\n",
      "Epoch 26/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6085 - reconstruction_loss: 0.8011 - kl_loss: 4.0120 - lr: 1.2500e-04\n",
      "Epoch 27/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6123 - reconstruction_loss: 0.7986 - kl_loss: 4.0194 - lr: 1.2500e-04\n",
      "Epoch 28/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5996 - reconstruction_loss: 0.7910 - kl_loss: 4.0351 - lr: 1.2500e-04\n",
      "Epoch 29/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6028 - reconstruction_loss: 0.8061 - kl_loss: 4.0242 - lr: 1.2500e-04\n",
      "Epoch 30/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6393 - reconstruction_loss: 0.8161 - kl_loss: 4.0360 - lr: 1.2500e-04\n",
      "Epoch 31/300\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.6108 - reconstruction_loss: 0.7974 - kl_loss: 4.0433\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6108 - reconstruction_loss: 0.7966 - kl_loss: 4.0431 - lr: 1.2500e-04\n",
      "Epoch 32/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5981 - reconstruction_loss: 0.7835 - kl_loss: 4.0476 - lr: 6.2500e-05\n",
      "Epoch 33/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5863 - reconstruction_loss: 0.7783 - kl_loss: 4.0574 - lr: 6.2500e-05\n",
      "Epoch 34/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5904 - reconstruction_loss: 0.7767 - kl_loss: 4.0549 - lr: 6.2500e-05\n",
      "Epoch 35/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5911 - reconstruction_loss: 0.7759 - kl_loss: 4.0708 - lr: 6.2500e-05\n",
      "Epoch 36/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5931 - reconstruction_loss: 0.7728 - kl_loss: 4.0756 - lr: 6.2500e-05\n",
      "Epoch 37/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5814 - reconstruction_loss: 0.7702 - kl_loss: 4.0701 - lr: 6.2500e-05\n",
      "Epoch 38/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5799 - reconstruction_loss: 0.7637 - kl_loss: 4.0733 - lr: 6.2500e-05\n",
      "Epoch 39/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5842 - reconstruction_loss: 0.7625 - kl_loss: 4.0784 - lr: 6.2500e-05\n",
      "Epoch 40/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5786 - reconstruction_loss: 0.7613 - kl_loss: 4.0788 - lr: 6.2500e-05\n",
      "Epoch 41/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5730 - reconstruction_loss: 0.7612 - kl_loss: 4.0793 - lr: 6.2500e-05\n",
      "Epoch 42/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5779 - reconstruction_loss: 0.7587 - kl_loss: 4.0827 - lr: 6.2500e-05\n",
      "Epoch 43/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5743 - reconstruction_loss: 0.7597 - kl_loss: 4.0791 - lr: 6.2500e-05\n",
      "Epoch 44/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5754 - reconstruction_loss: 0.7599 - kl_loss: 4.0782 - lr: 6.2500e-05\n",
      "Epoch 45/300\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.5775 - reconstruction_loss: 0.7646 - kl_loss: 4.0851\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5776 - reconstruction_loss: 0.7639 - kl_loss: 4.0854 - lr: 6.2500e-05\n",
      "Epoch 46/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5841 - reconstruction_loss: 0.7633 - kl_loss: 4.0888 - lr: 3.1250e-05\n",
      "Epoch 47/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5773 - reconstruction_loss: 0.7614 - kl_loss: 4.0855 - lr: 3.1250e-05\n",
      "Epoch 48/300\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.5779 - reconstruction_loss: 0.7585 - kl_loss: 4.0939\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5779 - reconstruction_loss: 0.7576 - kl_loss: 4.0936 - lr: 3.1250e-05\n",
      "Epoch 49/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5806 - reconstruction_loss: 0.7610 - kl_loss: 4.0958 - lr: 1.5625e-05\n",
      "Epoch 50/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5740 - reconstruction_loss: 0.7601 - kl_loss: 4.0832 - lr: 1.5625e-05\n",
      "Epoch 51/300\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.5735 - reconstruction_loss: 0.7582 - kl_loss: 4.0936\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5735 - reconstruction_loss: 0.7575 - kl_loss: 4.0938 - lr: 1.5625e-05\n",
      "Epoch 52/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5731 - reconstruction_loss: 0.7567 - kl_loss: 4.0826 - lr: 7.8125e-06\n",
      "Epoch 53/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5721 - reconstruction_loss: 0.7552 - kl_loss: 4.0908 - lr: 7.8125e-06\n",
      "Epoch 54/300\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.5741 - reconstruction_loss: 0.7552 - kl_loss: 4.0955 - lr: 7.8125e-06\n",
      "Epoch 55/300\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.5764 - reconstruction_loss: 0.7561 - kl_loss: 4.0910\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 1.5763 - reconstruction_loss: 0.7553 - kl_loss: 4.0912 - lr: 7.8125e-06\n",
      "Epoch 56/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5685 - reconstruction_loss: 0.7517 - kl_loss: 4.0919 - lr: 3.9063e-06\n",
      "Epoch 57/300\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 1.5677 - reconstruction_loss: 0.7535 - kl_loss: 4.0903 - lr: 3.9063e-06\n",
      "Epoch 58/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5702 - reconstruction_loss: 0.7537 - kl_loss: 4.0858 - lr: 3.9063e-06\n",
      "Epoch 59/300\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 1.5686 - reconstruction_loss: 0.7521 - kl_loss: 4.0876 - lr: 3.9063e-06\n",
      "Epoch 60/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5729 - reconstruction_loss: 0.7526 - kl_loss: 4.0883 - lr: 3.9063e-06\n",
      "Epoch 61/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5657 - reconstruction_loss: 0.7527 - kl_loss: 4.0847 - lr: 3.9063e-06\n",
      "Epoch 62/300\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.5722 - reconstruction_loss: 0.7529 - kl_loss: 4.0878\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5722 - reconstruction_loss: 0.7520 - kl_loss: 4.0879 - lr: 3.9063e-06\n",
      "Epoch 63/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5715 - reconstruction_loss: 0.7532 - kl_loss: 4.0835 - lr: 1.9531e-06\n",
      "Epoch 64/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5691 - reconstruction_loss: 0.7526 - kl_loss: 4.0823 - lr: 1.9531e-06\n",
      "Epoch 65/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5691 - reconstruction_loss: 0.7523 - kl_loss: 4.0842 - lr: 1.9531e-06\n",
      "Epoch 66/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5675 - reconstruction_loss: 0.7529 - kl_loss: 4.0872 - lr: 1.9531e-06\n",
      "Epoch 67/300\n",
      "776/782 [============================>.] - ETA: 0s - loss: 1.5709 - reconstruction_loss: 0.7530 - kl_loss: 4.0893\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 1.5709 - reconstruction_loss: 0.7523 - kl_loss: 4.0890 - lr: 1.9531e-06\n",
      "Epoch 68/300\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 1.5718 - reconstruction_loss: 0.7514 - kl_loss: 4.0873 - lr: 1.0000e-06\n",
      "Epoch 69/300\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 1.5711 - reconstruction_loss: 0.7511 - kl_loss: 4.0889 - lr: 1.0000e-06\n",
      "Epoch 70/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5699 - reconstruction_loss: 0.7518 - kl_loss: 4.0859 - lr: 1.0000e-06\n",
      "Epoch 71/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5673 - reconstruction_loss: 0.7525 - kl_loss: 4.0845 - lr: 1.0000e-06\n",
      "Epoch 72/300\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.5710 - reconstruction_loss: 0.7523 - kl_loss: 4.0833 - lr: 1.0000e-06\n",
      "Epoch 73/300\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 1.5667 - reconstruction_loss: 0.7522 - kl_loss: 4.0850 - lr: 1.0000e-06\n",
      "Epoch 74/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5731 - reconstruction_loss: 0.7518 - kl_loss: 4.0856 - lr: 1.0000e-06\n",
      "Epoch 75/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5682 - reconstruction_loss: 0.7517 - kl_loss: 4.0877 - lr: 1.0000e-06\n",
      "Epoch 76/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5700 - reconstruction_loss: 0.7515 - kl_loss: 4.0860 - lr: 1.0000e-06\n",
      "Epoch 77/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5697 - reconstruction_loss: 0.7522 - kl_loss: 4.0866 - lr: 1.0000e-06\n",
      "Epoch 78/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5709 - reconstruction_loss: 0.7519 - kl_loss: 4.0858 - lr: 1.0000e-06\n",
      "Epoch 79/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5699 - reconstruction_loss: 0.7508 - kl_loss: 4.0843 - lr: 1.0000e-06\n",
      "Epoch 80/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5687 - reconstruction_loss: 0.7518 - kl_loss: 4.0842 - lr: 1.0000e-06\n",
      "Epoch 81/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5677 - reconstruction_loss: 0.7513 - kl_loss: 4.0862 - lr: 1.0000e-06\n",
      "Epoch 82/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5684 - reconstruction_loss: 0.7505 - kl_loss: 4.0868 - lr: 1.0000e-06\n",
      "Epoch 83/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5717 - reconstruction_loss: 0.7506 - kl_loss: 4.0851 - lr: 1.0000e-06\n",
      "Epoch 84/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5729 - reconstruction_loss: 0.7527 - kl_loss: 4.0852 - lr: 1.0000e-06\n",
      "Epoch 85/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5644 - reconstruction_loss: 0.7500 - kl_loss: 4.0879 - lr: 1.0000e-06\n",
      "Epoch 86/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5692 - reconstruction_loss: 0.7505 - kl_loss: 4.0896 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5662 - reconstruction_loss: 0.7511 - kl_loss: 4.0884 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5675 - reconstruction_loss: 0.7491 - kl_loss: 4.0879 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5716 - reconstruction_loss: 0.7516 - kl_loss: 4.0897 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5715 - reconstruction_loss: 0.7506 - kl_loss: 4.0909 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5682 - reconstruction_loss: 0.7504 - kl_loss: 4.0891 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5679 - reconstruction_loss: 0.7513 - kl_loss: 4.0904 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5699 - reconstruction_loss: 0.7512 - kl_loss: 4.0913 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5710 - reconstruction_loss: 0.7498 - kl_loss: 4.0919 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5687 - reconstruction_loss: 0.7498 - kl_loss: 4.0909 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5697 - reconstruction_loss: 0.7494 - kl_loss: 4.0889 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5646 - reconstruction_loss: 0.7497 - kl_loss: 4.0884 - lr: 1.0000e-06\n",
      "Epoch 98/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5685 - reconstruction_loss: 0.7505 - kl_loss: 4.0884 - lr: 1.0000e-06\n",
      "Epoch 98: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 12:45:26,997]\u001b[0m Trial 44 finished with value: 0.005868431675296872 and parameters: {'encoder_layers': 6, 'encoder_units_l1': 81, 'encoder_units_l2': 293, 'encoder_units_l3': 839, 'encoder_units_l4': 786, 'encoder_units_l5': 195, 'encoder_units_l6': 237, 'decoder_layers': 7, 'decoder_units_l1': 972, 'decoder_units_l2': 266, 'decoder_units_l3': 343, 'decoder_units_l4': 231, 'decoder_units_l5': 49, 'decoder_units_l6': 303, 'decoder_units_l7': 23, 'beta': 0.2, 'lr': 0.001, 'batch_size': 128}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 7ms/step - loss: 10.9481 - reconstruction_loss: 10.8569 - kl_loss: 0.0297 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8120 - reconstruction_loss: 10.8457 - kl_loss: 7.4346e-07 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8682 - reconstruction_loss: 10.8466 - kl_loss: 4.0005e-07 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8412 - reconstruction_loss: 10.8466 - kl_loss: 2.7921e-07 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 10.8684 - reconstruction_loss: 10.8467 - kl_loss: 7.8604e-07\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8684 - reconstruction_loss: 10.8467 - kl_loss: 7.8604e-07 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8258 - reconstruction_loss: 10.8448 - kl_loss: 1.1901e-07 - lr: 0.0050\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8587 - reconstruction_loss: 10.8448 - kl_loss: 7.0528e-08 - lr: 0.0050\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8568 - reconstruction_loss: 10.8448 - kl_loss: 6.0601e-08 - lr: 0.0050\n",
      "Epoch 9/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 10.8395 - reconstruction_loss: 10.8545 - kl_loss: 3.9497e-07\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8396 - reconstruction_loss: 10.8447 - kl_loss: 3.9409e-07 - lr: 0.0050\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8487 - reconstruction_loss: 10.8439 - kl_loss: 3.1334e-08 - lr: 0.0025\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8587 - reconstruction_loss: 10.8439 - kl_loss: 2.7441e-08 - lr: 0.0025\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8293 - reconstruction_loss: 10.8439 - kl_loss: 2.4679e-08 - lr: 0.0025\n",
      "Epoch 13/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 10.8393 - reconstruction_loss: 10.8533 - kl_loss: 8.2815e-08\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8394 - reconstruction_loss: 10.8439 - kl_loss: 8.2591e-08 - lr: 0.0025\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8534 - reconstruction_loss: 10.8433 - kl_loss: 1.5207e-08 - lr: 0.0012\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8428 - reconstruction_loss: 10.8431 - kl_loss: 1.3159e-08 - lr: 0.0012\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8346 - reconstruction_loss: 10.8435 - kl_loss: 1.2488e-08 - lr: 0.0012\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8164 - reconstruction_loss: 10.8433 - kl_loss: 3.8929e-08 - lr: 0.0012\n",
      "Epoch 18/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 10.8567 - reconstruction_loss: 10.8543 - kl_loss: 5.9580e-08\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8566 - reconstruction_loss: 10.8433 - kl_loss: 5.9079e-08 - lr: 0.0012\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8021 - reconstruction_loss: 10.8430 - kl_loss: 6.8466e-09 - lr: 6.2500e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8268 - reconstruction_loss: 10.8431 - kl_loss: 6.0659e-09 - lr: 6.2500e-04\n",
      "Epoch 21/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 10.8380 - reconstruction_loss: 10.8518 - kl_loss: 5.7950e-09\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8380 - reconstruction_loss: 10.8430 - kl_loss: 5.7867e-09 - lr: 6.2500e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8725 - reconstruction_loss: 10.8430 - kl_loss: 4.5342e-09 - lr: 3.1250e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8647 - reconstruction_loss: 10.8429 - kl_loss: 4.2376e-09 - lr: 3.1250e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8273 - reconstruction_loss: 10.8429 - kl_loss: 3.9665e-09 - lr: 3.1250e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8354 - reconstruction_loss: 10.8429 - kl_loss: 3.7785e-09 - lr: 3.1250e-04\n",
      "Epoch 26/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 10.8645 - reconstruction_loss: 10.8538 - kl_loss: 4.5141e-09\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8644 - reconstruction_loss: 10.8429 - kl_loss: 4.4961e-09 - lr: 3.1250e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8496 - reconstruction_loss: 10.8428 - kl_loss: 2.5977e-09 - lr: 1.5625e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8741 - reconstruction_loss: 10.8428 - kl_loss: 2.3850e-09 - lr: 1.5625e-04\n",
      "Epoch 29/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 10.8295 - reconstruction_loss: 10.8536 - kl_loss: 2.2560e-09\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8297 - reconstruction_loss: 10.8428 - kl_loss: 2.2502e-09 - lr: 1.5625e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8776 - reconstruction_loss: 10.8428 - kl_loss: 1.9324e-09 - lr: 7.8125e-05\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8674 - reconstruction_loss: 10.8428 - kl_loss: 1.8133e-09 - lr: 7.8125e-05\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8790 - reconstruction_loss: 10.8428 - kl_loss: 1.7209e-09 - lr: 7.8125e-05\n",
      "Epoch 33/300\n",
      "384/391 [============================>.] - ETA: 0s - loss: 10.8612 - reconstruction_loss: 10.8549 - kl_loss: 1.6128e-09\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8610 - reconstruction_loss: 10.8428 - kl_loss: 1.6113e-09 - lr: 7.8125e-05\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8466 - reconstruction_loss: 10.8428 - kl_loss: 1.4594e-09 - lr: 3.9062e-05\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8519 - reconstruction_loss: 10.8428 - kl_loss: 1.3792e-09 - lr: 3.9062e-05\n",
      "Epoch 36/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 10.8886 - reconstruction_loss: 10.8534 - kl_loss: 1.3322e-09\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8882 - reconstruction_loss: 10.8428 - kl_loss: 1.3291e-09 - lr: 3.9062e-05\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8430 - reconstruction_loss: 10.8427 - kl_loss: 1.2603e-09 - lr: 1.9531e-05\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8748 - reconstruction_loss: 10.8427 - kl_loss: 1.2305e-09 - lr: 1.9531e-05\n",
      "Epoch 39/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 10.8476 - reconstruction_loss: 10.8543 - kl_loss: 1.1695e-09\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8476 - reconstruction_loss: 10.8427 - kl_loss: 1.1689e-09 - lr: 1.9531e-05\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8731 - reconstruction_loss: 10.8427 - kl_loss: 1.1322e-09 - lr: 9.7656e-06\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8494 - reconstruction_loss: 10.8427 - kl_loss: 1.1153e-09 - lr: 9.7656e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 10.8536 - reconstruction_loss: 10.8529 - kl_loss: 1.0939e-09\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8535 - reconstruction_loss: 10.8427 - kl_loss: 1.0919e-09 - lr: 9.7656e-06\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8635 - reconstruction_loss: 10.8427 - kl_loss: 1.0716e-09 - lr: 4.8828e-06\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8545 - reconstruction_loss: 10.8427 - kl_loss: 1.0536e-09 - lr: 4.8828e-06\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 10.8480 - reconstruction_loss: 10.8427 - kl_loss: 1.0420e-09\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8480 - reconstruction_loss: 10.8427 - kl_loss: 1.0420e-09 - lr: 4.8828e-06\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8610 - reconstruction_loss: 10.8427 - kl_loss: 1.0361e-09 - lr: 2.4414e-06\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8370 - reconstruction_loss: 10.8427 - kl_loss: 1.0249e-09 - lr: 2.4414e-06\n",
      "Epoch 48/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 10.8513 - reconstruction_loss: 10.8545 - kl_loss: 1.0122e-09\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8513 - reconstruction_loss: 10.8427 - kl_loss: 1.0130e-09 - lr: 2.4414e-06\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8608 - reconstruction_loss: 10.8427 - kl_loss: 1.0089e-09 - lr: 1.2207e-06\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8407 - reconstruction_loss: 10.8427 - kl_loss: 1.0010e-09 - lr: 1.2207e-06\n",
      "Epoch 51/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 10.8521 - reconstruction_loss: 10.8554 - kl_loss: 1.0066e-09\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8520 - reconstruction_loss: 10.8427 - kl_loss: 1.0068e-09 - lr: 1.2207e-06\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8574 - reconstruction_loss: 10.8427 - kl_loss: 9.9885e-10 - lr: 1.0000e-06\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8540 - reconstruction_loss: 10.8427 - kl_loss: 9.9147e-10 - lr: 1.0000e-06\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8445 - reconstruction_loss: 10.8427 - kl_loss: 9.8384e-10 - lr: 1.0000e-06\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8803 - reconstruction_loss: 10.8427 - kl_loss: 9.8396e-10 - lr: 1.0000e-06\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8482 - reconstruction_loss: 10.8427 - kl_loss: 9.9587e-10 - lr: 1.0000e-06\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8489 - reconstruction_loss: 10.8427 - kl_loss: 9.8253e-10 - lr: 1.0000e-06\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8300 - reconstruction_loss: 10.8427 - kl_loss: 9.7045e-10 - lr: 1.0000e-06\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8581 - reconstruction_loss: 10.8427 - kl_loss: 9.7199e-10 - lr: 1.0000e-06\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8449 - reconstruction_loss: 10.8427 - kl_loss: 9.6241e-10 - lr: 1.0000e-06\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8607 - reconstruction_loss: 10.8427 - kl_loss: 9.5609e-10 - lr: 1.0000e-06\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.8506 - reconstruction_loss: 10.8427 - kl_loss: 9.5532e-10 - lr: 1.0000e-06\n",
      "Epoch 62: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 12:48:21,353]\u001b[0m Trial 45 finished with value: 0.042395068762529034 and parameters: {'encoder_layers': 5, 'encoder_units_l1': 131, 'encoder_units_l2': 394, 'encoder_units_l3': 342, 'encoder_units_l4': 536, 'encoder_units_l5': 25, 'decoder_layers': 5, 'decoder_units_l1': 411, 'decoder_units_l2': 406, 'decoder_units_l3': 164, 'decoder_units_l4': 359, 'decoder_units_l5': 93, 'beta': 0.2, 'lr': 0.01, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 10.3976 - reconstruction_loss: 8.6346 - kl_loss: 0.3132 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0954 - reconstruction_loss: 8.3441 - kl_loss: 0.3493 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0766 - reconstruction_loss: 8.3185 - kl_loss: 0.3517 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.1165 - reconstruction_loss: 8.3081 - kl_loss: 0.3526 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.1079 - reconstruction_loss: 8.3037 - kl_loss: 0.3542 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0490 - reconstruction_loss: 8.2955 - kl_loss: 0.3531 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.1009 - reconstruction_loss: 8.2940 - kl_loss: 0.3543 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.1155 - reconstruction_loss: 8.2929 - kl_loss: 0.3543 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 10.0912 - reconstruction_loss: 8.3046 - kl_loss: 0.3529\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0910 - reconstruction_loss: 8.2987 - kl_loss: 0.3527 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0581 - reconstruction_loss: 8.2710 - kl_loss: 0.3563 - lr: 5.0000e-04\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0720 - reconstruction_loss: 8.2834 - kl_loss: 0.3557 - lr: 5.0000e-04\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0734 - reconstruction_loss: 8.2828 - kl_loss: 0.3538 - lr: 5.0000e-04\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0310 - reconstruction_loss: 8.2636 - kl_loss: 0.3550 - lr: 5.0000e-04\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0266 - reconstruction_loss: 8.2707 - kl_loss: 0.3512 - lr: 5.0000e-04\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0733 - reconstruction_loss: 8.2813 - kl_loss: 0.3540 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 10.0633 - reconstruction_loss: 8.2787 - kl_loss: 0.3554 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 10.0427 - reconstruction_loss: 8.2765 - kl_loss: 0.3523\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0427 - reconstruction_loss: 8.2693 - kl_loss: 0.3523 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 10.0705 - reconstruction_loss: 8.2748 - kl_loss: 0.3541 - lr: 2.5000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0349 - reconstruction_loss: 8.2518 - kl_loss: 0.3532 - lr: 2.5000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0436 - reconstruction_loss: 8.2609 - kl_loss: 0.3562 - lr: 2.5000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0299 - reconstruction_loss: 8.2517 - kl_loss: 0.3534 - lr: 2.5000e-04\n",
      "Epoch 22/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 10.0857 - reconstruction_loss: 8.2717 - kl_loss: 0.3566\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0853 - reconstruction_loss: 8.2637 - kl_loss: 0.3568 - lr: 2.5000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 10.0785 - reconstruction_loss: 8.2677 - kl_loss: 0.3577 - lr: 1.2500e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.0378 - reconstruction_loss: 8.2585 - kl_loss: 0.3543 - lr: 1.2500e-04\n",
      "Epoch 25/300\n",
      "384/391 [============================>.] - ETA: 0s - loss: 10.0551 - reconstruction_loss: 8.2820 - kl_loss: 0.3564\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0552 - reconstruction_loss: 8.2719 - kl_loss: 0.3564 - lr: 1.2500e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0337 - reconstruction_loss: 8.2496 - kl_loss: 0.3577 - lr: 6.2500e-05\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0741 - reconstruction_loss: 8.2707 - kl_loss: 0.3581 - lr: 6.2500e-05\n",
      "Epoch 28/300\n",
      "384/391 [============================>.] - ETA: 0s - loss: 10.1008 - reconstruction_loss: 8.2684 - kl_loss: 0.3577\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.0998 - reconstruction_loss: 8.2577 - kl_loss: 0.3577 - lr: 6.2500e-05\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 10.0624 - reconstruction_loss: 8.2604 - kl_loss: 0.3560 - lr: 3.1250e-05\n",
      "Epoch 29: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 12:49:31,304]\u001b[0m Trial 46 finished with value: 0.03238568836171027 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 63, 'encoder_units_l2': 233, 'encoder_units_l3': 674, 'decoder_layers': 6, 'decoder_units_l1': 260, 'decoder_units_l2': 166, 'decoder_units_l3': 220, 'decoder_units_l4': 125, 'decoder_units_l5': 162, 'decoder_units_l6': 37, 'beta': 5, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 3.6893 - reconstruction_loss: 3.1295 - kl_loss: 1.5271 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 3.1294 - reconstruction_loss: 2.4349 - kl_loss: 2.3279 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 2.3237 - reconstruction_loss: 1.5568 - kl_loss: 3.2319 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 2.0088 - reconstruction_loss: 1.2269 - kl_loss: 3.5963 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.8405 - reconstruction_loss: 1.0353 - kl_loss: 3.8338 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.7403 - reconstruction_loss: 0.9132 - kl_loss: 4.0089 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.6705 - reconstruction_loss: 0.8507 - kl_loss: 4.0034 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.6239 - reconstruction_loss: 0.8106 - kl_loss: 4.0040 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.6092 - reconstruction_loss: 0.8102 - kl_loss: 3.9949 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5913 - reconstruction_loss: 0.7828 - kl_loss: 3.9977 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5922 - reconstruction_loss: 0.7998 - kl_loss: 3.9909 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "782/782 [==============================] - 11s 13ms/step - loss: 1.6020 - reconstruction_loss: 0.7865 - kl_loss: 3.9949 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5635 - reconstruction_loss: 0.7659 - kl_loss: 3.9907 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5669 - reconstruction_loss: 0.7669 - kl_loss: 3.9811 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5699 - reconstruction_loss: 0.7735 - kl_loss: 4.0017 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5676 - reconstruction_loss: 0.7665 - kl_loss: 4.0068 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5541 - reconstruction_loss: 0.7603 - kl_loss: 3.9856 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "782/782 [==============================] - 11s 13ms/step - loss: 1.5682 - reconstruction_loss: 0.8195 - kl_loss: 3.9927 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.6171 - reconstruction_loss: 0.7760 - kl_loss: 4.0408 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.5782 - reconstruction_loss: 0.7655 - kl_loss: 4.0544\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5782 - reconstruction_loss: 0.7648 - kl_loss: 4.0542 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5491 - reconstruction_loss: 0.7414 - kl_loss: 4.0425 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5418 - reconstruction_loss: 0.7343 - kl_loss: 4.0178 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "782/782 [==============================] - 11s 13ms/step - loss: 1.7636 - reconstruction_loss: 1.0592 - kl_loss: 3.9381 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.8053 - reconstruction_loss: 0.9820 - kl_loss: 3.9403 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.7334 - reconstruction_loss: 0.8976 - kl_loss: 4.0184\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.7332 - reconstruction_loss: 0.8963 - kl_loss: 4.0184 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5558 - reconstruction_loss: 0.7353 - kl_loss: 4.0587 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5491 - reconstruction_loss: 0.7343 - kl_loss: 4.0893 - lr: 2.5000e-04\n",
      "Epoch 28/300\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.5547 - reconstruction_loss: 0.7328 - kl_loss: 4.0949\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5547 - reconstruction_loss: 0.7320 - kl_loss: 4.0947 - lr: 2.5000e-04\n",
      "Epoch 29/300\n",
      "782/782 [==============================] - 11s 13ms/step - loss: 1.5390 - reconstruction_loss: 0.7217 - kl_loss: 4.0777 - lr: 1.2500e-04\n",
      "Epoch 30/300\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.5388 - reconstruction_loss: 0.7229 - kl_loss: 4.0588 - lr: 1.2500e-04\n",
      "Epoch 31/300\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.5385 - reconstruction_loss: 0.7265 - kl_loss: 4.0748 - lr: 1.2500e-04\n",
      "Epoch 32/300\n",
      "782/782 [==============================] - 11s 13ms/step - loss: 1.5427 - reconstruction_loss: 0.7260 - kl_loss: 4.0875 - lr: 1.2500e-04\n",
      "Epoch 33/300\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.5436 - reconstruction_loss: 0.7267 - kl_loss: 4.0915\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "782/782 [==============================] - 11s 13ms/step - loss: 1.5436 - reconstruction_loss: 0.7259 - kl_loss: 4.0912 - lr: 1.2500e-04\n",
      "Epoch 34/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5426 - reconstruction_loss: 0.7232 - kl_loss: 4.0866 - lr: 6.2500e-05\n",
      "Epoch 35/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5436 - reconstruction_loss: 0.7232 - kl_loss: 4.0904 - lr: 6.2500e-05\n",
      "Epoch 36/300\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.5457 - reconstruction_loss: 0.7218 - kl_loss: 4.0926\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5457 - reconstruction_loss: 0.7209 - kl_loss: 4.0928 - lr: 6.2500e-05\n",
      "Epoch 37/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5372 - reconstruction_loss: 0.7186 - kl_loss: 4.0876 - lr: 3.1250e-05\n",
      "Epoch 38/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5366 - reconstruction_loss: 0.7168 - kl_loss: 4.0948 - lr: 3.1250e-05\n",
      "Epoch 39/300\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.5391 - reconstruction_loss: 0.7189 - kl_loss: 4.0889\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5391 - reconstruction_loss: 0.7181 - kl_loss: 4.0889 - lr: 3.1250e-05\n",
      "Epoch 40/300\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5351 - reconstruction_loss: 0.7172 - kl_loss: 4.0889 - lr: 1.5625e-05\n",
      "Epoch 40: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 12:56:27,235]\u001b[0m Trial 47 finished with value: 0.005583152879158532 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 44, 'encoder_units_l2': 172, 'encoder_units_l3': 260, 'encoder_units_l4': 177, 'decoder_layers': 8, 'decoder_units_l1': 833, 'decoder_units_l2': 624, 'decoder_units_l3': 112, 'decoder_units_l4': 306, 'decoder_units_l5': 38, 'decoder_units_l6': 130, 'decoder_units_l7': 226, 'decoder_units_l8': 510, 'beta': 0.2, 'lr': 0.001, 'batch_size': 128}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 7s 16ms/step - loss: 9.1553 - reconstruction_loss: 7.1874 - kl_loss: 0.9197 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.5074 - reconstruction_loss: 4.5594 - kl_loss: 1.8535 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3379 - reconstruction_loss: 4.4560 - kl_loss: 1.8906 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3404 - reconstruction_loss: 4.4183 - kl_loss: 1.9100 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3131 - reconstruction_loss: 4.3605 - kl_loss: 1.9342 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2538 - reconstruction_loss: 4.2318 - kl_loss: 1.9964 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1716 - reconstruction_loss: 4.0265 - kl_loss: 2.1092 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0484 - reconstruction_loss: 3.8176 - kl_loss: 2.2064 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9563 - reconstruction_loss: 3.6488 - kl_loss: 2.2761 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8517 - reconstruction_loss: 3.5083 - kl_loss: 2.3312 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7719 - reconstruction_loss: 3.3793 - kl_loss: 2.3753 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6939 - reconstruction_loss: 3.2757 - kl_loss: 2.4072 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.6464 - reconstruction_loss: 3.2049 - kl_loss: 2.4195 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.5846 - reconstruction_loss: 3.1468 - kl_loss: 2.4390 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.5781 - reconstruction_loss: 3.1100 - kl_loss: 2.4523 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.5409 - reconstruction_loss: 3.0842 - kl_loss: 2.4566 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.5225 - reconstruction_loss: 3.0467 - kl_loss: 2.4625 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4844 - reconstruction_loss: 3.0157 - kl_loss: 2.4802 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4756 - reconstruction_loss: 3.0019 - kl_loss: 2.4831 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4585 - reconstruction_loss: 2.9889 - kl_loss: 2.4721 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4599 - reconstruction_loss: 2.9689 - kl_loss: 2.4846 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.4628 - reconstruction_loss: 2.9541 - kl_loss: 2.4965 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.4432 - reconstruction_loss: 2.9432 - kl_loss: 2.5003 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.4338 - reconstruction_loss: 2.9235 - kl_loss: 2.5010 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4113 - reconstruction_loss: 2.9076 - kl_loss: 2.5096 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4263 - reconstruction_loss: 2.9175 - kl_loss: 2.5132 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4424 - reconstruction_loss: 2.9205 - kl_loss: 2.5172 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4204 - reconstruction_loss: 2.8967 - kl_loss: 2.5185 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4116 - reconstruction_loss: 2.8940 - kl_loss: 2.5187 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4080 - reconstruction_loss: 2.8698 - kl_loss: 2.5326 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4246 - reconstruction_loss: 2.8614 - kl_loss: 2.5410 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3940 - reconstruction_loss: 2.8513 - kl_loss: 2.5451 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3992 - reconstruction_loss: 2.8624 - kl_loss: 2.5379 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3955 - reconstruction_loss: 2.8477 - kl_loss: 2.5492 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3847 - reconstruction_loss: 2.8366 - kl_loss: 2.5479 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4090 - reconstruction_loss: 2.8611 - kl_loss: 2.5429 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4000 - reconstruction_loss: 2.8425 - kl_loss: 2.5547 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 5.4036 - reconstruction_loss: 2.8521 - kl_loss: 2.5579\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.4036 - reconstruction_loss: 2.8493 - kl_loss: 2.5580 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3710 - reconstruction_loss: 2.8103 - kl_loss: 2.5616 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3769 - reconstruction_loss: 2.8032 - kl_loss: 2.5636 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3796 - reconstruction_loss: 2.8050 - kl_loss: 2.5675 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3825 - reconstruction_loss: 2.7966 - kl_loss: 2.5815 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 5.3664 - reconstruction_loss: 2.7995 - kl_loss: 2.5714\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3665 - reconstruction_loss: 2.7971 - kl_loss: 2.5717 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 5.3789 - reconstruction_loss: 2.7769 - kl_loss: 2.5874 - lr: 2.5000e-05\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3484 - reconstruction_loss: 2.7751 - kl_loss: 2.5811 - lr: 2.5000e-05\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3587 - reconstruction_loss: 2.7721 - kl_loss: 2.5932 - lr: 2.5000e-05\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3513 - reconstruction_loss: 2.7654 - kl_loss: 2.5922 - lr: 2.5000e-05\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 5.3569 - reconstruction_loss: 2.7663 - kl_loss: 2.5901\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3569 - reconstruction_loss: 2.7663 - kl_loss: 2.5901 - lr: 2.5000e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3507 - reconstruction_loss: 2.7578 - kl_loss: 2.5855 - lr: 1.2500e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3452 - reconstruction_loss: 2.7549 - kl_loss: 2.5907 - lr: 1.2500e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3406 - reconstruction_loss: 2.7519 - kl_loss: 2.5903 - lr: 1.2500e-05\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3531 - reconstruction_loss: 2.7537 - kl_loss: 2.5983 - lr: 1.2500e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3508 - reconstruction_loss: 2.7533 - kl_loss: 2.5980 - lr: 1.2500e-05\n",
      "Epoch 54/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 5.3544 - reconstruction_loss: 2.7475 - kl_loss: 2.5996\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3543 - reconstruction_loss: 2.7454 - kl_loss: 2.5996 - lr: 1.2500e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3474 - reconstruction_loss: 2.7477 - kl_loss: 2.5963 - lr: 6.2500e-06\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3486 - reconstruction_loss: 2.7457 - kl_loss: 2.6027 - lr: 6.2500e-06\n",
      "Epoch 57/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 5.3635 - reconstruction_loss: 2.7539 - kl_loss: 2.5985\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3634 - reconstruction_loss: 2.7508 - kl_loss: 2.5985 - lr: 6.2500e-06\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3556 - reconstruction_loss: 2.7453 - kl_loss: 2.6104 - lr: 3.1250e-06\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3531 - reconstruction_loss: 2.7444 - kl_loss: 2.6000 - lr: 3.1250e-06\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3488 - reconstruction_loss: 2.7422 - kl_loss: 2.5970 - lr: 3.1250e-06\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3619 - reconstruction_loss: 2.7467 - kl_loss: 2.6049 - lr: 3.1250e-06\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3359 - reconstruction_loss: 2.7431 - kl_loss: 2.5959 - lr: 3.1250e-06\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3486 - reconstruction_loss: 2.7440 - kl_loss: 2.6010 - lr: 3.1250e-06\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3554 - reconstruction_loss: 2.7417 - kl_loss: 2.6047 - lr: 3.1250e-06\n",
      "Epoch 65/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 5.3540 - reconstruction_loss: 2.7416 - kl_loss: 2.6012\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3539 - reconstruction_loss: 2.7387 - kl_loss: 2.6011 - lr: 3.1250e-06\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3431 - reconstruction_loss: 2.7480 - kl_loss: 2.5949 - lr: 1.5625e-06\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3341 - reconstruction_loss: 2.7406 - kl_loss: 2.5971 - lr: 1.5625e-06\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3393 - reconstruction_loss: 2.7360 - kl_loss: 2.6060 - lr: 1.5625e-06\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3514 - reconstruction_loss: 2.7396 - kl_loss: 2.5989 - lr: 1.5625e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3343 - reconstruction_loss: 2.7394 - kl_loss: 2.5960 - lr: 1.5625e-06\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3397 - reconstruction_loss: 2.7391 - kl_loss: 2.6036 - lr: 1.5625e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3323 - reconstruction_loss: 2.7352 - kl_loss: 2.6011 - lr: 1.5625e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3356 - reconstruction_loss: 2.7354 - kl_loss: 2.5983 - lr: 1.5625e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 5.3246 - reconstruction_loss: 2.7337 - kl_loss: 2.5977 - lr: 1.5625e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3321 - reconstruction_loss: 2.7340 - kl_loss: 2.6012 - lr: 1.5625e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3390 - reconstruction_loss: 2.7348 - kl_loss: 2.5997 - lr: 1.5625e-06\n",
      "Epoch 77/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 5.3333 - reconstruction_loss: 2.7380 - kl_loss: 2.6009\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3334 - reconstruction_loss: 2.7352 - kl_loss: 2.6010 - lr: 1.5625e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3453 - reconstruction_loss: 2.7314 - kl_loss: 2.6067 - lr: 1.0000e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 5.3517 - reconstruction_loss: 2.7330 - kl_loss: 2.6067 - lr: 1.0000e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3501 - reconstruction_loss: 2.7311 - kl_loss: 2.6072 - lr: 1.0000e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3306 - reconstruction_loss: 2.7308 - kl_loss: 2.5963 - lr: 1.0000e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3419 - reconstruction_loss: 2.7420 - kl_loss: 2.6011 - lr: 1.0000e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3199 - reconstruction_loss: 2.7276 - kl_loss: 2.5970 - lr: 1.0000e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3429 - reconstruction_loss: 2.7408 - kl_loss: 2.6004 - lr: 1.0000e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3365 - reconstruction_loss: 2.7320 - kl_loss: 2.6025 - lr: 1.0000e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3411 - reconstruction_loss: 2.7346 - kl_loss: 2.6036 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3493 - reconstruction_loss: 2.7314 - kl_loss: 2.6036 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3339 - reconstruction_loss: 2.7314 - kl_loss: 2.6024 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3414 - reconstruction_loss: 2.7307 - kl_loss: 2.6091 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3280 - reconstruction_loss: 2.7287 - kl_loss: 2.6005 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3307 - reconstruction_loss: 2.7321 - kl_loss: 2.5999 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3267 - reconstruction_loss: 2.7351 - kl_loss: 2.5962 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 5.3409 - reconstruction_loss: 2.7349 - kl_loss: 2.6028 - lr: 1.0000e-06\n",
      "Epoch 93: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 13:05:05,683]\u001b[0m Trial 48 finished with value: 0.010672217831080006 and parameters: {'encoder_layers': 5, 'encoder_units_l1': 190, 'encoder_units_l2': 652, 'encoder_units_l3': 75, 'encoder_units_l4': 423, 'encoder_units_l5': 757, 'decoder_layers': 7, 'decoder_units_l1': 292, 'decoder_units_l2': 88, 'decoder_units_l3': 304, 'decoder_units_l4': 192, 'decoder_units_l5': 246, 'decoder_units_l6': 567, 'decoder_units_l7': 41, 'beta': 1, 'lr': 0.0001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.9153 - reconstruction_loss: 1.6027 - kl_loss: 1.2125 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.8002 - reconstruction_loss: 1.4597 - kl_loss: 1.4926 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.6821 - reconstruction_loss: 1.3384 - kl_loss: 1.7020 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.6259 - reconstruction_loss: 1.2533 - kl_loss: 1.8328 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.5413 - reconstruction_loss: 1.0895 - kl_loss: 2.2369 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.4675 - reconstruction_loss: 0.9170 - kl_loss: 2.4157 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.2810 - reconstruction_loss: 0.7531 - kl_loss: 2.6232 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.2693 - reconstruction_loss: 0.7352 - kl_loss: 2.6523 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.2562 - reconstruction_loss: 0.7256 - kl_loss: 2.6590 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2545 - reconstruction_loss: 0.7163 - kl_loss: 2.6886 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2570 - reconstruction_loss: 0.7176 - kl_loss: 2.6974 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2458 - reconstruction_loss: 0.7039 - kl_loss: 2.7130 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2427 - reconstruction_loss: 0.6937 - kl_loss: 2.7334 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2399 - reconstruction_loss: 0.6881 - kl_loss: 2.7629 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2396 - reconstruction_loss: 0.6899 - kl_loss: 2.7706 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2363 - reconstruction_loss: 0.6760 - kl_loss: 2.7985 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2354 - reconstruction_loss: 0.6665 - kl_loss: 2.8306 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2313 - reconstruction_loss: 0.6575 - kl_loss: 2.8515 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2252 - reconstruction_loss: 0.6520 - kl_loss: 2.8811 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2270 - reconstruction_loss: 0.6464 - kl_loss: 2.8944 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2388 - reconstruction_loss: 0.6529 - kl_loss: 2.9137 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2233 - reconstruction_loss: 0.6387 - kl_loss: 2.9168 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2189 - reconstruction_loss: 0.6358 - kl_loss: 2.9324 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2183 - reconstruction_loss: 0.6284 - kl_loss: 2.9439 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2286 - reconstruction_loss: 0.6333 - kl_loss: 2.9518 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2155 - reconstruction_loss: 0.6189 - kl_loss: 2.9854 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2139 - reconstruction_loss: 0.6158 - kl_loss: 2.9846 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2128 - reconstruction_loss: 0.6114 - kl_loss: 3.0073 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.2129 - reconstruction_loss: 0.6126 - kl_loss: 3.0103 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "1559/1563 [============================>.] - ETA: 0s - loss: 1.2347 - reconstruction_loss: 0.6219 - kl_loss: 3.0193\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2347 - reconstruction_loss: 0.6217 - kl_loss: 3.0195 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2012 - reconstruction_loss: 0.5990 - kl_loss: 3.0058 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1962 - reconstruction_loss: 0.5965 - kl_loss: 3.0032 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1960 - reconstruction_loss: 0.5917 - kl_loss: 3.0126 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1957 - reconstruction_loss: 0.5879 - kl_loss: 3.0174 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1894 - reconstruction_loss: 0.5861 - kl_loss: 3.0340 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1928 - reconstruction_loss: 0.5838 - kl_loss: 3.0444 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 1.1937 - reconstruction_loss: 0.5822 - kl_loss: 3.0584\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.1937 - reconstruction_loss: 0.5821 - kl_loss: 3.0585 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.1881 - reconstruction_loss: 0.5727 - kl_loss: 3.0765 - lr: 2.5000e-04\n",
      "Epoch 39/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1846 - reconstruction_loss: 0.5651 - kl_loss: 3.0790 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1831 - reconstruction_loss: 0.5661 - kl_loss: 3.0937 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1913 - reconstruction_loss: 0.5632 - kl_loss: 3.1233 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 1.1830 - reconstruction_loss: 0.5615 - kl_loss: 3.1166\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1830 - reconstruction_loss: 0.5615 - kl_loss: 3.1166 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1800 - reconstruction_loss: 0.5555 - kl_loss: 3.1266 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1802 - reconstruction_loss: 0.5554 - kl_loss: 3.1268 - lr: 1.2500e-04\n",
      "Epoch 45/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1772 - reconstruction_loss: 0.5567 - kl_loss: 3.1072 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1791 - reconstruction_loss: 0.5576 - kl_loss: 3.0939 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1759 - reconstruction_loss: 0.5559 - kl_loss: 3.1021 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1785 - reconstruction_loss: 0.5545 - kl_loss: 3.1034 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1760 - reconstruction_loss: 0.5535 - kl_loss: 3.1103 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1735 - reconstruction_loss: 0.5488 - kl_loss: 3.1179 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1740 - reconstruction_loss: 0.5505 - kl_loss: 3.1123 - lr: 1.2500e-04\n",
      "Epoch 52/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1731 - reconstruction_loss: 0.5479 - kl_loss: 3.1286 - lr: 1.2500e-04\n",
      "Epoch 53/300\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 1.1748 - reconstruction_loss: 0.5479 - kl_loss: 3.1285\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1748 - reconstruction_loss: 0.5476 - kl_loss: 3.1282 - lr: 1.2500e-04\n",
      "Epoch 54/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1717 - reconstruction_loss: 0.5471 - kl_loss: 3.1222 - lr: 6.2500e-05\n",
      "Epoch 55/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1700 - reconstruction_loss: 0.5470 - kl_loss: 3.1175 - lr: 6.2500e-05\n",
      "Epoch 56/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1716 - reconstruction_loss: 0.5446 - kl_loss: 3.1232 - lr: 6.2500e-05\n",
      "Epoch 57/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1675 - reconstruction_loss: 0.5442 - kl_loss: 3.1215 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1679 - reconstruction_loss: 0.5444 - kl_loss: 3.1167 - lr: 6.2500e-05\n",
      "Epoch 59/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1690 - reconstruction_loss: 0.5426 - kl_loss: 3.1302 - lr: 6.2500e-05\n",
      "Epoch 60/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1682 - reconstruction_loss: 0.5422 - kl_loss: 3.1310 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1668 - reconstruction_loss: 0.5400 - kl_loss: 3.1321 - lr: 6.2500e-05\n",
      "Epoch 62/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1680 - reconstruction_loss: 0.5418 - kl_loss: 3.1291 - lr: 6.2500e-05\n",
      "Epoch 63/300\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1686 - reconstruction_loss: 0.5441 - kl_loss: 3.1222 - lr: 6.2500e-05\n",
      "Epoch 64/300\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 1.1672 - reconstruction_loss: 0.5413 - kl_loss: 3.1329\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1672 - reconstruction_loss: 0.5413 - kl_loss: 3.1329 - lr: 6.2500e-05\n",
      "Epoch 65/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1650 - reconstruction_loss: 0.5399 - kl_loss: 3.1328 - lr: 3.1250e-05\n",
      "Epoch 66/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1668 - reconstruction_loss: 0.5418 - kl_loss: 3.1307 - lr: 3.1250e-05\n",
      "Epoch 67/300\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 1.1689 - reconstruction_loss: 0.5429 - kl_loss: 3.1283\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1689 - reconstruction_loss: 0.5427 - kl_loss: 3.1284 - lr: 3.1250e-05\n",
      "Epoch 68/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1674 - reconstruction_loss: 0.5415 - kl_loss: 3.1268 - lr: 1.5625e-05\n",
      "Epoch 69/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1663 - reconstruction_loss: 0.5412 - kl_loss: 3.1243 - lr: 1.5625e-05\n",
      "Epoch 70/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1647 - reconstruction_loss: 0.5403 - kl_loss: 3.1292 - lr: 1.5625e-05\n",
      "Epoch 71/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1662 - reconstruction_loss: 0.5409 - kl_loss: 3.1254 - lr: 1.5625e-05\n",
      "Epoch 72/300\n",
      "1559/1563 [============================>.] - ETA: 0s - loss: 1.1679 - reconstruction_loss: 0.5405 - kl_loss: 3.1318\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1679 - reconstruction_loss: 0.5404 - kl_loss: 3.1319 - lr: 1.5625e-05\n",
      "Epoch 73/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1651 - reconstruction_loss: 0.5396 - kl_loss: 3.1318 - lr: 7.8125e-06\n",
      "Epoch 74/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1649 - reconstruction_loss: 0.5381 - kl_loss: 3.1322 - lr: 7.8125e-06\n",
      "Epoch 75/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1655 - reconstruction_loss: 0.5393 - kl_loss: 3.1275 - lr: 7.8125e-06\n",
      "Epoch 76/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1639 - reconstruction_loss: 0.5388 - kl_loss: 3.1344 - lr: 7.8125e-06\n",
      "Epoch 77/300\n",
      "1559/1563 [============================>.] - ETA: 0s - loss: 1.1647 - reconstruction_loss: 0.5385 - kl_loss: 3.1355\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1647 - reconstruction_loss: 0.5383 - kl_loss: 3.1356 - lr: 7.8125e-06\n",
      "Epoch 78/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1678 - reconstruction_loss: 0.5398 - kl_loss: 3.1357 - lr: 3.9063e-06\n",
      "Epoch 79/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1640 - reconstruction_loss: 0.5364 - kl_loss: 3.1366 - lr: 3.9063e-06\n",
      "Epoch 80/300\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1631 - reconstruction_loss: 0.5386 - kl_loss: 3.1308 - lr: 3.9063e-06\n",
      "Epoch 81/300\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1663 - reconstruction_loss: 0.5385 - kl_loss: 3.1304 - lr: 3.9063e-06\n",
      "Epoch 82/300\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 1.1638 - reconstruction_loss: 0.5393 - kl_loss: 3.1267\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1638 - reconstruction_loss: 0.5392 - kl_loss: 3.1267 - lr: 3.9063e-06\n",
      "Epoch 83/300\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1678 - reconstruction_loss: 0.5401 - kl_loss: 3.1325 - lr: 1.9531e-06\n",
      "Epoch 84/300\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1638 - reconstruction_loss: 0.5385 - kl_loss: 3.1329 - lr: 1.9531e-06\n",
      "Epoch 85/300\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 1.1639 - reconstruction_loss: 0.5376 - kl_loss: 3.1316\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1639 - reconstruction_loss: 0.5376 - kl_loss: 3.1316 - lr: 1.9531e-06\n",
      "Epoch 86/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1687 - reconstruction_loss: 0.5390 - kl_loss: 3.1323 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1676 - reconstruction_loss: 0.5392 - kl_loss: 3.1322 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1654 - reconstruction_loss: 0.5385 - kl_loss: 3.1329 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1617 - reconstruction_loss: 0.5377 - kl_loss: 3.1308 - lr: 1.0000e-06\n",
      "Epoch 89: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 13:19:52,948]\u001b[0m Trial 49 finished with value: 0.008417712545425159 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 807, 'encoder_units_l2': 55, 'encoder_units_l3': 420, 'decoder_layers': 8, 'decoder_units_l1': 67, 'decoder_units_l2': 986, 'decoder_units_l3': 17, 'decoder_units_l4': 835, 'decoder_units_l5': 27, 'decoder_units_l6': 262, 'decoder_units_l7': 55, 'decoder_units_l8': 43, 'beta': 0.2, 'lr': 0.001, 'batch_size': 64}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 5s 11ms/step - loss: 6.0824 - reconstruction_loss: 4.1100 - kl_loss: 3.4035 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 3.4976 - reconstruction_loss: 2.4477 - kl_loss: 4.2471 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 3.0074 - reconstruction_loss: 2.1007 - kl_loss: 4.4039 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.9292 - reconstruction_loss: 2.0248 - kl_loss: 4.5248 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.8365 - reconstruction_loss: 1.9119 - kl_loss: 4.6038 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.7223 - reconstruction_loss: 1.7106 - kl_loss: 4.7879 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.7327 - reconstruction_loss: 1.7403 - kl_loss: 4.9563 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 3.1525 - reconstruction_loss: 2.2241 - kl_loss: 4.8996 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.8705 - reconstruction_loss: 1.9237 - kl_loss: 4.8099\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.8706 - reconstruction_loss: 1.9220 - kl_loss: 4.8111 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.6849 - reconstruction_loss: 1.6240 - kl_loss: 4.9534 - lr: 5.0000e-04\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.5107 - reconstruction_loss: 1.5120 - kl_loss: 4.9210 - lr: 5.0000e-04\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4075 - reconstruction_loss: 1.4221 - kl_loss: 4.9315 - lr: 5.0000e-04\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4010 - reconstruction_loss: 1.4029 - kl_loss: 4.9024 - lr: 5.0000e-04\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3668 - reconstruction_loss: 1.3893 - kl_loss: 4.8726 - lr: 5.0000e-04\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4539 - reconstruction_loss: 1.4654 - kl_loss: 4.9658 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3775 - reconstruction_loss: 1.3734 - kl_loss: 4.9217 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4161 - reconstruction_loss: 1.4432 - kl_loss: 5.0561 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.7414 - reconstruction_loss: 1.8287 - kl_loss: 4.9931 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.6027 - reconstruction_loss: 1.4658 - kl_loss: 4.9379\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.6019 - reconstruction_loss: 1.4641 - kl_loss: 4.9380 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2967 - reconstruction_loss: 1.3190 - kl_loss: 4.8989 - lr: 2.5000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2929 - reconstruction_loss: 1.3135 - kl_loss: 4.8679 - lr: 2.5000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.3031 - reconstruction_loss: 1.3264 - kl_loss: 4.8726 - lr: 2.5000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2691 - reconstruction_loss: 1.2952 - kl_loss: 4.8253 - lr: 2.5000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2794 - reconstruction_loss: 1.2994 - kl_loss: 4.8300 - lr: 2.5000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2637 - reconstruction_loss: 1.3078 - kl_loss: 4.8323 - lr: 2.5000e-04\n",
      "Epoch 26/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2701 - reconstruction_loss: 1.3102 - kl_loss: 4.8292\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2702 - reconstruction_loss: 1.3087 - kl_loss: 4.8298 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2856 - reconstruction_loss: 1.3039 - kl_loss: 4.8728 - lr: 1.2500e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2718 - reconstruction_loss: 1.2994 - kl_loss: 4.8545 - lr: 1.2500e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2700 - reconstruction_loss: 1.2955 - kl_loss: 4.8792\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2700 - reconstruction_loss: 1.2955 - kl_loss: 4.8792 - lr: 1.2500e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2696 - reconstruction_loss: 1.2938 - kl_loss: 4.8891 - lr: 6.2500e-05\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2775 - reconstruction_loss: 1.3025 - kl_loss: 4.8923 - lr: 6.2500e-05\n",
      "Epoch 32/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2740 - reconstruction_loss: 1.2939 - kl_loss: 4.8834\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2740 - reconstruction_loss: 1.2931 - kl_loss: 4.8836 - lr: 6.2500e-05\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2551 - reconstruction_loss: 1.2790 - kl_loss: 4.8618 - lr: 3.1250e-05\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2421 - reconstruction_loss: 1.2770 - kl_loss: 4.8491 - lr: 3.1250e-05\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2488 - reconstruction_loss: 1.2738 - kl_loss: 4.8611 - lr: 3.1250e-05\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2385 - reconstruction_loss: 1.2715 - kl_loss: 4.8340 - lr: 3.1250e-05\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2404 - reconstruction_loss: 1.2709 - kl_loss: 4.8365 - lr: 3.1250e-05\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2341 - reconstruction_loss: 1.2674 - kl_loss: 4.8354 - lr: 3.1250e-05\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2262 - reconstruction_loss: 1.2698 - kl_loss: 4.8239 - lr: 3.1250e-05\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2423 - reconstruction_loss: 1.2722 - kl_loss: 4.8392 - lr: 3.1250e-05\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2379 - reconstruction_loss: 1.2682 - kl_loss: 4.8295 - lr: 3.1250e-05\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2359 - reconstruction_loss: 1.2681 - kl_loss: 4.8296 - lr: 3.1250e-05\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2398 - reconstruction_loss: 1.2732 - kl_loss: 4.8350 - lr: 3.1250e-05\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2393 - reconstruction_loss: 1.2695 - kl_loss: 4.8366 - lr: 3.1250e-05\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2292 - reconstruction_loss: 1.2642 - kl_loss: 4.8253 - lr: 3.1250e-05\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2287 - reconstruction_loss: 1.2654 - kl_loss: 4.8178 - lr: 3.1250e-05\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2289 - reconstruction_loss: 1.2670 - kl_loss: 4.8244 - lr: 3.1250e-05\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2325 - reconstruction_loss: 1.2648 - kl_loss: 4.8243 - lr: 3.1250e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2257 - reconstruction_loss: 1.2640 - kl_loss: 4.8197 - lr: 3.1250e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2271 - reconstruction_loss: 1.2651 - kl_loss: 4.8101 - lr: 3.1250e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2263 - reconstruction_loss: 1.2618 - kl_loss: 4.7979 - lr: 3.1250e-05\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2291 - reconstruction_loss: 1.2634 - kl_loss: 4.8017 - lr: 3.1250e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2202 - reconstruction_loss: 1.2624 - kl_loss: 4.7915 - lr: 3.1250e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2170 - reconstruction_loss: 1.2621 - kl_loss: 4.7923 - lr: 3.1250e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2205 - reconstruction_loss: 1.2571 - kl_loss: 4.7925 - lr: 3.1250e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2167 - reconstruction_loss: 1.2576 - kl_loss: 4.7875 - lr: 3.1250e-05\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2205 - reconstruction_loss: 1.2578 - kl_loss: 4.7924 - lr: 3.1250e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2154 - reconstruction_loss: 1.2559 - kl_loss: 4.8000 - lr: 3.1250e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2267 - reconstruction_loss: 1.2659 - kl_loss: 4.7865\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2267 - reconstruction_loss: 1.2659 - kl_loss: 4.7865 - lr: 3.1250e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2158 - reconstruction_loss: 1.2564 - kl_loss: 4.7885 - lr: 1.5625e-05\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2103 - reconstruction_loss: 1.2538 - kl_loss: 4.7867 - lr: 1.5625e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2180 - reconstruction_loss: 1.2550 - kl_loss: 4.7756 - lr: 1.5625e-05\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2047 - reconstruction_loss: 1.2548 - kl_loss: 4.7748 - lr: 1.5625e-05\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2153 - reconstruction_loss: 1.2546 - kl_loss: 4.7753 - lr: 1.5625e-05\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2136 - reconstruction_loss: 1.2516 - kl_loss: 4.7807 - lr: 1.5625e-05\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2102 - reconstruction_loss: 1.2562 - kl_loss: 4.7757 - lr: 1.5625e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2147 - reconstruction_loss: 1.2544 - kl_loss: 4.7734 - lr: 1.5625e-05\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2156 - reconstruction_loss: 1.2541 - kl_loss: 4.7733\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2156 - reconstruction_loss: 1.2541 - kl_loss: 4.7733 - lr: 1.5625e-05\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2111 - reconstruction_loss: 1.2536 - kl_loss: 4.7755 - lr: 7.8125e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2077 - reconstruction_loss: 1.2524 - kl_loss: 4.7726 - lr: 7.8125e-06\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2015 - reconstruction_loss: 1.2537 - kl_loss: 4.7720 - lr: 7.8125e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2080 - reconstruction_loss: 1.2517 - kl_loss: 4.7832 - lr: 7.8125e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2006 - reconstruction_loss: 1.2530 - kl_loss: 4.7685 - lr: 7.8125e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2103 - reconstruction_loss: 1.2517 - kl_loss: 4.7687 - lr: 7.8125e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2075 - reconstruction_loss: 1.2505 - kl_loss: 4.7750 - lr: 7.8125e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2127 - reconstruction_loss: 1.2518 - kl_loss: 4.7713 - lr: 7.8125e-06\n",
      "Epoch 77/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2064 - reconstruction_loss: 1.2533 - kl_loss: 4.7729\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2064 - reconstruction_loss: 1.2520 - kl_loss: 4.7730 - lr: 7.8125e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2005 - reconstruction_loss: 1.2518 - kl_loss: 4.7746 - lr: 3.9063e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2013 - reconstruction_loss: 1.2511 - kl_loss: 4.7656 - lr: 3.9063e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2046 - reconstruction_loss: 1.2495 - kl_loss: 4.7703 - lr: 3.9063e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2082 - reconstruction_loss: 1.2512 - kl_loss: 4.7688 - lr: 3.9063e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2054 - reconstruction_loss: 1.2526 - kl_loss: 4.7655 - lr: 3.9063e-06\n",
      "Epoch 83/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.2056 - reconstruction_loss: 1.2541 - kl_loss: 4.7643\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2056 - reconstruction_loss: 1.2528 - kl_loss: 4.7642 - lr: 3.9063e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2061 - reconstruction_loss: 1.2505 - kl_loss: 4.7681 - lr: 1.9531e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2019 - reconstruction_loss: 1.2488 - kl_loss: 4.7632 - lr: 1.9531e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2043 - reconstruction_loss: 1.2495 - kl_loss: 4.7576 - lr: 1.9531e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1995 - reconstruction_loss: 1.2508 - kl_loss: 4.7578 - lr: 1.9531e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2032 - reconstruction_loss: 1.2495 - kl_loss: 4.7621 - lr: 1.9531e-06\n",
      "Epoch 89/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2085 - reconstruction_loss: 1.2523 - kl_loss: 4.7662\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2085 - reconstruction_loss: 1.2511 - kl_loss: 4.7663 - lr: 1.9531e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2006 - reconstruction_loss: 1.2477 - kl_loss: 4.7673 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2023 - reconstruction_loss: 1.2487 - kl_loss: 4.7602 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1923 - reconstruction_loss: 1.2486 - kl_loss: 4.7595 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1993 - reconstruction_loss: 1.2493 - kl_loss: 4.7605 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2027 - reconstruction_loss: 1.2481 - kl_loss: 4.7662 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1950 - reconstruction_loss: 1.2471 - kl_loss: 4.7622 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1952 - reconstruction_loss: 1.2463 - kl_loss: 4.7639 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2044 - reconstruction_loss: 1.2480 - kl_loss: 4.7632 - lr: 1.0000e-06\n",
      "Epoch 98/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1976 - reconstruction_loss: 1.2473 - kl_loss: 4.7627 - lr: 1.0000e-06\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2059 - reconstruction_loss: 1.2472 - kl_loss: 4.7692 - lr: 1.0000e-06\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2024 - reconstruction_loss: 1.2465 - kl_loss: 4.7676 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2020 - reconstruction_loss: 1.2470 - kl_loss: 4.7687 - lr: 1.0000e-06\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2003 - reconstruction_loss: 1.2468 - kl_loss: 4.7627 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2003 - reconstruction_loss: 1.2463 - kl_loss: 4.7628 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2022 - reconstruction_loss: 1.2469 - kl_loss: 4.7685 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2016 - reconstruction_loss: 1.2496 - kl_loss: 4.7629 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2006 - reconstruction_loss: 1.2493 - kl_loss: 4.7604 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2037 - reconstruction_loss: 1.2503 - kl_loss: 4.7618 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2080 - reconstruction_loss: 1.2487 - kl_loss: 4.7638 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2060 - reconstruction_loss: 1.2492 - kl_loss: 4.7679 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2003 - reconstruction_loss: 1.2451 - kl_loss: 4.7671 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1964 - reconstruction_loss: 1.2460 - kl_loss: 4.7624 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1976 - reconstruction_loss: 1.2474 - kl_loss: 4.7622 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1987 - reconstruction_loss: 1.2472 - kl_loss: 4.7625 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2034 - reconstruction_loss: 1.2483 - kl_loss: 4.7635 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2008 - reconstruction_loss: 1.2467 - kl_loss: 4.7624 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2016 - reconstruction_loss: 1.2467 - kl_loss: 4.7635 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2002 - reconstruction_loss: 1.2472 - kl_loss: 4.7622 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2045 - reconstruction_loss: 1.2472 - kl_loss: 4.7649 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2034 - reconstruction_loss: 1.2462 - kl_loss: 4.7649 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2003 - reconstruction_loss: 1.2463 - kl_loss: 4.7610 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2000 - reconstruction_loss: 1.2466 - kl_loss: 4.7597 - lr: 1.0000e-06\n",
      "Epoch 121: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 13:28:17,856]\u001b[0m Trial 50 finished with value: 0.004881238054561147 and parameters: {'encoder_layers': 7, 'encoder_units_l1': 71, 'encoder_units_l2': 907, 'encoder_units_l3': 896, 'encoder_units_l4': 655, 'encoder_units_l5': 306, 'encoder_units_l6': 46, 'encoder_units_l7': 120, 'decoder_layers': 6, 'decoder_units_l1': 16, 'decoder_units_l2': 144, 'decoder_units_l3': 453, 'decoder_units_l4': 549, 'decoder_units_l5': 80, 'decoder_units_l6': 393, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 7.1941 - reconstruction_loss: 5.6921 - kl_loss: 2.2545 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 4.2523 - reconstruction_loss: 3.2970 - kl_loss: 3.7206 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 3.2151 - reconstruction_loss: 2.2473 - kl_loss: 4.2891 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.9434 - reconstruction_loss: 2.0162 - kl_loss: 4.4800 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.7623 - reconstruction_loss: 1.8137 - kl_loss: 4.6485 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.7385 - reconstruction_loss: 1.7825 - kl_loss: 4.7860 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.6810 - reconstruction_loss: 1.7034 - kl_loss: 4.8827 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.6272 - reconstruction_loss: 1.6228 - kl_loss: 4.9709 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.5775 - reconstruction_loss: 1.5483 - kl_loss: 5.0493 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.5344 - reconstruction_loss: 1.5221 - kl_loss: 5.0691 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.6182 - reconstruction_loss: 1.5226 - kl_loss: 5.1221 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4684 - reconstruction_loss: 1.4506 - kl_loss: 5.0650 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4155 - reconstruction_loss: 1.4160 - kl_loss: 5.0173 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4442 - reconstruction_loss: 1.4420 - kl_loss: 5.0430 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4766 - reconstruction_loss: 1.4612 - kl_loss: 5.0191 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4048 - reconstruction_loss: 1.4097 - kl_loss: 4.9767 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4021 - reconstruction_loss: 1.4130 - kl_loss: 4.9703 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3486 - reconstruction_loss: 1.3779 - kl_loss: 4.9498 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3793 - reconstruction_loss: 1.3846 - kl_loss: 4.9260 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.4472 - reconstruction_loss: 1.4556 - kl_loss: 4.9752 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3779 - reconstruction_loss: 1.3718 - kl_loss: 4.9384 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3378 - reconstruction_loss: 1.4322 - kl_loss: 4.8864 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.8460 - reconstruction_loss: 1.7849 - kl_loss: 4.7871 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.5386 - reconstruction_loss: 1.4293 - kl_loss: 5.0274\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.5383 - reconstruction_loss: 1.4293 - kl_loss: 5.0274 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2984 - reconstruction_loss: 1.3097 - kl_loss: 4.9015 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2963 - reconstruction_loss: 1.3270 - kl_loss: 4.8702 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3163 - reconstruction_loss: 1.3280 - kl_loss: 4.8966 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.3041 - reconstruction_loss: 1.3179 - kl_loss: 4.9253\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3041 - reconstruction_loss: 1.3166 - kl_loss: 4.9257 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2650 - reconstruction_loss: 1.2843 - kl_loss: 4.8643 - lr: 2.5000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2773 - reconstruction_loss: 1.3117 - kl_loss: 4.9253 - lr: 2.5000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3194 - reconstruction_loss: 1.3151 - kl_loss: 4.9449 - lr: 2.5000e-04\n",
      "Epoch 32/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2958 - reconstruction_loss: 1.3083 - kl_loss: 4.8898\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2957 - reconstruction_loss: 1.3069 - kl_loss: 4.8901 - lr: 2.5000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2804 - reconstruction_loss: 1.2915 - kl_loss: 4.8738 - lr: 1.2500e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2792 - reconstruction_loss: 1.2942 - kl_loss: 4.8699 - lr: 1.2500e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2527 - reconstruction_loss: 1.2810 - kl_loss: 4.8649 - lr: 1.2500e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2674 - reconstruction_loss: 1.2840 - kl_loss: 4.8831 - lr: 1.2500e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2554 - reconstruction_loss: 1.2797 - kl_loss: 4.8741 - lr: 1.2500e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2468 - reconstruction_loss: 1.2717 - kl_loss: 4.8640 - lr: 1.2500e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2444 - reconstruction_loss: 1.2752 - kl_loss: 4.8501 - lr: 1.2500e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2549 - reconstruction_loss: 1.2734 - kl_loss: 4.8636 - lr: 1.2500e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2420 - reconstruction_loss: 1.2675 - kl_loss: 4.8511 - lr: 1.2500e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2309 - reconstruction_loss: 1.2665 - kl_loss: 4.8514 - lr: 1.2500e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2431 - reconstruction_loss: 1.2680 - kl_loss: 4.8568 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2368 - reconstruction_loss: 1.2668 - kl_loss: 4.8479 - lr: 1.2500e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2349 - reconstruction_loss: 1.2674 - kl_loss: 4.8555 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2326 - reconstruction_loss: 1.2577 - kl_loss: 4.8552 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2241 - reconstruction_loss: 1.2598 - kl_loss: 4.8323 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2218 - reconstruction_loss: 1.2605 - kl_loss: 4.8166 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2244 - reconstruction_loss: 1.2575 - kl_loss: 4.8212 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 4s 12ms/step - loss: 2.2264 - reconstruction_loss: 1.2574 - kl_loss: 4.8236 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2173 - reconstruction_loss: 1.2571 - kl_loss: 4.8201 - lr: 1.2500e-04\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2192 - reconstruction_loss: 1.2585 - kl_loss: 4.8154 - lr: 1.2500e-04\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2226 - reconstruction_loss: 1.2547 - kl_loss: 4.8183 - lr: 1.2500e-04\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2156 - reconstruction_loss: 1.2563 - kl_loss: 4.8112 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2239 - reconstruction_loss: 1.2579 - kl_loss: 4.8293 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2195 - reconstruction_loss: 1.2548 - kl_loss: 4.8125 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2195 - reconstruction_loss: 1.2529 - kl_loss: 4.8202 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2136 - reconstruction_loss: 1.2487 - kl_loss: 4.8126 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2124 - reconstruction_loss: 1.2548 - kl_loss: 4.8020 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2113 - reconstruction_loss: 1.2503 - kl_loss: 4.7961 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2113 - reconstruction_loss: 1.2496 - kl_loss: 4.7847 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2075 - reconstruction_loss: 1.2512 - kl_loss: 4.7772 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2028 - reconstruction_loss: 1.2508 - kl_loss: 4.7721 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2019 - reconstruction_loss: 1.2499 - kl_loss: 4.7839 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2060 - reconstruction_loss: 1.2472 - kl_loss: 4.7965 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.2074 - reconstruction_loss: 1.2542 - kl_loss: 4.7775\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2074 - reconstruction_loss: 1.2530 - kl_loss: 4.7772 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1995 - reconstruction_loss: 1.2438 - kl_loss: 4.7743 - lr: 6.2500e-05\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1990 - reconstruction_loss: 1.2426 - kl_loss: 4.7712 - lr: 6.2500e-05\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1992 - reconstruction_loss: 1.2453 - kl_loss: 4.7644 - lr: 6.2500e-05\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1993 - reconstruction_loss: 1.2418 - kl_loss: 4.7709 - lr: 6.2500e-05\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1971 - reconstruction_loss: 1.2420 - kl_loss: 4.7715 - lr: 6.2500e-05\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1973 - reconstruction_loss: 1.2447 - kl_loss: 4.7630 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.1957 - reconstruction_loss: 1.2431 - kl_loss: 4.7727\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1957 - reconstruction_loss: 1.2419 - kl_loss: 4.7726 - lr: 6.2500e-05\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1941 - reconstruction_loss: 1.2405 - kl_loss: 4.7637 - lr: 3.1250e-05\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1973 - reconstruction_loss: 1.2400 - kl_loss: 4.7614 - lr: 3.1250e-05\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.1839 - reconstruction_loss: 1.2403 - kl_loss: 4.7532 - lr: 3.1250e-05\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1944 - reconstruction_loss: 1.2375 - kl_loss: 4.7655 - lr: 3.1250e-05\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1883 - reconstruction_loss: 1.2414 - kl_loss: 4.7530 - lr: 3.1250e-05\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1972 - reconstruction_loss: 1.2403 - kl_loss: 4.7637 - lr: 3.1250e-05\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1895 - reconstruction_loss: 1.2391 - kl_loss: 4.7564 - lr: 3.1250e-05\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1953 - reconstruction_loss: 1.2378 - kl_loss: 4.7703 - lr: 3.1250e-05\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1869 - reconstruction_loss: 1.2403 - kl_loss: 4.7514 - lr: 3.1250e-05\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1899 - reconstruction_loss: 1.2378 - kl_loss: 4.7513 - lr: 3.1250e-05\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1906 - reconstruction_loss: 1.2378 - kl_loss: 4.7618 - lr: 3.1250e-05\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1945 - reconstruction_loss: 1.2385 - kl_loss: 4.7518 - lr: 3.1250e-05\n",
      "Epoch 86/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.1857 - reconstruction_loss: 1.2390 - kl_loss: 4.7586\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1857 - reconstruction_loss: 1.2374 - kl_loss: 4.7590 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1891 - reconstruction_loss: 1.2381 - kl_loss: 4.7425 - lr: 1.5625e-05\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1892 - reconstruction_loss: 1.2361 - kl_loss: 4.7662 - lr: 1.5625e-05\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1865 - reconstruction_loss: 1.2353 - kl_loss: 4.7588 - lr: 1.5625e-05\n",
      "Epoch 90/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.1815 - reconstruction_loss: 1.2373 - kl_loss: 4.7502\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1816 - reconstruction_loss: 1.2364 - kl_loss: 4.7504 - lr: 1.5625e-05\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1909 - reconstruction_loss: 1.2356 - kl_loss: 4.7626 - lr: 7.8125e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1906 - reconstruction_loss: 1.2344 - kl_loss: 4.7599 - lr: 7.8125e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1875 - reconstruction_loss: 1.2342 - kl_loss: 4.7585 - lr: 7.8125e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1867 - reconstruction_loss: 1.2333 - kl_loss: 4.7680 - lr: 7.8125e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1868 - reconstruction_loss: 1.2347 - kl_loss: 4.7569 - lr: 7.8125e-06\n",
      "Epoch 96/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.1906 - reconstruction_loss: 1.2359 - kl_loss: 4.7637\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1906 - reconstruction_loss: 1.2344 - kl_loss: 4.7640 - lr: 7.8125e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1750 - reconstruction_loss: 1.2325 - kl_loss: 4.7485 - lr: 3.9063e-06\n",
      "Epoch 98/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1924 - reconstruction_loss: 1.2360 - kl_loss: 4.7462 - lr: 3.9063e-06\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1854 - reconstruction_loss: 1.2340 - kl_loss: 4.7561 - lr: 3.9063e-06\n",
      "Epoch 100/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - ETA: 0s - loss: 2.1848 - reconstruction_loss: 1.2338 - kl_loss: 4.7551\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1848 - reconstruction_loss: 1.2338 - kl_loss: 4.7551 - lr: 3.9063e-06\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1851 - reconstruction_loss: 1.2353 - kl_loss: 4.7511 - lr: 1.9531e-06\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1923 - reconstruction_loss: 1.2337 - kl_loss: 4.7542 - lr: 1.9531e-06\n",
      "Epoch 103/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.1851 - reconstruction_loss: 1.2343 - kl_loss: 4.7559\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1851 - reconstruction_loss: 1.2333 - kl_loss: 4.7559 - lr: 1.9531e-06\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1829 - reconstruction_loss: 1.2336 - kl_loss: 4.7520 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1800 - reconstruction_loss: 1.2334 - kl_loss: 4.7498 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1797 - reconstruction_loss: 1.2362 - kl_loss: 4.7481 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.1845 - reconstruction_loss: 1.2343 - kl_loss: 4.7533 - lr: 1.0000e-06\n",
      "Epoch 107: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 13:36:01,150]\u001b[0m Trial 51 finished with value: 0.004814435295280407 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 78, 'encoder_units_l2': 482, 'encoder_units_l3': 636, 'encoder_units_l4': 362, 'decoder_layers': 8, 'decoder_units_l1': 817, 'decoder_units_l2': 225, 'decoder_units_l3': 242, 'decoder_units_l4': 380, 'decoder_units_l5': 123, 'decoder_units_l6': 210, 'decoder_units_l7': 21, 'decoder_units_l8': 317, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 7.4846 - reconstruction_loss: 6.2448 - kl_loss: 1.8866 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 4.3343 - reconstruction_loss: 3.1646 - kl_loss: 3.8028 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.0905 - reconstruction_loss: 2.1883 - kl_loss: 4.3031 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.8890 - reconstruction_loss: 1.9677 - kl_loss: 4.4905 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.8015 - reconstruction_loss: 1.8186 - kl_loss: 4.6716 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.7938 - reconstruction_loss: 1.7629 - kl_loss: 4.8087 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.6352 - reconstruction_loss: 1.6585 - kl_loss: 4.9135 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.6161 - reconstruction_loss: 1.6060 - kl_loss: 4.9335 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5122 - reconstruction_loss: 1.5447 - kl_loss: 4.9446 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4961 - reconstruction_loss: 1.5191 - kl_loss: 4.9526 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.6996 - reconstruction_loss: 1.5997 - kl_loss: 5.0007 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4572 - reconstruction_loss: 1.4851 - kl_loss: 4.9401 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4646 - reconstruction_loss: 1.4749 - kl_loss: 4.9204 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5405 - reconstruction_loss: 1.5223 - kl_loss: 5.0137 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4953 - reconstruction_loss: 1.4655 - kl_loss: 4.9541 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4151 - reconstruction_loss: 1.4183 - kl_loss: 4.9405 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4880 - reconstruction_loss: 1.5031 - kl_loss: 4.9855 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4492 - reconstruction_loss: 1.4593 - kl_loss: 4.9465 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.4311 - reconstruction_loss: 1.4233 - kl_loss: 4.9924\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4311 - reconstruction_loss: 1.4233 - kl_loss: 4.9924 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3446 - reconstruction_loss: 1.3357 - kl_loss: 4.9604 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3305 - reconstruction_loss: 1.3562 - kl_loss: 4.9644 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3363 - reconstruction_loss: 1.3378 - kl_loss: 4.9292 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3472 - reconstruction_loss: 1.3961 - kl_loss: 4.9650 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5587 - reconstruction_loss: 1.4769 - kl_loss: 4.9508 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3385 - reconstruction_loss: 1.3388 - kl_loss: 4.9153 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.5146 - reconstruction_loss: 1.5114 - kl_loss: 4.9484 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3262 - reconstruction_loss: 1.3358 - kl_loss: 4.9015 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.4246 - reconstruction_loss: 1.3829 - kl_loss: 4.9280 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3239 - reconstruction_loss: 1.3520 - kl_loss: 4.9154 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3039 - reconstruction_loss: 1.3205 - kl_loss: 4.8858 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3067 - reconstruction_loss: 1.3447 - kl_loss: 4.9293 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3811 - reconstruction_loss: 1.3740 - kl_loss: 4.9690 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.3293 - reconstruction_loss: 1.3372 - kl_loss: 4.9341\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.3293 - reconstruction_loss: 1.3359 - kl_loss: 4.9340 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2810 - reconstruction_loss: 1.2948 - kl_loss: 4.9083 - lr: 2.5000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2614 - reconstruction_loss: 1.2920 - kl_loss: 4.8814 - lr: 2.5000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2681 - reconstruction_loss: 1.2915 - kl_loss: 4.8635 - lr: 2.5000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2572 - reconstruction_loss: 1.2869 - kl_loss: 4.8623 - lr: 2.5000e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2509 - reconstruction_loss: 1.2792 - kl_loss: 4.8515 - lr: 2.5000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2528 - reconstruction_loss: 1.2782 - kl_loss: 4.8710 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2602 - reconstruction_loss: 1.2819 - kl_loss: 4.8565 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2596 - reconstruction_loss: 1.2854 - kl_loss: 4.8586\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2596 - reconstruction_loss: 1.2854 - kl_loss: 4.8586 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2489 - reconstruction_loss: 1.2782 - kl_loss: 4.8568 - lr: 1.2500e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2529 - reconstruction_loss: 1.2763 - kl_loss: 4.8449 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2468 - reconstruction_loss: 1.2727 - kl_loss: 4.8546 - lr: 1.2500e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2446 - reconstruction_loss: 1.2748 - kl_loss: 4.8513 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2343 - reconstruction_loss: 1.2688 - kl_loss: 4.8446 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2383 - reconstruction_loss: 1.2641 - kl_loss: 4.8388 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2328 - reconstruction_loss: 1.2651 - kl_loss: 4.8185 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2416 - reconstruction_loss: 1.2676 - kl_loss: 4.8419 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2335 - reconstruction_loss: 1.2661 - kl_loss: 4.8287 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2387 - reconstruction_loss: 1.2672 - kl_loss: 4.8291\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2387 - reconstruction_loss: 1.2672 - kl_loss: 4.8291 - lr: 1.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2240 - reconstruction_loss: 1.2578 - kl_loss: 4.8079 - lr: 6.2500e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2212 - reconstruction_loss: 1.2580 - kl_loss: 4.8092 - lr: 6.2500e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2198 - reconstruction_loss: 1.2580 - kl_loss: 4.8137 - lr: 6.2500e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2211 - reconstruction_loss: 1.2585 - kl_loss: 4.8160\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2211 - reconstruction_loss: 1.2585 - kl_loss: 4.8160 - lr: 6.2500e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2171 - reconstruction_loss: 1.2559 - kl_loss: 4.8157 - lr: 3.1250e-05\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2219 - reconstruction_loss: 1.2560 - kl_loss: 4.8070 - lr: 3.1250e-05\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2159 - reconstruction_loss: 1.2499 - kl_loss: 4.8124 - lr: 3.1250e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2111 - reconstruction_loss: 1.2553 - kl_loss: 4.8047 - lr: 3.1250e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2098 - reconstruction_loss: 1.2527 - kl_loss: 4.8128 - lr: 3.1250e-05\n",
      "Epoch 61/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2122 - reconstruction_loss: 1.2524 - kl_loss: 4.8081\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2122 - reconstruction_loss: 1.2511 - kl_loss: 4.8081 - lr: 3.1250e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2095 - reconstruction_loss: 1.2488 - kl_loss: 4.8020 - lr: 1.5625e-05\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2104 - reconstruction_loss: 1.2487 - kl_loss: 4.7989 - lr: 1.5625e-05\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2169 - reconstruction_loss: 1.2501 - kl_loss: 4.8003 - lr: 1.5625e-05\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2080 - reconstruction_loss: 1.2459 - kl_loss: 4.8008 - lr: 1.5625e-05\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2144 - reconstruction_loss: 1.2498 - kl_loss: 4.7975 - lr: 1.5625e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2008 - reconstruction_loss: 1.2451 - kl_loss: 4.8017 - lr: 1.5625e-05\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2069 - reconstruction_loss: 1.2463 - kl_loss: 4.8023 - lr: 1.5625e-05\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2147 - reconstruction_loss: 1.2476 - kl_loss: 4.8084 - lr: 1.5625e-05\n",
      "Epoch 70/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2082 - reconstruction_loss: 1.2499 - kl_loss: 4.8035\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2082 - reconstruction_loss: 1.2489 - kl_loss: 4.8036 - lr: 1.5625e-05\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2041 - reconstruction_loss: 1.2474 - kl_loss: 4.7977 - lr: 7.8125e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2062 - reconstruction_loss: 1.2455 - kl_loss: 4.8068 - lr: 7.8125e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2051 - reconstruction_loss: 1.2484 - kl_loss: 4.7895\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2051 - reconstruction_loss: 1.2484 - kl_loss: 4.7895 - lr: 7.8125e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2092 - reconstruction_loss: 1.2501 - kl_loss: 4.7906 - lr: 3.9063e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2038 - reconstruction_loss: 1.2470 - kl_loss: 4.8014 - lr: 3.9063e-06\n",
      "Epoch 76/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2043 - reconstruction_loss: 1.2467 - kl_loss: 4.8016\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.2043 - reconstruction_loss: 1.2455 - kl_loss: 4.8019 - lr: 3.9063e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2101 - reconstruction_loss: 1.2466 - kl_loss: 4.8001 - lr: 1.9531e-06\n",
      "Epoch 77: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 13:40:37,152]\u001b[0m Trial 52 finished with value: 0.004867776702069521 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 56, 'encoder_units_l2': 429, 'encoder_units_l3': 685, 'encoder_units_l4': 316, 'decoder_layers': 8, 'decoder_units_l1': 671, 'decoder_units_l2': 184, 'decoder_units_l3': 196, 'decoder_units_l4': 282, 'decoder_units_l5': 120, 'decoder_units_l6': 175, 'decoder_units_l7': 20, 'decoder_units_l8': 472, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 8s 17ms/step - loss: 6.2599 - reconstruction_loss: 4.2060 - kl_loss: 3.3228 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.8395 - reconstruction_loss: 2.8421 - kl_loss: 4.1342 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 3.2755 - reconstruction_loss: 2.2775 - kl_loss: 4.4758 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.0166 - reconstruction_loss: 2.0301 - kl_loss: 4.6858 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.8517 - reconstruction_loss: 1.8581 - kl_loss: 4.8220 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.7339 - reconstruction_loss: 1.7253 - kl_loss: 4.8868 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.6326 - reconstruction_loss: 1.6327 - kl_loss: 4.9220 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5895 - reconstruction_loss: 1.5828 - kl_loss: 4.9357 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5126 - reconstruction_loss: 1.5175 - kl_loss: 4.9583 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4851 - reconstruction_loss: 1.4992 - kl_loss: 4.9215 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4601 - reconstruction_loss: 1.4815 - kl_loss: 4.9239 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4372 - reconstruction_loss: 1.4613 - kl_loss: 4.9174 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4530 - reconstruction_loss: 1.4570 - kl_loss: 4.9072 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4331 - reconstruction_loss: 1.4328 - kl_loss: 4.8868 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4102 - reconstruction_loss: 1.4251 - kl_loss: 4.9067 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3999 - reconstruction_loss: 1.4537 - kl_loss: 4.8915 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4199 - reconstruction_loss: 1.4279 - kl_loss: 4.9026 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3934 - reconstruction_loss: 1.4067 - kl_loss: 4.8875 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3730 - reconstruction_loss: 1.3919 - kl_loss: 4.8840 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3718 - reconstruction_loss: 1.3857 - kl_loss: 4.8760 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3695 - reconstruction_loss: 1.3957 - kl_loss: 4.9071 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.3368 - reconstruction_loss: 1.3673 - kl_loss: 4.8612 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3508 - reconstruction_loss: 1.3964 - kl_loss: 4.8791 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4170 - reconstruction_loss: 1.4362 - kl_loss: 4.9178 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.3411 - reconstruction_loss: 1.3790 - kl_loss: 4.8649\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3413 - reconstruction_loss: 1.3779 - kl_loss: 4.8664 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3289 - reconstruction_loss: 1.3374 - kl_loss: 4.8739 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2968 - reconstruction_loss: 1.3238 - kl_loss: 4.8592 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3057 - reconstruction_loss: 1.3224 - kl_loss: 4.8704 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3046 - reconstruction_loss: 1.3347 - kl_loss: 4.8627 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3035 - reconstruction_loss: 1.3196 - kl_loss: 4.8591 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3103 - reconstruction_loss: 1.3306 - kl_loss: 4.8674 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2985 - reconstruction_loss: 1.3286 - kl_loss: 4.8252 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2973 - reconstruction_loss: 1.3170 - kl_loss: 4.8257 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2847 - reconstruction_loss: 1.3162 - kl_loss: 4.8354 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2922 - reconstruction_loss: 1.3205 - kl_loss: 4.8356 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2886 - reconstruction_loss: 1.3219 - kl_loss: 4.8478\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2886 - reconstruction_loss: 1.3205 - kl_loss: 4.8480 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2812 - reconstruction_loss: 1.3151 - kl_loss: 4.8601 - lr: 2.5000e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2701 - reconstruction_loss: 1.2981 - kl_loss: 4.8480 - lr: 2.5000e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2672 - reconstruction_loss: 1.2956 - kl_loss: 4.8337 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2545 - reconstruction_loss: 1.2870 - kl_loss: 4.8205 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2484 - reconstruction_loss: 1.2876 - kl_loss: 4.8057 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2579 - reconstruction_loss: 1.2879 - kl_loss: 4.8331 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2480 - reconstruction_loss: 1.2869 - kl_loss: 4.8143 - lr: 2.5000e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2496 - reconstruction_loss: 1.2833 - kl_loss: 4.8142 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2416 - reconstruction_loss: 1.2863 - kl_loss: 4.8068 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2478 - reconstruction_loss: 1.2865 - kl_loss: 4.8166 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2500 - reconstruction_loss: 1.2857 - kl_loss: 4.8360\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2500 - reconstruction_loss: 1.2849 - kl_loss: 4.8358 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2470 - reconstruction_loss: 1.2813 - kl_loss: 4.8050 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2352 - reconstruction_loss: 1.2706 - kl_loss: 4.8104 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2353 - reconstruction_loss: 1.2705 - kl_loss: 4.8179 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2362 - reconstruction_loss: 1.2728 - kl_loss: 4.8119 - lr: 1.2500e-04\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2283 - reconstruction_loss: 1.2694 - kl_loss: 4.8130 - lr: 1.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2319 - reconstruction_loss: 1.2734 - kl_loss: 4.8002 - lr: 1.2500e-04\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2374 - reconstruction_loss: 1.2696 - kl_loss: 4.8084 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2226 - reconstruction_loss: 1.2690 - kl_loss: 4.7989 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2305 - reconstruction_loss: 1.2720 - kl_loss: 4.7953 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2310 - reconstruction_loss: 1.2687 - kl_loss: 4.7985 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2376 - reconstruction_loss: 1.2682 - kl_loss: 4.8096 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2380 - reconstruction_loss: 1.2684 - kl_loss: 4.8097 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2342 - reconstruction_loss: 1.2655 - kl_loss: 4.8135 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2229 - reconstruction_loss: 1.2656 - kl_loss: 4.8096 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2162 - reconstruction_loss: 1.2592 - kl_loss: 4.8228 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2284 - reconstruction_loss: 1.2649 - kl_loss: 4.8056 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2291 - reconstruction_loss: 1.2641 - kl_loss: 4.8233 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2308 - reconstruction_loss: 1.2682 - kl_loss: 4.8052\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2308 - reconstruction_loss: 1.2670 - kl_loss: 4.8050 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2274 - reconstruction_loss: 1.2584 - kl_loss: 4.8136 - lr: 6.2500e-05\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2221 - reconstruction_loss: 1.2621 - kl_loss: 4.8088 - lr: 6.2500e-05\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2216 - reconstruction_loss: 1.2581 - kl_loss: 4.8155 - lr: 6.2500e-05\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2185 - reconstruction_loss: 1.2555 - kl_loss: 4.8106 - lr: 6.2500e-05\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2157 - reconstruction_loss: 1.2575 - kl_loss: 4.8057 - lr: 6.2500e-05\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2151 - reconstruction_loss: 1.2557 - kl_loss: 4.8054 - lr: 6.2500e-05\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2130 - reconstruction_loss: 1.2568 - kl_loss: 4.8069 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2195 - reconstruction_loss: 1.2545 - kl_loss: 4.8164 - lr: 6.2500e-05\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2138 - reconstruction_loss: 1.2546 - kl_loss: 4.8007 - lr: 6.2500e-05\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2269 - reconstruction_loss: 1.2566 - kl_loss: 4.8007 - lr: 6.2500e-05\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2159 - reconstruction_loss: 1.2547 - kl_loss: 4.8005 - lr: 6.2500e-05\n",
      "Epoch 77/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2228 - reconstruction_loss: 1.2556 - kl_loss: 4.8156\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2228 - reconstruction_loss: 1.2546 - kl_loss: 4.8149 - lr: 6.2500e-05\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2233 - reconstruction_loss: 1.2525 - kl_loss: 4.8142 - lr: 3.1250e-05\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2183 - reconstruction_loss: 1.2524 - kl_loss: 4.8145 - lr: 3.1250e-05\n",
      "Epoch 80/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2176 - reconstruction_loss: 1.2536 - kl_loss: 4.8169\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2176 - reconstruction_loss: 1.2525 - kl_loss: 4.8169 - lr: 3.1250e-05\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2160 - reconstruction_loss: 1.2487 - kl_loss: 4.8156 - lr: 1.5625e-05\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2104 - reconstruction_loss: 1.2486 - kl_loss: 4.8085 - lr: 1.5625e-05\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2183 - reconstruction_loss: 1.2484 - kl_loss: 4.8065 - lr: 1.5625e-05\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2154 - reconstruction_loss: 1.2507 - kl_loss: 4.8012 - lr: 1.5625e-05\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2188 - reconstruction_loss: 1.2506 - kl_loss: 4.8136 - lr: 1.5625e-05\n",
      "Epoch 86/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2126 - reconstruction_loss: 1.2510 - kl_loss: 4.8070\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2126 - reconstruction_loss: 1.2497 - kl_loss: 4.8072 - lr: 1.5625e-05\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2133 - reconstruction_loss: 1.2504 - kl_loss: 4.7934 - lr: 7.8125e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2070 - reconstruction_loss: 1.2478 - kl_loss: 4.8127 - lr: 7.8125e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2092 - reconstruction_loss: 1.2474 - kl_loss: 4.8156 - lr: 7.8125e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2074 - reconstruction_loss: 1.2498 - kl_loss: 4.7959 - lr: 7.8125e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2104 - reconstruction_loss: 1.2470 - kl_loss: 4.8098 - lr: 7.8125e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2075 - reconstruction_loss: 1.2473 - kl_loss: 4.8056 - lr: 7.8125e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2121 - reconstruction_loss: 1.2466 - kl_loss: 4.8036 - lr: 7.8125e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2031 - reconstruction_loss: 1.2460 - kl_loss: 4.8076 - lr: 7.8125e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2055 - reconstruction_loss: 1.2452 - kl_loss: 4.8077 - lr: 7.8125e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2075 - reconstruction_loss: 1.2469 - kl_loss: 4.8076 - lr: 7.8125e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2060 - reconstruction_loss: 1.2452 - kl_loss: 4.8146 - lr: 7.8125e-06\n",
      "Epoch 98/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2103 - reconstruction_loss: 1.2486 - kl_loss: 4.8122\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2103 - reconstruction_loss: 1.2472 - kl_loss: 4.8119 - lr: 7.8125e-06\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2066 - reconstruction_loss: 1.2475 - kl_loss: 4.8083 - lr: 3.9063e-06\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2038 - reconstruction_loss: 1.2440 - kl_loss: 4.8104 - lr: 3.9063e-06\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2037 - reconstruction_loss: 1.2450 - kl_loss: 4.7970 - lr: 3.9063e-06\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2093 - reconstruction_loss: 1.2490 - kl_loss: 4.7961 - lr: 3.9063e-06\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2069 - reconstruction_loss: 1.2476 - kl_loss: 4.8061 - lr: 3.9063e-06\n",
      "Epoch 104/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2053 - reconstruction_loss: 1.2446 - kl_loss: 4.8113\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2054 - reconstruction_loss: 1.2433 - kl_loss: 4.8114 - lr: 3.9063e-06\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2129 - reconstruction_loss: 1.2474 - kl_loss: 4.8061 - lr: 1.9531e-06\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2035 - reconstruction_loss: 1.2453 - kl_loss: 4.8059 - lr: 1.9531e-06\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2088 - reconstruction_loss: 1.2461 - kl_loss: 4.8073\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2088 - reconstruction_loss: 1.2461 - kl_loss: 4.8073 - lr: 1.9531e-06\n",
      "Epoch 108/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2048 - reconstruction_loss: 1.2437 - kl_loss: 4.8135 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2098 - reconstruction_loss: 1.2430 - kl_loss: 4.8139 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.1975 - reconstruction_loss: 1.2449 - kl_loss: 4.8086 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2056 - reconstruction_loss: 1.2446 - kl_loss: 4.8091 - lr: 1.0000e-06\n",
      "Epoch 111: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 13:52:50,432]\u001b[0m Trial 53 finished with value: 0.004866158768822328 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 23, 'encoder_units_l2': 292, 'encoder_units_l3': 567, 'decoder_layers': 7, 'decoder_units_l1': 572, 'decoder_units_l2': 251, 'decoder_units_l3': 247, 'decoder_units_l4': 388, 'decoder_units_l5': 135, 'decoder_units_l6': 92, 'decoder_units_l7': 30, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 7.0535 - reconstruction_loss: 6.1695 - kl_loss: 1.8868 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 5.4443 - reconstruction_loss: 3.7370 - kl_loss: 3.3780 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 3.0918 - reconstruction_loss: 2.2305 - kl_loss: 4.2712 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 3.0116 - reconstruction_loss: 2.1278 - kl_loss: 4.4652 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.8296 - reconstruction_loss: 1.8898 - kl_loss: 4.6667 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.7628 - reconstruction_loss: 1.7737 - kl_loss: 4.8723 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.6140 - reconstruction_loss: 1.6074 - kl_loss: 4.8680 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.6486 - reconstruction_loss: 1.7963 - kl_loss: 4.8439 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.7120 - reconstruction_loss: 1.7639 - kl_loss: 4.7937 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.6417 - reconstruction_loss: 1.6775 - kl_loss: 4.8184\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.6417 - reconstruction_loss: 1.6753 - kl_loss: 4.8191 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5209 - reconstruction_loss: 1.5339 - kl_loss: 4.8842 - lr: 5.0000e-04\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5215 - reconstruction_loss: 1.5305 - kl_loss: 4.8580 - lr: 5.0000e-04\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4795 - reconstruction_loss: 1.5068 - kl_loss: 4.8805 - lr: 5.0000e-04\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4644 - reconstruction_loss: 1.4623 - kl_loss: 4.8982 - lr: 5.0000e-04\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4834 - reconstruction_loss: 1.4430 - kl_loss: 4.9362 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4069 - reconstruction_loss: 1.4571 - kl_loss: 4.8928 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3892 - reconstruction_loss: 1.4315 - kl_loss: 4.8968 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3734 - reconstruction_loss: 1.4040 - kl_loss: 4.8821 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4217 - reconstruction_loss: 1.4407 - kl_loss: 4.9258 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4243 - reconstruction_loss: 1.4607 - kl_loss: 4.8992 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.5527 - reconstruction_loss: 1.5579 - kl_loss: 4.9811\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5527 - reconstruction_loss: 1.5565 - kl_loss: 4.9809 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3565 - reconstruction_loss: 1.3379 - kl_loss: 4.9501 - lr: 2.5000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2982 - reconstruction_loss: 1.3213 - kl_loss: 4.8868 - lr: 2.5000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3062 - reconstruction_loss: 1.3174 - kl_loss: 4.8884 - lr: 2.5000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3735 - reconstruction_loss: 1.3670 - kl_loss: 4.9726 - lr: 2.5000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3000 - reconstruction_loss: 1.3326 - kl_loss: 4.9531 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.3347 - reconstruction_loss: 1.3216 - kl_loss: 4.9598\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3346 - reconstruction_loss: 1.3202 - kl_loss: 4.9591 - lr: 2.5000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2527 - reconstruction_loss: 1.2924 - kl_loss: 4.8036 - lr: 1.2500e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2501 - reconstruction_loss: 1.2863 - kl_loss: 4.7997 - lr: 1.2500e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2597 - reconstruction_loss: 1.2925 - kl_loss: 4.7939 - lr: 1.2500e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2503 - reconstruction_loss: 1.2884 - kl_loss: 4.7831 - lr: 1.2500e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2405 - reconstruction_loss: 1.2917 - kl_loss: 4.8127 - lr: 1.2500e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.3176 - reconstruction_loss: 1.3116 - kl_loss: 4.9654 - lr: 1.2500e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3038 - reconstruction_loss: 1.2951 - kl_loss: 4.9185\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.3038 - reconstruction_loss: 1.2951 - kl_loss: 4.9185 - lr: 1.2500e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2452 - reconstruction_loss: 1.2731 - kl_loss: 4.8476 - lr: 6.2500e-05\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2390 - reconstruction_loss: 1.2722 - kl_loss: 4.8395 - lr: 6.2500e-05\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2278 - reconstruction_loss: 1.2683 - kl_loss: 4.8146 - lr: 6.2500e-05\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2273 - reconstruction_loss: 1.2667 - kl_loss: 4.7942 - lr: 6.2500e-05\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2188 - reconstruction_loss: 1.2651 - kl_loss: 4.7761 - lr: 6.2500e-05\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2305 - reconstruction_loss: 1.3011 - kl_loss: 4.7979 - lr: 6.2500e-05\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2640 - reconstruction_loss: 1.2913 - kl_loss: 4.8249 - lr: 6.2500e-05\n",
      "Epoch 42/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2476 - reconstruction_loss: 1.2885 - kl_loss: 4.8010\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2476 - reconstruction_loss: 1.2873 - kl_loss: 4.8012 - lr: 6.2500e-05\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2346 - reconstruction_loss: 1.2766 - kl_loss: 4.7744 - lr: 3.1250e-05\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2286 - reconstruction_loss: 1.2712 - kl_loss: 4.7730 - lr: 3.1250e-05\n",
      "Epoch 45/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2310 - reconstruction_loss: 1.2703 - kl_loss: 4.7791\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2309 - reconstruction_loss: 1.2689 - kl_loss: 4.7790 - lr: 3.1250e-05\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2226 - reconstruction_loss: 1.2629 - kl_loss: 4.7824 - lr: 1.5625e-05\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2139 - reconstruction_loss: 1.2623 - kl_loss: 4.7779 - lr: 1.5625e-05\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2219 - reconstruction_loss: 1.2664 - kl_loss: 4.7648 - lr: 1.5625e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2166 - reconstruction_loss: 1.2623 - kl_loss: 4.7692 - lr: 1.5625e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2137 - reconstruction_loss: 1.2678 - kl_loss: 4.7605 - lr: 1.5625e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2155 - reconstruction_loss: 1.2635 - kl_loss: 4.7681 - lr: 1.5625e-05\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2158 - reconstruction_loss: 1.2604 - kl_loss: 4.7740 - lr: 1.5625e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2217 - reconstruction_loss: 1.2686 - kl_loss: 4.7655 - lr: 1.5625e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2170 - reconstruction_loss: 1.2652 - kl_loss: 4.7721 - lr: 1.5625e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2248 - reconstruction_loss: 1.2641 - kl_loss: 4.7697\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2248 - reconstruction_loss: 1.2641 - kl_loss: 4.7697 - lr: 1.5625e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2189 - reconstruction_loss: 1.2627 - kl_loss: 4.7679 - lr: 7.8125e-06\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2183 - reconstruction_loss: 1.2622 - kl_loss: 4.7613 - lr: 7.8125e-06\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2144 - reconstruction_loss: 1.2641 - kl_loss: 4.7572 - lr: 7.8125e-06\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2149 - reconstruction_loss: 1.2618 - kl_loss: 4.7553 - lr: 7.8125e-06\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2165 - reconstruction_loss: 1.2623 - kl_loss: 4.7622 - lr: 7.8125e-06\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2116 - reconstruction_loss: 1.2589 - kl_loss: 4.7676 - lr: 7.8125e-06\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2048 - reconstruction_loss: 1.2586 - kl_loss: 4.7685 - lr: 7.8125e-06\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2150 - reconstruction_loss: 1.2587 - kl_loss: 4.7645 - lr: 7.8125e-06\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2097 - reconstruction_loss: 1.2589 - kl_loss: 4.7669 - lr: 7.8125e-06\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2111 - reconstruction_loss: 1.2602 - kl_loss: 4.7671 - lr: 7.8125e-06\n",
      "Epoch 66/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2159 - reconstruction_loss: 1.2611 - kl_loss: 4.7613\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2159 - reconstruction_loss: 1.2601 - kl_loss: 4.7611 - lr: 7.8125e-06\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2145 - reconstruction_loss: 1.2619 - kl_loss: 4.7580 - lr: 3.9063e-06\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2166 - reconstruction_loss: 1.2590 - kl_loss: 4.7623 - lr: 3.9063e-06\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2123 - reconstruction_loss: 1.2549 - kl_loss: 4.7696 - lr: 3.9063e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2122 - reconstruction_loss: 1.2580 - kl_loss: 4.7648 - lr: 3.9063e-06\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2085 - reconstruction_loss: 1.2597 - kl_loss: 4.7652 - lr: 3.9063e-06\n",
      "Epoch 72/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2112 - reconstruction_loss: 1.2580 - kl_loss: 4.7611\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2111 - reconstruction_loss: 1.2569 - kl_loss: 4.7614 - lr: 3.9063e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2082 - reconstruction_loss: 1.2565 - kl_loss: 4.7631 - lr: 1.9531e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2082 - reconstruction_loss: 1.2560 - kl_loss: 4.7618 - lr: 1.9531e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2088 - reconstruction_loss: 1.2585 - kl_loss: 4.7689 - lr: 1.9531e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2094 - reconstruction_loss: 1.2560 - kl_loss: 4.7726 - lr: 1.9531e-06\n",
      "Epoch 77/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2064 - reconstruction_loss: 1.2558 - kl_loss: 4.7703\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2064 - reconstruction_loss: 1.2546 - kl_loss: 4.7702 - lr: 1.9531e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2122 - reconstruction_loss: 1.2544 - kl_loss: 4.7721 - lr: 1.0000e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2070 - reconstruction_loss: 1.2547 - kl_loss: 4.7689 - lr: 1.0000e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2035 - reconstruction_loss: 1.2537 - kl_loss: 4.7689 - lr: 1.0000e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2123 - reconstruction_loss: 1.2548 - kl_loss: 4.7677 - lr: 1.0000e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2122 - reconstruction_loss: 1.2559 - kl_loss: 4.7672 - lr: 1.0000e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2149 - reconstruction_loss: 1.2559 - kl_loss: 4.7678 - lr: 1.0000e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2053 - reconstruction_loss: 1.2563 - kl_loss: 4.7681 - lr: 1.0000e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2110 - reconstruction_loss: 1.2552 - kl_loss: 4.7645 - lr: 1.0000e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2096 - reconstruction_loss: 1.2556 - kl_loss: 4.7624 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2055 - reconstruction_loss: 1.2562 - kl_loss: 4.7640 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2054 - reconstruction_loss: 1.2550 - kl_loss: 4.7669 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2025 - reconstruction_loss: 1.2527 - kl_loss: 4.7675 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2117 - reconstruction_loss: 1.2539 - kl_loss: 4.7690 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2071 - reconstruction_loss: 1.2529 - kl_loss: 4.7683 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2090 - reconstruction_loss: 1.2532 - kl_loss: 4.7658 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2147 - reconstruction_loss: 1.2541 - kl_loss: 4.7675 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2088 - reconstruction_loss: 1.2543 - kl_loss: 4.7705 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2108 - reconstruction_loss: 1.2522 - kl_loss: 4.7723 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2066 - reconstruction_loss: 1.2526 - kl_loss: 4.7665 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2066 - reconstruction_loss: 1.2536 - kl_loss: 4.7674 - lr: 1.0000e-06\n",
      "Epoch 98/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2021 - reconstruction_loss: 1.2530 - kl_loss: 4.7649 - lr: 1.0000e-06\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2061 - reconstruction_loss: 1.2531 - kl_loss: 4.7638 - lr: 1.0000e-06\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2073 - reconstruction_loss: 1.2523 - kl_loss: 4.7695 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2091 - reconstruction_loss: 1.2528 - kl_loss: 4.7671 - lr: 1.0000e-06\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2109 - reconstruction_loss: 1.2538 - kl_loss: 4.7644 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.1978 - reconstruction_loss: 1.2519 - kl_loss: 4.7648 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2046 - reconstruction_loss: 1.2525 - kl_loss: 4.7638 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2043 - reconstruction_loss: 1.2542 - kl_loss: 4.7648 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2018 - reconstruction_loss: 1.2539 - kl_loss: 4.7640 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2131 - reconstruction_loss: 1.2539 - kl_loss: 4.7678 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2013 - reconstruction_loss: 1.2530 - kl_loss: 4.7621 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2028 - reconstruction_loss: 1.2534 - kl_loss: 4.7640 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2031 - reconstruction_loss: 1.2523 - kl_loss: 4.7610 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2008 - reconstruction_loss: 1.2529 - kl_loss: 4.7591 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2072 - reconstruction_loss: 1.2540 - kl_loss: 4.7630 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2049 - reconstruction_loss: 1.2511 - kl_loss: 4.7638 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2112 - reconstruction_loss: 1.2529 - kl_loss: 4.7637 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2078 - reconstruction_loss: 1.2530 - kl_loss: 4.7622 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2084 - reconstruction_loss: 1.2524 - kl_loss: 4.7680 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2101 - reconstruction_loss: 1.2509 - kl_loss: 4.7682 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2152 - reconstruction_loss: 1.2521 - kl_loss: 4.7692 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2092 - reconstruction_loss: 1.2516 - kl_loss: 4.7702 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2021 - reconstruction_loss: 1.2517 - kl_loss: 4.7701 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2026 - reconstruction_loss: 1.2524 - kl_loss: 4.7691 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2091 - reconstruction_loss: 1.2512 - kl_loss: 4.7686 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2105 - reconstruction_loss: 1.2528 - kl_loss: 4.7687 - lr: 1.0000e-06\n",
      "Epoch 123: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 14:07:03,686]\u001b[0m Trial 54 finished with value: 0.00489289585242944 and parameters: {'encoder_layers': 6, 'encoder_units_l1': 124, 'encoder_units_l2': 609, 'encoder_units_l3': 894, 'encoder_units_l4': 806, 'encoder_units_l5': 28, 'encoder_units_l6': 208, 'decoder_layers': 8, 'decoder_units_l1': 475, 'decoder_units_l2': 351, 'decoder_units_l3': 142, 'decoder_units_l4': 525, 'decoder_units_l5': 108, 'decoder_units_l6': 198, 'decoder_units_l7': 664, 'decoder_units_l8': 184, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 8ms/step - loss: 5.7514 - reconstruction_loss: 3.9504 - kl_loss: 3.4340 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.2445 - reconstruction_loss: 2.3078 - kl_loss: 4.1615 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.9693 - reconstruction_loss: 2.0794 - kl_loss: 4.3070 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.8636 - reconstruction_loss: 1.9308 - kl_loss: 4.4308 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.7965 - reconstruction_loss: 1.8601 - kl_loss: 4.5424 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.6908 - reconstruction_loss: 1.7124 - kl_loss: 4.6851 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.6403 - reconstruction_loss: 1.6830 - kl_loss: 4.8073 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.6367 - reconstruction_loss: 1.6580 - kl_loss: 4.9509 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5556 - reconstruction_loss: 1.5469 - kl_loss: 4.9708 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.6820 - reconstruction_loss: 1.6937 - kl_loss: 4.9553 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5590 - reconstruction_loss: 1.5353 - kl_loss: 4.8958 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4923 - reconstruction_loss: 1.4790 - kl_loss: 4.9228 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4680 - reconstruction_loss: 1.4746 - kl_loss: 4.9904 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5631 - reconstruction_loss: 1.5081 - kl_loss: 5.0373 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.4825 - reconstruction_loss: 1.4861 - kl_loss: 5.0011\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4826 - reconstruction_loss: 1.4860 - kl_loss: 5.0031 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4482 - reconstruction_loss: 1.3764 - kl_loss: 4.9758 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3187 - reconstruction_loss: 1.3322 - kl_loss: 4.9009 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3157 - reconstruction_loss: 1.3305 - kl_loss: 4.8785 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3048 - reconstruction_loss: 1.3350 - kl_loss: 4.8695 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3345 - reconstruction_loss: 1.3475 - kl_loss: 4.9448 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.3289 - reconstruction_loss: 1.3357 - kl_loss: 4.9043\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3287 - reconstruction_loss: 1.3337 - kl_loss: 4.9045 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2719 - reconstruction_loss: 1.2924 - kl_loss: 4.8645 - lr: 2.5000e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2723 - reconstruction_loss: 1.2909 - kl_loss: 4.8861 - lr: 2.5000e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2692 - reconstruction_loss: 1.2874 - kl_loss: 4.8818 - lr: 2.5000e-04\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2572 - reconstruction_loss: 1.2860 - kl_loss: 4.8761 - lr: 2.5000e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2820 - reconstruction_loss: 1.3120 - kl_loss: 4.8879 - lr: 2.5000e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2702 - reconstruction_loss: 1.2967 - kl_loss: 4.8428 - lr: 2.5000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2548 - reconstruction_loss: 1.2876 - kl_loss: 4.8219 - lr: 2.5000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2743 - reconstruction_loss: 1.3056 - kl_loss: 4.8391 - lr: 2.5000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2732 - reconstruction_loss: 1.3106 - kl_loss: 4.8660 - lr: 2.5000e-04\n",
      "Epoch 31/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.3157 - reconstruction_loss: 1.3364 - kl_loss: 4.9116\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3157 - reconstruction_loss: 1.3349 - kl_loss: 4.9113 - lr: 2.5000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2968 - reconstruction_loss: 1.3172 - kl_loss: 4.8996 - lr: 1.2500e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2707 - reconstruction_loss: 1.2948 - kl_loss: 4.8699 - lr: 1.2500e-04\n",
      "Epoch 34/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2588 - reconstruction_loss: 1.2940 - kl_loss: 4.8546\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2588 - reconstruction_loss: 1.2930 - kl_loss: 4.8542 - lr: 1.2500e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2472 - reconstruction_loss: 1.2818 - kl_loss: 4.8366 - lr: 6.2500e-05\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2445 - reconstruction_loss: 1.2734 - kl_loss: 4.8454 - lr: 6.2500e-05\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2383 - reconstruction_loss: 1.2711 - kl_loss: 4.8306 - lr: 6.2500e-05\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2358 - reconstruction_loss: 1.2722 - kl_loss: 4.8308 - lr: 6.2500e-05\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2412 - reconstruction_loss: 1.2702 - kl_loss: 4.8322 - lr: 6.2500e-05\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2347 - reconstruction_loss: 1.2695 - kl_loss: 4.8233 - lr: 6.2500e-05\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2355 - reconstruction_loss: 1.2782 - kl_loss: 4.8269 - lr: 6.2500e-05\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2460 - reconstruction_loss: 1.2691 - kl_loss: 4.8329 - lr: 6.2500e-05\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2310 - reconstruction_loss: 1.2663 - kl_loss: 4.8200 - lr: 6.2500e-05\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2358 - reconstruction_loss: 1.2641 - kl_loss: 4.8323 - lr: 6.2500e-05\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2231 - reconstruction_loss: 1.2641 - kl_loss: 4.8026 - lr: 6.2500e-05\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2240 - reconstruction_loss: 1.2639 - kl_loss: 4.8175 - lr: 6.2500e-05\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2269 - reconstruction_loss: 1.2637 - kl_loss: 4.8199 - lr: 6.2500e-05\n",
      "Epoch 48/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2307 - reconstruction_loss: 1.2670 - kl_loss: 4.8207\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2307 - reconstruction_loss: 1.2653 - kl_loss: 4.8206 - lr: 6.2500e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2340 - reconstruction_loss: 1.2647 - kl_loss: 4.8095 - lr: 3.1250e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2190 - reconstruction_loss: 1.2641 - kl_loss: 4.8017 - lr: 3.1250e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2260 - reconstruction_loss: 1.2639 - kl_loss: 4.8107 - lr: 3.1250e-05\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2214 - reconstruction_loss: 1.2599 - kl_loss: 4.8246 - lr: 3.1250e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2260 - reconstruction_loss: 1.2627 - kl_loss: 4.8116\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2260 - reconstruction_loss: 1.2627 - kl_loss: 4.8116 - lr: 3.1250e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2274 - reconstruction_loss: 1.2619 - kl_loss: 4.8217 - lr: 1.5625e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2236 - reconstruction_loss: 1.2579 - kl_loss: 4.8140 - lr: 1.5625e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2284 - reconstruction_loss: 1.2593 - kl_loss: 4.8230 - lr: 1.5625e-05\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2204 - reconstruction_loss: 1.2603 - kl_loss: 4.8094 - lr: 1.5625e-05\n",
      "Epoch 58/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.2293 - reconstruction_loss: 1.2609 - kl_loss: 4.8192\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2292 - reconstruction_loss: 1.2595 - kl_loss: 4.8198 - lr: 1.5625e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2194 - reconstruction_loss: 1.2572 - kl_loss: 4.8167 - lr: 7.8125e-06\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2224 - reconstruction_loss: 1.2609 - kl_loss: 4.8058 - lr: 7.8125e-06\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2238 - reconstruction_loss: 1.2603 - kl_loss: 4.8107 - lr: 7.8125e-06\n",
      "Epoch 62/300\n",
      "385/391 [============================>.] - ETA: 0s - loss: 2.2188 - reconstruction_loss: 1.2611 - kl_loss: 4.8050\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2188 - reconstruction_loss: 1.2601 - kl_loss: 4.8054 - lr: 7.8125e-06\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2231 - reconstruction_loss: 1.2563 - kl_loss: 4.8117 - lr: 3.9063e-06\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2171 - reconstruction_loss: 1.2567 - kl_loss: 4.8093 - lr: 3.9063e-06\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2222 - reconstruction_loss: 1.2556 - kl_loss: 4.8055 - lr: 3.9063e-06\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2148 - reconstruction_loss: 1.2567 - kl_loss: 4.8093 - lr: 3.9063e-06\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2216 - reconstruction_loss: 1.2577 - kl_loss: 4.8122 - lr: 3.9063e-06\n",
      "Epoch 68/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.2183 - reconstruction_loss: 1.2575 - kl_loss: 4.8193\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2183 - reconstruction_loss: 1.2563 - kl_loss: 4.8191 - lr: 3.9063e-06\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2156 - reconstruction_loss: 1.2567 - kl_loss: 4.8075 - lr: 1.9531e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2075 - reconstruction_loss: 1.2557 - kl_loss: 4.8022 - lr: 1.9531e-06\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2204 - reconstruction_loss: 1.2588 - kl_loss: 4.8052 - lr: 1.9531e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2215 - reconstruction_loss: 1.2566 - kl_loss: 4.8067 - lr: 1.9531e-06\n",
      "Epoch 73/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 2.2203 - reconstruction_loss: 1.2572 - kl_loss: 4.8129\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2203 - reconstruction_loss: 1.2558 - kl_loss: 4.8129 - lr: 1.9531e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2180 - reconstruction_loss: 1.2539 - kl_loss: 4.8136 - lr: 1.0000e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2145 - reconstruction_loss: 1.2533 - kl_loss: 4.8109 - lr: 1.0000e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2175 - reconstruction_loss: 1.2537 - kl_loss: 4.8080 - lr: 1.0000e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2181 - reconstruction_loss: 1.2533 - kl_loss: 4.8078 - lr: 1.0000e-06\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2189 - reconstruction_loss: 1.2544 - kl_loss: 4.8087 - lr: 1.0000e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2124 - reconstruction_loss: 1.2528 - kl_loss: 4.8086 - lr: 1.0000e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2231 - reconstruction_loss: 1.2550 - kl_loss: 4.8126 - lr: 1.0000e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2093 - reconstruction_loss: 1.2525 - kl_loss: 4.8110 - lr: 1.0000e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2197 - reconstruction_loss: 1.2538 - kl_loss: 4.8097 - lr: 1.0000e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2155 - reconstruction_loss: 1.2526 - kl_loss: 4.8099 - lr: 1.0000e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2144 - reconstruction_loss: 1.2527 - kl_loss: 4.8099 - lr: 1.0000e-06\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2174 - reconstruction_loss: 1.2525 - kl_loss: 4.8108 - lr: 1.0000e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2117 - reconstruction_loss: 1.2524 - kl_loss: 4.8117 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2149 - reconstruction_loss: 1.2532 - kl_loss: 4.8135 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2146 - reconstruction_loss: 1.2528 - kl_loss: 4.8105 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2172 - reconstruction_loss: 1.2530 - kl_loss: 4.8088 - lr: 1.0000e-06\n",
      "Epoch 89: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 14:11:50,334]\u001b[0m Trial 55 finished with value: 0.004896403141849065 and parameters: {'encoder_layers': 5, 'encoder_units_l1': 102, 'encoder_units_l2': 354, 'encoder_units_l3': 458, 'encoder_units_l4': 632, 'encoder_units_l5': 52, 'decoder_layers': 3, 'decoder_units_l1': 753, 'decoder_units_l2': 129, 'decoder_units_l3': 616, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 11.4185 - reconstruction_loss: 10.9457 - kl_loss: 0.0010 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8518 - reconstruction_loss: 10.8462 - kl_loss: 3.5890e-08 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8430 - reconstruction_loss: 10.8468 - kl_loss: 2.7578e-08 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8842 - reconstruction_loss: 10.8472 - kl_loss: 2.9950e-08 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 10.8728 - reconstruction_loss: 10.8577 - kl_loss: 5.7910e-08\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8725 - reconstruction_loss: 10.8469 - kl_loss: 5.7323e-08 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8468 - reconstruction_loss: 10.8449 - kl_loss: 2.0008e-10 - lr: 0.0050\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8607 - reconstruction_loss: 10.8446 - kl_loss: 2.6499e-11 - lr: 0.0050\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8508 - reconstruction_loss: 10.8445 - kl_loss: 1.5780e-11 - lr: 0.0050\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8351 - reconstruction_loss: 10.8447 - kl_loss: 3.2146e-08 - lr: 0.0050\n",
      "Epoch 10/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 10.8719 - reconstruction_loss: 10.8549 - kl_loss: 6.3502e-08\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8718 - reconstruction_loss: 10.8449 - kl_loss: 6.3340e-08 - lr: 0.0050\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8366 - reconstruction_loss: 10.8437 - kl_loss: 2.7987e-11 - lr: 0.0025\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8321 - reconstruction_loss: 10.8439 - kl_loss: 1.1314e-11 - lr: 0.0025\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8643 - reconstruction_loss: 10.8440 - kl_loss: 8.3366e-12 - lr: 0.0025\n",
      "Epoch 14/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 10.8593 - reconstruction_loss: 10.8547 - kl_loss: 1.5156e-08\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8592 - reconstruction_loss: 10.8438 - kl_loss: 1.5079e-08 - lr: 0.0025\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8708 - reconstruction_loss: 10.8432 - kl_loss: 1.0123e-11 - lr: 0.0012\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8534 - reconstruction_loss: 10.8432 - kl_loss: 7.4434e-12 - lr: 0.0012\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8387 - reconstruction_loss: 10.8433 - kl_loss: 3.8706e-12 - lr: 0.0012\n",
      "Epoch 18/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 10.8411 - reconstruction_loss: 10.8525 - kl_loss: 4.1572e-09\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8412 - reconstruction_loss: 10.8433 - kl_loss: 4.1040e-09 - lr: 0.0012\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8883 - reconstruction_loss: 10.8431 - kl_loss: 6.5502e-12 - lr: 6.2500e-04\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8383 - reconstruction_loss: 10.8431 - kl_loss: 3.2751e-12 - lr: 6.2500e-04\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8182 - reconstruction_loss: 10.8431 - kl_loss: 2.9774e-12 - lr: 6.2500e-04\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8449 - reconstruction_loss: 10.8430 - kl_loss: 9.1614e-10 - lr: 6.2500e-04\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8746 - reconstruction_loss: 10.8431 - kl_loss: 1.6462e-09 - lr: 6.2500e-04\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8679 - reconstruction_loss: 10.8430 - kl_loss: 7.8335e-10 - lr: 6.2500e-04\n",
      "Epoch 25/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 10.8669 - reconstruction_loss: 10.8545 - kl_loss: 2.7041e-09\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8668 - reconstruction_loss: 10.8430 - kl_loss: 2.6972e-09 - lr: 6.2500e-04\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8681 - reconstruction_loss: 10.8429 - kl_loss: 2.0842e-12 - lr: 3.1250e-04\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8576 - reconstruction_loss: 10.8429 - kl_loss: 2.0842e-12 - lr: 3.1250e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8225 - reconstruction_loss: 10.8429 - kl_loss: 2.0842e-12 - lr: 3.1250e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 10.8353 - reconstruction_loss: 10.8429 - kl_loss: 2.3819e-12\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8353 - reconstruction_loss: 10.8429 - kl_loss: 2.3819e-12 - lr: 3.1250e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8634 - reconstruction_loss: 10.8428 - kl_loss: 1.4887e-12 - lr: 1.5625e-04\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8215 - reconstruction_loss: 10.8428 - kl_loss: 1.4887e-12 - lr: 1.5625e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8391 - reconstruction_loss: 10.8428 - kl_loss: 8.9321e-13 - lr: 1.5625e-04\n",
      "Epoch 33/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 10.8553 - reconstruction_loss: 10.8527 - kl_loss: 8.9550e-13\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8553 - reconstruction_loss: 10.8428 - kl_loss: 8.9321e-13 - lr: 1.5625e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8469 - reconstruction_loss: 10.8428 - kl_loss: 8.9321e-13 - lr: 7.8125e-05\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8475 - reconstruction_loss: 10.8428 - kl_loss: 8.9321e-13 - lr: 7.8125e-05\n",
      "Epoch 36/300\n",
      "390/391 [============================>.] - ETA: 0s - loss: 10.8401 - reconstruction_loss: 10.8532 - kl_loss: 8.9550e-13\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8401 - reconstruction_loss: 10.8428 - kl_loss: 8.9321e-13 - lr: 7.8125e-05\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8438 - reconstruction_loss: 10.8428 - kl_loss: 8.9321e-13 - lr: 3.9062e-05\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8559 - reconstruction_loss: 10.8428 - kl_loss: 8.9321e-13 - lr: 3.9062e-05\n",
      "Epoch 39/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 10.8559 - reconstruction_loss: 10.8535 - kl_loss: 9.0244e-13\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8559 - reconstruction_loss: 10.8428 - kl_loss: 8.9321e-13 - lr: 3.9062e-05\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 10.8652 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.9531e-05\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8316 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.9531e-05\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 10.8552 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8551 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.9531e-05\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8551 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 9.7656e-06\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8201 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 9.7656e-06\n",
      "Epoch 45/300\n",
      "386/391 [============================>.] - ETA: 0s - loss: 10.8709 - reconstruction_loss: 10.8520 - kl_loss: 9.0478e-13\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8706 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 9.7656e-06\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8508 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 4.8828e-06\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8193 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 4.8828e-06\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 10.8688 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8687 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 4.8828e-06\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8515 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 2.4414e-06\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8667 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 2.4414e-06\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 10.8606 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8605 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 2.4414e-06\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8459 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.2207e-06\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8522 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.2207e-06\n",
      "Epoch 54/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 10.8897 - reconstruction_loss: 10.8524 - kl_loss: 8.9780e-13\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8894 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.2207e-06\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8657 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.0000e-06\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8864 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.0000e-06\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8609 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.0000e-06\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8674 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.0000e-06\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8452 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.0000e-06\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8696 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.0000e-06\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8753 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.0000e-06\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8477 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.0000e-06\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 10.8822 - reconstruction_loss: 10.8427 - kl_loss: 8.9321e-13 - lr: 1.0000e-06\n",
      "Epoch 63: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 14:15:48,465]\u001b[0m Trial 56 finished with value: 0.04239506861461306 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 90, 'encoder_units_l2': 190, 'encoder_units_l3': 671, 'encoder_units_l4': 231, 'decoder_layers': 8, 'decoder_units_l1': 373, 'decoder_units_l2': 436, 'decoder_units_l3': 308, 'decoder_units_l4': 224, 'decoder_units_l5': 76, 'decoder_units_l6': 301, 'decoder_units_l7': 17, 'decoder_units_l8': 285, 'beta': 5, 'lr': 0.01, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 3.2605 - reconstruction_loss: 2.2019 - kl_loss: 2.5505 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 2.0324 - reconstruction_loss: 1.2907 - kl_loss: 3.3989 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.9096 - reconstruction_loss: 1.1894 - kl_loss: 3.5187 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.8445 - reconstruction_loss: 1.1092 - kl_loss: 3.6243 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.8072 - reconstruction_loss: 1.0343 - kl_loss: 3.7825 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7668 - reconstruction_loss: 0.9694 - kl_loss: 3.9034 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7137 - reconstruction_loss: 0.9230 - kl_loss: 3.9359 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7061 - reconstruction_loss: 0.9013 - kl_loss: 3.9846 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6865 - reconstruction_loss: 0.8794 - kl_loss: 4.0172 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6813 - reconstruction_loss: 0.8694 - kl_loss: 4.0245 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6961 - reconstruction_loss: 0.8701 - kl_loss: 4.0430 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6560 - reconstruction_loss: 0.8443 - kl_loss: 4.0509 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6754 - reconstruction_loss: 0.8758 - kl_loss: 4.0826 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6519 - reconstruction_loss: 0.8414 - kl_loss: 4.0571 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6489 - reconstruction_loss: 0.8359 - kl_loss: 4.0804 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6669 - reconstruction_loss: 0.8434 - kl_loss: 4.0608 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6845 - reconstruction_loss: 0.8931 - kl_loss: 4.1862 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "777/782 [============================>.] - ETA: 0s - loss: 1.7399 - reconstruction_loss: 0.8887 - kl_loss: 4.1872\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7398 - reconstruction_loss: 0.8879 - kl_loss: 4.1880 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6470 - reconstruction_loss: 0.8065 - kl_loss: 4.1047 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6203 - reconstruction_loss: 0.8005 - kl_loss: 4.0950 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6159 - reconstruction_loss: 0.7982 - kl_loss: 4.0852 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6254 - reconstruction_loss: 0.8066 - kl_loss: 4.0780 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6083 - reconstruction_loss: 0.7916 - kl_loss: 4.0770 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6177 - reconstruction_loss: 0.8077 - kl_loss: 4.0754 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6110 - reconstruction_loss: 0.7961 - kl_loss: 4.0840 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6080 - reconstruction_loss: 0.7915 - kl_loss: 4.0669 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6008 - reconstruction_loss: 0.7849 - kl_loss: 4.0689 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5982 - reconstruction_loss: 0.7860 - kl_loss: 4.0544 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6144 - reconstruction_loss: 0.7941 - kl_loss: 4.0745 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5853 - reconstruction_loss: 0.7726 - kl_loss: 4.0492 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5872 - reconstruction_loss: 0.7759 - kl_loss: 4.0540 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5763 - reconstruction_loss: 0.7735 - kl_loss: 4.0343 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5865 - reconstruction_loss: 0.7777 - kl_loss: 4.0595 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5875 - reconstruction_loss: 0.7803 - kl_loss: 4.0503 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "777/782 [============================>.] - ETA: 0s - loss: 1.5971 - reconstruction_loss: 0.7816 - kl_loss: 4.0653\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5971 - reconstruction_loss: 0.7806 - kl_loss: 4.0650 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5783 - reconstruction_loss: 0.7674 - kl_loss: 4.0412 - lr: 2.5000e-04\n",
      "Epoch 37/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5746 - reconstruction_loss: 0.7601 - kl_loss: 4.0603 - lr: 2.5000e-04\n",
      "Epoch 38/300\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.5687 - reconstruction_loss: 0.7576 - kl_loss: 4.0504 - lr: 2.5000e-04\n",
      "Epoch 39/300\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.5646 - reconstruction_loss: 0.7560 - kl_loss: 4.0344 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.5629 - reconstruction_loss: 0.7579 - kl_loss: 4.0363 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.5632 - reconstruction_loss: 0.7572 - kl_loss: 4.0428 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "777/782 [============================>.] - ETA: 0s - loss: 1.5663 - reconstruction_loss: 0.7578 - kl_loss: 4.0463\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5663 - reconstruction_loss: 0.7573 - kl_loss: 4.0465 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.5614 - reconstruction_loss: 0.7521 - kl_loss: 4.0415 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5607 - reconstruction_loss: 0.7495 - kl_loss: 4.0400 - lr: 1.2500e-04\n",
      "Epoch 45/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5571 - reconstruction_loss: 0.7480 - kl_loss: 4.0336 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.5563 - reconstruction_loss: 0.7472 - kl_loss: 4.0424 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5548 - reconstruction_loss: 0.7475 - kl_loss: 4.0346 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5587 - reconstruction_loss: 0.7478 - kl_loss: 4.0405 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.5527 - reconstruction_loss: 0.7456 - kl_loss: 4.0367 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5505 - reconstruction_loss: 0.7447 - kl_loss: 4.0429 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5528 - reconstruction_loss: 0.7435 - kl_loss: 4.0429 - lr: 1.2500e-04\n",
      "Epoch 52/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5509 - reconstruction_loss: 0.7443 - kl_loss: 4.0312 - lr: 1.2500e-04\n",
      "Epoch 53/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5581 - reconstruction_loss: 0.7461 - kl_loss: 4.0321 - lr: 1.2500e-04\n",
      "Epoch 54/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5490 - reconstruction_loss: 0.7409 - kl_loss: 4.0335 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5505 - reconstruction_loss: 0.7407 - kl_loss: 4.0499 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5488 - reconstruction_loss: 0.7408 - kl_loss: 4.0245 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5482 - reconstruction_loss: 0.7414 - kl_loss: 4.0322 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5425 - reconstruction_loss: 0.7405 - kl_loss: 4.0290 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.5485 - reconstruction_loss: 0.7441 - kl_loss: 4.0318\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.5485 - reconstruction_loss: 0.7435 - kl_loss: 4.0321 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5453 - reconstruction_loss: 0.7380 - kl_loss: 4.0282 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5470 - reconstruction_loss: 0.7411 - kl_loss: 4.0396 - lr: 6.2500e-05\n",
      "Epoch 62/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5467 - reconstruction_loss: 0.7399 - kl_loss: 4.0298 - lr: 6.2500e-05\n",
      "Epoch 63/300\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.5443 - reconstruction_loss: 0.7406 - kl_loss: 4.0210\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5443 - reconstruction_loss: 0.7398 - kl_loss: 4.0206 - lr: 6.2500e-05\n",
      "Epoch 64/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5452 - reconstruction_loss: 0.7363 - kl_loss: 4.0296 - lr: 3.1250e-05\n",
      "Epoch 65/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5415 - reconstruction_loss: 0.7352 - kl_loss: 4.0256 - lr: 3.1250e-05\n",
      "Epoch 66/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5480 - reconstruction_loss: 0.7385 - kl_loss: 4.0333 - lr: 3.1250e-05\n",
      "Epoch 67/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5427 - reconstruction_loss: 0.7362 - kl_loss: 4.0281 - lr: 3.1250e-05\n",
      "Epoch 68/300\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.5406 - reconstruction_loss: 0.7368 - kl_loss: 4.0307\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5406 - reconstruction_loss: 0.7361 - kl_loss: 4.0308 - lr: 3.1250e-05\n",
      "Epoch 69/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5455 - reconstruction_loss: 0.7352 - kl_loss: 4.0289 - lr: 1.5625e-05\n",
      "Epoch 70/300\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.5368 - reconstruction_loss: 0.7351 - kl_loss: 4.0189 - lr: 1.5625e-05\n",
      "Epoch 71/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5387 - reconstruction_loss: 0.7337 - kl_loss: 4.0236 - lr: 1.5625e-05\n",
      "Epoch 72/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5427 - reconstruction_loss: 0.7353 - kl_loss: 4.0189 - lr: 1.5625e-05\n",
      "Epoch 73/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5443 - reconstruction_loss: 0.7343 - kl_loss: 4.0355 - lr: 1.5625e-05\n",
      "Epoch 74/300\n",
      "776/782 [============================>.] - ETA: 0s - loss: 1.5379 - reconstruction_loss: 0.7359 - kl_loss: 4.0249\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5380 - reconstruction_loss: 0.7350 - kl_loss: 4.0248 - lr: 1.5625e-05\n",
      "Epoch 75/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5375 - reconstruction_loss: 0.7329 - kl_loss: 4.0246 - lr: 7.8125e-06\n",
      "Epoch 76/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5376 - reconstruction_loss: 0.7337 - kl_loss: 4.0193 - lr: 7.8125e-06\n",
      "Epoch 77/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5380 - reconstruction_loss: 0.7340 - kl_loss: 4.0205 - lr: 7.8125e-06\n",
      "Epoch 78/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5368 - reconstruction_loss: 0.7339 - kl_loss: 4.0222 - lr: 7.8125e-06\n",
      "Epoch 79/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5358 - reconstruction_loss: 0.7319 - kl_loss: 4.0267 - lr: 7.8125e-06\n",
      "Epoch 80/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5362 - reconstruction_loss: 0.7333 - kl_loss: 4.0203 - lr: 7.8125e-06\n",
      "Epoch 81/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5378 - reconstruction_loss: 0.7325 - kl_loss: 4.0250 - lr: 7.8125e-06\n",
      "Epoch 82/300\n",
      "777/782 [============================>.] - ETA: 0s - loss: 1.5385 - reconstruction_loss: 0.7327 - kl_loss: 4.0324\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5385 - reconstruction_loss: 0.7318 - kl_loss: 4.0328 - lr: 7.8125e-06\n",
      "Epoch 83/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5340 - reconstruction_loss: 0.7314 - kl_loss: 4.0286 - lr: 3.9063e-06\n",
      "Epoch 84/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5395 - reconstruction_loss: 0.7311 - kl_loss: 4.0293 - lr: 3.9063e-06\n",
      "Epoch 85/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5398 - reconstruction_loss: 0.7310 - kl_loss: 4.0320 - lr: 3.9063e-06\n",
      "Epoch 86/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5396 - reconstruction_loss: 0.7309 - kl_loss: 4.0289 - lr: 3.9063e-06\n",
      "Epoch 87/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5427 - reconstruction_loss: 0.7315 - kl_loss: 4.0306 - lr: 3.9063e-06\n",
      "Epoch 88/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5373 - reconstruction_loss: 0.7301 - kl_loss: 4.0279 - lr: 3.9063e-06\n",
      "Epoch 89/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5397 - reconstruction_loss: 0.7318 - kl_loss: 4.0277 - lr: 3.9063e-06\n",
      "Epoch 90/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5379 - reconstruction_loss: 0.7313 - kl_loss: 4.0249 - lr: 3.9063e-06\n",
      "Epoch 91/300\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.5373 - reconstruction_loss: 0.7318 - kl_loss: 4.0225\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5373 - reconstruction_loss: 0.7312 - kl_loss: 4.0229 - lr: 3.9063e-06\n",
      "Epoch 92/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5381 - reconstruction_loss: 0.7325 - kl_loss: 4.0242 - lr: 1.9531e-06\n",
      "Epoch 93/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5393 - reconstruction_loss: 0.7302 - kl_loss: 4.0280 - lr: 1.9531e-06\n",
      "Epoch 94/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5340 - reconstruction_loss: 0.7301 - kl_loss: 4.0257 - lr: 1.9531e-06\n",
      "Epoch 95/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5347 - reconstruction_loss: 0.7303 - kl_loss: 4.0259 - lr: 1.9531e-06\n",
      "Epoch 96/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5367 - reconstruction_loss: 0.7311 - kl_loss: 4.0290 - lr: 1.9531e-06\n",
      "Epoch 97/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5382 - reconstruction_loss: 0.7283 - kl_loss: 4.0289 - lr: 1.9531e-06\n",
      "Epoch 98/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5345 - reconstruction_loss: 0.7305 - kl_loss: 4.0205 - lr: 1.9531e-06\n",
      "Epoch 99/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5355 - reconstruction_loss: 0.7302 - kl_loss: 4.0215 - lr: 1.9531e-06\n",
      "Epoch 100/300\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.5347 - reconstruction_loss: 0.7314 - kl_loss: 4.0230\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5347 - reconstruction_loss: 0.7314 - kl_loss: 4.0230 - lr: 1.9531e-06\n",
      "Epoch 101/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5393 - reconstruction_loss: 0.7298 - kl_loss: 4.0248 - lr: 1.0000e-06\n",
      "Epoch 102/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5367 - reconstruction_loss: 0.7309 - kl_loss: 4.0235 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5401 - reconstruction_loss: 0.7314 - kl_loss: 4.0255 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5349 - reconstruction_loss: 0.7303 - kl_loss: 4.0271 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5331 - reconstruction_loss: 0.7299 - kl_loss: 4.0263 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5360 - reconstruction_loss: 0.7310 - kl_loss: 4.0269 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5378 - reconstruction_loss: 0.7312 - kl_loss: 4.0288 - lr: 1.0000e-06\n",
      "Epoch 107: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 14:26:31,568]\u001b[0m Trial 57 finished with value: 0.005707565696702038 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 165, 'encoder_units_l2': 243, 'encoder_units_l3': 1017, 'decoder_layers': 7, 'decoder_units_l1': 991, 'decoder_units_l2': 220, 'decoder_units_l3': 190, 'decoder_units_l4': 141, 'decoder_units_l5': 178, 'decoder_units_l6': 246, 'decoder_units_l7': 28, 'beta': 0.2, 'lr': 0.001, 'batch_size': 128}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "391/391 [==============================] - 5s 10ms/step - loss: 6.6484 - reconstruction_loss: 4.7849 - kl_loss: 2.8818 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 3.8037 - reconstruction_loss: 2.6337 - kl_loss: 4.1028 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.9377 - reconstruction_loss: 2.0221 - kl_loss: 4.4278 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.8012 - reconstruction_loss: 1.8566 - kl_loss: 4.6573 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6668 - reconstruction_loss: 1.6846 - kl_loss: 4.8210 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.6373 - reconstruction_loss: 1.6588 - kl_loss: 4.9329 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5812 - reconstruction_loss: 1.5726 - kl_loss: 4.9570 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5156 - reconstruction_loss: 1.5137 - kl_loss: 4.9731 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5258 - reconstruction_loss: 1.5567 - kl_loss: 5.0462 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5415 - reconstruction_loss: 1.5291 - kl_loss: 4.9850 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4614 - reconstruction_loss: 1.5029 - kl_loss: 4.9910 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4832 - reconstruction_loss: 1.5031 - kl_loss: 5.0445 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4700 - reconstruction_loss: 1.4834 - kl_loss: 5.0213 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5395 - reconstruction_loss: 1.4936 - kl_loss: 5.0627 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4516 - reconstruction_loss: 1.4754 - kl_loss: 5.0349 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4311 - reconstruction_loss: 1.4370 - kl_loss: 5.0467 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5332 - reconstruction_loss: 1.5182 - kl_loss: 5.1276 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5560 - reconstruction_loss: 1.5037 - kl_loss: 5.1452 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4313 - reconstruction_loss: 1.4057 - kl_loss: 5.0108 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3870 - reconstruction_loss: 1.3946 - kl_loss: 4.9860 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4336 - reconstruction_loss: 1.3879 - kl_loss: 5.0042 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.4182 - reconstruction_loss: 1.4201 - kl_loss: 4.9657 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3723 - reconstruction_loss: 1.3669 - kl_loss: 4.9771 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4237 - reconstruction_loss: 1.4026 - kl_loss: 5.0507 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.4656 - reconstruction_loss: 1.4300 - kl_loss: 5.0138 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.5774 - reconstruction_loss: 1.4654 - kl_loss: 4.9924\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.5762 - reconstruction_loss: 1.4633 - kl_loss: 4.9916 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3017 - reconstruction_loss: 1.3095 - kl_loss: 4.9065 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2813 - reconstruction_loss: 1.2973 - kl_loss: 4.9132 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2778 - reconstruction_loss: 1.3020 - kl_loss: 4.9068 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.3022 - reconstruction_loss: 1.3037 - kl_loss: 4.9165 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.2875 - reconstruction_loss: 1.3002 - kl_loss: 4.9070\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2874 - reconstruction_loss: 1.2992 - kl_loss: 4.9065 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2546 - reconstruction_loss: 1.2785 - kl_loss: 4.8808 - lr: 2.5000e-04\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2444 - reconstruction_loss: 1.2779 - kl_loss: 4.8737 - lr: 2.5000e-04\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2734 - reconstruction_loss: 1.2807 - kl_loss: 4.9295 - lr: 2.5000e-04\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2616 - reconstruction_loss: 1.2801 - kl_loss: 4.9021 - lr: 2.5000e-04\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2934 - reconstruction_loss: 1.2866 - kl_loss: 4.9558\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2933 - reconstruction_loss: 1.2866 - kl_loss: 4.9558 - lr: 2.5000e-04\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2583 - reconstruction_loss: 1.2711 - kl_loss: 4.9206 - lr: 1.2500e-04\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2480 - reconstruction_loss: 1.2672 - kl_loss: 4.9147 - lr: 1.2500e-04\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2503 - reconstruction_loss: 1.2690 - kl_loss: 4.9004 - lr: 1.2500e-04\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2405 - reconstruction_loss: 1.2664 - kl_loss: 4.8822 - lr: 1.2500e-04\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2412 - reconstruction_loss: 1.2630 - kl_loss: 4.8726 - lr: 1.2500e-04\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2367 - reconstruction_loss: 1.2607 - kl_loss: 4.8743 - lr: 1.2500e-04\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2453 - reconstruction_loss: 1.2653 - kl_loss: 4.8872 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2423 - reconstruction_loss: 1.2670 - kl_loss: 4.8755 - lr: 1.2500e-04\n",
      "Epoch 45/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2425 - reconstruction_loss: 1.2650 - kl_loss: 4.8686\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2424 - reconstruction_loss: 1.2639 - kl_loss: 4.8686 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2327 - reconstruction_loss: 1.2605 - kl_loss: 4.8676 - lr: 6.2500e-05\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2287 - reconstruction_loss: 1.2579 - kl_loss: 4.8605 - lr: 6.2500e-05\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2253 - reconstruction_loss: 1.2541 - kl_loss: 4.8531 - lr: 6.2500e-05\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2395 - reconstruction_loss: 1.2553 - kl_loss: 4.8575 - lr: 6.2500e-05\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2227 - reconstruction_loss: 1.2553 - kl_loss: 4.8506 - lr: 6.2500e-05\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2219 - reconstruction_loss: 1.2536 - kl_loss: 4.8450 - lr: 6.2500e-05\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2218 - reconstruction_loss: 1.2513 - kl_loss: 4.8572 - lr: 6.2500e-05\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2279 - reconstruction_loss: 1.2511 - kl_loss: 4.8562 - lr: 6.2500e-05\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2217 - reconstruction_loss: 1.2521 - kl_loss: 4.8430 - lr: 6.2500e-05\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2217 - reconstruction_loss: 1.2502 - kl_loss: 4.8366 - lr: 6.2500e-05\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2243 - reconstruction_loss: 1.2517 - kl_loss: 4.8388 - lr: 6.2500e-05\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2269 - reconstruction_loss: 1.2528 - kl_loss: 4.8438 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.2186 - reconstruction_loss: 1.2532 - kl_loss: 4.8475\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2186 - reconstruction_loss: 1.2516 - kl_loss: 4.8475 - lr: 6.2500e-05\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2189 - reconstruction_loss: 1.2508 - kl_loss: 4.8400 - lr: 3.1250e-05\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2233 - reconstruction_loss: 1.2526 - kl_loss: 4.8344 - lr: 3.1250e-05\n",
      "Epoch 61/300\n",
      "387/391 [============================>.] - ETA: 0s - loss: 2.2251 - reconstruction_loss: 1.2517 - kl_loss: 4.8367\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2250 - reconstruction_loss: 1.2503 - kl_loss: 4.8367 - lr: 3.1250e-05\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2246 - reconstruction_loss: 1.2501 - kl_loss: 4.8286 - lr: 1.5625e-05\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2173 - reconstruction_loss: 1.2507 - kl_loss: 4.8251 - lr: 1.5625e-05\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2176 - reconstruction_loss: 1.2516 - kl_loss: 4.8331 - lr: 1.5625e-05\n",
      "Epoch 65/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2185 - reconstruction_loss: 1.2506 - kl_loss: 4.8365\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2184 - reconstruction_loss: 1.2497 - kl_loss: 4.8367 - lr: 1.5625e-05\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2108 - reconstruction_loss: 1.2461 - kl_loss: 4.8436 - lr: 7.8125e-06\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2211 - reconstruction_loss: 1.2477 - kl_loss: 4.8391 - lr: 7.8125e-06\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2095 - reconstruction_loss: 1.2472 - kl_loss: 4.8298 - lr: 7.8125e-06\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2164 - reconstruction_loss: 1.2476 - kl_loss: 4.8283 - lr: 7.8125e-06\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2141 - reconstruction_loss: 1.2474 - kl_loss: 4.8332 - lr: 7.8125e-06\n",
      "Epoch 71/300\n",
      "388/391 [============================>.] - ETA: 0s - loss: 2.2182 - reconstruction_loss: 1.2493 - kl_loss: 4.8377\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2181 - reconstruction_loss: 1.2479 - kl_loss: 4.8378 - lr: 7.8125e-06\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2158 - reconstruction_loss: 1.2457 - kl_loss: 4.8310 - lr: 3.9063e-06\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2151 - reconstruction_loss: 1.2470 - kl_loss: 4.8264 - lr: 3.9063e-06\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2119 - reconstruction_loss: 1.2482 - kl_loss: 4.8292 - lr: 3.9063e-06\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2160 - reconstruction_loss: 1.2456 - kl_loss: 4.8302 - lr: 3.9063e-06\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2141 - reconstruction_loss: 1.2472 - kl_loss: 4.8300 - lr: 3.9063e-06\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2117 - reconstruction_loss: 1.2464 - kl_loss: 4.8286 - lr: 3.9063e-06\n",
      "Epoch 78/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2122 - reconstruction_loss: 1.2479 - kl_loss: 4.8300\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.2122 - reconstruction_loss: 1.2464 - kl_loss: 4.8301 - lr: 3.9063e-06\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2054 - reconstruction_loss: 1.2456 - kl_loss: 4.8366 - lr: 1.9531e-06\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2097 - reconstruction_loss: 1.2450 - kl_loss: 4.8360 - lr: 1.9531e-06\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2184 - reconstruction_loss: 1.2438 - kl_loss: 4.8347 - lr: 1.9531e-06\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2079 - reconstruction_loss: 1.2436 - kl_loss: 4.8335 - lr: 1.9531e-06\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2097 - reconstruction_loss: 1.2450 - kl_loss: 4.8272 - lr: 1.9531e-06\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2135 - reconstruction_loss: 1.2446 - kl_loss: 4.8318 - lr: 1.9531e-06\n",
      "Epoch 85/300\n",
      "389/391 [============================>.] - ETA: 0s - loss: 2.2060 - reconstruction_loss: 1.2455 - kl_loss: 4.8355\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2060 - reconstruction_loss: 1.2441 - kl_loss: 4.8354 - lr: 1.9531e-06\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2103 - reconstruction_loss: 1.2429 - kl_loss: 4.8338 - lr: 1.0000e-06\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2099 - reconstruction_loss: 1.2431 - kl_loss: 4.8334 - lr: 1.0000e-06\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2126 - reconstruction_loss: 1.2430 - kl_loss: 4.8343 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2072 - reconstruction_loss: 1.2432 - kl_loss: 4.8363 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2089 - reconstruction_loss: 1.2413 - kl_loss: 4.8350 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2066 - reconstruction_loss: 1.2412 - kl_loss: 4.8332 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2144 - reconstruction_loss: 1.2432 - kl_loss: 4.8293 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2121 - reconstruction_loss: 1.2424 - kl_loss: 4.8289 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2071 - reconstruction_loss: 1.2422 - kl_loss: 4.8284 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2130 - reconstruction_loss: 1.2435 - kl_loss: 4.8290 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2112 - reconstruction_loss: 1.2418 - kl_loss: 4.8306 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2033 - reconstruction_loss: 1.2419 - kl_loss: 4.8277 - lr: 1.0000e-06\n",
      "Epoch 98/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 4s 11ms/step - loss: 2.2109 - reconstruction_loss: 1.2427 - kl_loss: 4.8291 - lr: 1.0000e-06\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2154 - reconstruction_loss: 1.2431 - kl_loss: 4.8302 - lr: 1.0000e-06\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2097 - reconstruction_loss: 1.2436 - kl_loss: 4.8301 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2096 - reconstruction_loss: 1.2447 - kl_loss: 4.8296 - lr: 1.0000e-06\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2092 - reconstruction_loss: 1.2428 - kl_loss: 4.8296 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2114 - reconstruction_loss: 1.2435 - kl_loss: 4.8309 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2109 - reconstruction_loss: 1.2430 - kl_loss: 4.8318 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2093 - reconstruction_loss: 1.2412 - kl_loss: 4.8317 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2137 - reconstruction_loss: 1.2429 - kl_loss: 4.8305 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.2141 - reconstruction_loss: 1.2435 - kl_loss: 4.8305 - lr: 1.0000e-06\n",
      "Epoch 107: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 14:33:48,122]\u001b[0m Trial 58 finished with value: 0.0048590648167104455 and parameters: {'encoder_layers': 4, 'encoder_units_l1': 246, 'encoder_units_l2': 157, 'encoder_units_l3': 21, 'encoder_units_l4': 494, 'decoder_layers': 7, 'decoder_units_l1': 569, 'decoder_units_l2': 541, 'decoder_units_l3': 264, 'decoder_units_l4': 633, 'decoder_units_l5': 291, 'decoder_units_l6': 446, 'decoder_units_l7': 38, 'beta': 0.2, 'lr': 0.001, 'batch_size': 256}. Best is trial 15 with value: 0.004807672164569487.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 7ms/step - loss: 12.5522 - reconstruction_loss: 8.7007 - kl_loss: 3.9314 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.7517 - reconstruction_loss: 6.3886 - kl_loss: 4.6036 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 6.1565 - reconstruction_loss: 4.8360 - kl_loss: 5.1882 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.3877 - reconstruction_loss: 4.2609 - kl_loss: 5.3487 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.1362 - reconstruction_loss: 3.9450 - kl_loss: 5.4659 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.9337 - reconstruction_loss: 3.7710 - kl_loss: 5.5626 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.7910 - reconstruction_loss: 3.6607 - kl_loss: 5.6087 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.7455 - reconstruction_loss: 3.5593 - kl_loss: 5.5911 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.6277 - reconstruction_loss: 3.4639 - kl_loss: 5.6060 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.5572 - reconstruction_loss: 3.4058 - kl_loss: 5.5975 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.5281 - reconstruction_loss: 3.3514 - kl_loss: 5.6051 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.4507 - reconstruction_loss: 3.2814 - kl_loss: 5.5948 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.3111 - reconstruction_loss: 3.1647 - kl_loss: 5.6230 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.2213 - reconstruction_loss: 3.0760 - kl_loss: 5.6664 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.1883 - reconstruction_loss: 3.0159 - kl_loss: 5.6643 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.1757 - reconstruction_loss: 2.9933 - kl_loss: 5.6868 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.1286 - reconstruction_loss: 2.9797 - kl_loss: 5.6991 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0291 - reconstruction_loss: 2.9194 - kl_loss: 5.7140 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9473 - reconstruction_loss: 2.8085 - kl_loss: 5.7295 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9575 - reconstruction_loss: 2.7876 - kl_loss: 5.7602 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9147 - reconstruction_loss: 2.7410 - kl_loss: 5.7277 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9068 - reconstruction_loss: 2.7210 - kl_loss: 5.7187 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8176 - reconstruction_loss: 2.6491 - kl_loss: 5.7134 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8059 - reconstruction_loss: 2.6554 - kl_loss: 5.6795 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8097 - reconstruction_loss: 2.6606 - kl_loss: 5.7348 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7721 - reconstruction_loss: 2.6335 - kl_loss: 5.7394 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7784 - reconstruction_loss: 2.6167 - kl_loss: 5.7275 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7110 - reconstruction_loss: 2.5817 - kl_loss: 5.7122 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6675 - reconstruction_loss: 2.5370 - kl_loss: 5.7058 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7121 - reconstruction_loss: 2.5202 - kl_loss: 5.6742 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6495 - reconstruction_loss: 2.4956 - kl_loss: 5.6983 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6006 - reconstruction_loss: 2.4760 - kl_loss: 5.6801 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6293 - reconstruction_loss: 2.4856 - kl_loss: 5.6687 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5814 - reconstruction_loss: 2.4676 - kl_loss: 5.6580 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5753 - reconstruction_loss: 2.4357 - kl_loss: 5.6246 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5726 - reconstruction_loss: 2.4309 - kl_loss: 5.6321 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5530 - reconstruction_loss: 2.4131 - kl_loss: 5.6163 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6158 - reconstruction_loss: 2.4829 - kl_loss: 5.6353 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6187 - reconstruction_loss: 2.4923 - kl_loss: 5.6680 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.6038 - reconstruction_loss: 2.4527 - kl_loss: 5.6420\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6035 - reconstruction_loss: 2.4443 - kl_loss: 5.6419 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5237 - reconstruction_loss: 2.3889 - kl_loss: 5.5927 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5138 - reconstruction_loss: 2.3887 - kl_loss: 5.5849 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5087 - reconstruction_loss: 2.3748 - kl_loss: 5.5836 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5378 - reconstruction_loss: 2.3946 - kl_loss: 5.5969 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5395 - reconstruction_loss: 2.4285 - kl_loss: 5.6172 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.5323 - reconstruction_loss: 2.4096 - kl_loss: 5.5567\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5318 - reconstruction_loss: 2.3999 - kl_loss: 5.5574 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4981 - reconstruction_loss: 2.3752 - kl_loss: 5.5766 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4772 - reconstruction_loss: 2.3589 - kl_loss: 5.5654 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4868 - reconstruction_loss: 2.3556 - kl_loss: 5.5573 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4644 - reconstruction_loss: 2.3599 - kl_loss: 5.5684 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4962 - reconstruction_loss: 2.3634 - kl_loss: 5.5568 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4684 - reconstruction_loss: 2.3462 - kl_loss: 5.5453 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4512 - reconstruction_loss: 2.3458 - kl_loss: 5.5454 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4744 - reconstruction_loss: 2.3477 - kl_loss: 5.5259 - lr: 2.5000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4498 - reconstruction_loss: 2.3467 - kl_loss: 5.5347 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4572 - reconstruction_loss: 2.3481 - kl_loss: 5.5306 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4630 - reconstruction_loss: 2.3326 - kl_loss: 5.5186 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4630 - reconstruction_loss: 2.3387 - kl_loss: 5.5348 - lr: 2.5000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4395 - reconstruction_loss: 2.3391 - kl_loss: 5.5147 - lr: 2.5000e-04\n",
      "Epoch 60/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.4437 - reconstruction_loss: 2.3435 - kl_loss: 5.5318\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4438 - reconstruction_loss: 2.3357 - kl_loss: 5.5308 - lr: 2.5000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4372 - reconstruction_loss: 2.3183 - kl_loss: 5.4993 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4197 - reconstruction_loss: 2.3109 - kl_loss: 5.5124 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4338 - reconstruction_loss: 2.3165 - kl_loss: 5.5244 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4239 - reconstruction_loss: 2.3116 - kl_loss: 5.5185 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.4478 - reconstruction_loss: 2.3250 - kl_loss: 5.5135\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4469 - reconstruction_loss: 2.3148 - kl_loss: 5.5128 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4246 - reconstruction_loss: 2.3118 - kl_loss: 5.5155 - lr: 6.2500e-05\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4106 - reconstruction_loss: 2.3066 - kl_loss: 5.5050 - lr: 6.2500e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4239 - reconstruction_loss: 2.3098 - kl_loss: 5.5290 - lr: 6.2500e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4169 - reconstruction_loss: 2.3061 - kl_loss: 5.5146 - lr: 6.2500e-05\n",
      "Epoch 70/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4254 - reconstruction_loss: 2.3168 - kl_loss: 5.5140\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4252 - reconstruction_loss: 2.3083 - kl_loss: 5.5144 - lr: 6.2500e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4109 - reconstruction_loss: 2.2963 - kl_loss: 5.5171 - lr: 3.1250e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4027 - reconstruction_loss: 2.2979 - kl_loss: 5.5163 - lr: 3.1250e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4004 - reconstruction_loss: 2.3003 - kl_loss: 5.5120 - lr: 3.1250e-05\n",
      "Epoch 74/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4002 - reconstruction_loss: 2.3108 - kl_loss: 5.5126\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4003 - reconstruction_loss: 2.3015 - kl_loss: 5.5123 - lr: 3.1250e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4096 - reconstruction_loss: 2.3008 - kl_loss: 5.5149 - lr: 1.5625e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4090 - reconstruction_loss: 2.2967 - kl_loss: 5.5090 - lr: 1.5625e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4168 - reconstruction_loss: 2.2972 - kl_loss: 5.5142 - lr: 1.5625e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4103 - reconstruction_loss: 2.2934 - kl_loss: 5.5149 - lr: 1.5625e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4193 - reconstruction_loss: 2.2990 - kl_loss: 5.5075 - lr: 1.5625e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4089 - reconstruction_loss: 2.2953 - kl_loss: 5.5108 - lr: 1.5625e-05\n",
      "Epoch 81/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.4017 - reconstruction_loss: 2.3049 - kl_loss: 5.5169\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4018 - reconstruction_loss: 2.2981 - kl_loss: 5.5171 - lr: 1.5625e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4061 - reconstruction_loss: 2.2911 - kl_loss: 5.5184 - lr: 7.8125e-06\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4038 - reconstruction_loss: 2.2942 - kl_loss: 5.5118 - lr: 7.8125e-06\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.3938 - reconstruction_loss: 2.2940 - kl_loss: 5.5097 - lr: 7.8125e-06\n",
      "Epoch 85/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4097 - reconstruction_loss: 2.3012 - kl_loss: 5.5173\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4095 - reconstruction_loss: 2.2927 - kl_loss: 5.5182 - lr: 7.8125e-06\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4064 - reconstruction_loss: 2.2931 - kl_loss: 5.5061 - lr: 3.9063e-06\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4137 - reconstruction_loss: 2.2918 - kl_loss: 5.5113 - lr: 3.9063e-06\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4001 - reconstruction_loss: 2.2923 - kl_loss: 5.5100 - lr: 3.9063e-06\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4106 - reconstruction_loss: 2.2913 - kl_loss: 5.5088 - lr: 3.9063e-06\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4010 - reconstruction_loss: 2.2925 - kl_loss: 5.5009 - lr: 3.9063e-06\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4152 - reconstruction_loss: 2.2923 - kl_loss: 5.5109 - lr: 3.9063e-06\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.3967 - reconstruction_loss: 2.2906 - kl_loss: 5.5098 - lr: 3.9063e-06\n",
      "Epoch 93/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.3974 - reconstruction_loss: 2.3012 - kl_loss: 5.5085\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.3975 - reconstruction_loss: 2.2921 - kl_loss: 5.5090 - lr: 3.9063e-06\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4111 - reconstruction_loss: 2.2895 - kl_loss: 5.5077 - lr: 1.9531e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.3996 - reconstruction_loss: 2.2906 - kl_loss: 5.5077 - lr: 1.9531e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.3996 - reconstruction_loss: 2.2926 - kl_loss: 5.5094 - lr: 1.9531e-06\n",
      "Epoch 97/300\n",
      "189/196 [===========================>..] - ETA: 0s - loss: 3.3941 - reconstruction_loss: 2.2975 - kl_loss: 5.5138\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.3942 - reconstruction_loss: 2.2898 - kl_loss: 5.5133 - lr: 1.9531e-06\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4022 - reconstruction_loss: 2.2880 - kl_loss: 5.5143 - lr: 1.0000e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4054 - reconstruction_loss: 2.2892 - kl_loss: 5.5132 - lr: 1.0000e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.3876 - reconstruction_loss: 2.2908 - kl_loss: 5.5118 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4083 - reconstruction_loss: 2.2892 - kl_loss: 5.5121 - lr: 1.0000e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4064 - reconstruction_loss: 2.2898 - kl_loss: 5.5097 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4073 - reconstruction_loss: 2.2908 - kl_loss: 5.5102 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4001 - reconstruction_loss: 2.2899 - kl_loss: 5.5093 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.3989 - reconstruction_loss: 2.2924 - kl_loss: 5.5116 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4079 - reconstruction_loss: 2.2898 - kl_loss: 5.5112 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.3956 - reconstruction_loss: 2.2900 - kl_loss: 5.5111 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4053 - reconstruction_loss: 2.2908 - kl_loss: 5.5114 - lr: 1.0000e-06\n",
      "Epoch 108: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 14:36:22,078]\u001b[0m Trial 59 finished with value: 0.004485743356822597 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 69, 'encoder_units_l2': 473, 'encoder_units_l3': 522, 'decoder_layers': 4, 'decoder_units_l1': 208, 'decoder_units_l2': 67, 'decoder_units_l3': 872, 'decoder_units_l4': 29, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 59 with value: 0.004485743356822597.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 4s 16ms/step - loss: 13.1509 - reconstruction_loss: 8.9793 - kl_loss: 3.9031 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 7.7217 - reconstruction_loss: 6.4406 - kl_loss: 4.5266 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 6.1727 - reconstruction_loss: 4.9037 - kl_loss: 5.1318 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 5.3613 - reconstruction_loss: 4.1685 - kl_loss: 5.4528 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 5.0662 - reconstruction_loss: 3.8557 - kl_loss: 5.6036 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.8316 - reconstruction_loss: 3.6452 - kl_loss: 5.6823 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.6966 - reconstruction_loss: 3.5074 - kl_loss: 5.7228 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.5310 - reconstruction_loss: 3.3496 - kl_loss: 5.7481 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.5293 - reconstruction_loss: 3.3296 - kl_loss: 5.7386 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.3855 - reconstruction_loss: 3.2155 - kl_loss: 5.7465 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.3217 - reconstruction_loss: 3.1452 - kl_loss: 5.7715 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.2430 - reconstruction_loss: 3.0604 - kl_loss: 5.7622 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.1605 - reconstruction_loss: 2.9896 - kl_loss: 5.7667 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.1094 - reconstruction_loss: 2.9443 - kl_loss: 5.8149 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.1267 - reconstruction_loss: 2.9178 - kl_loss: 5.8265 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.0026 - reconstruction_loss: 2.8287 - kl_loss: 5.8111 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.9936 - reconstruction_loss: 2.8127 - kl_loss: 5.8193 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.0478 - reconstruction_loss: 2.8025 - kl_loss: 5.7716 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.9123 - reconstruction_loss: 2.7249 - kl_loss: 5.8073 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.8801 - reconstruction_loss: 2.7038 - kl_loss: 5.8237 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.9025 - reconstruction_loss: 2.7504 - kl_loss: 5.8422 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.8631 - reconstruction_loss: 2.7175 - kl_loss: 5.7971 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.8950 - reconstruction_loss: 2.7032 - kl_loss: 5.7957 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.8618 - reconstruction_loss: 2.6817 - kl_loss: 5.8052 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.8164 - reconstruction_loss: 2.6694 - kl_loss: 5.7820 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.8641 - reconstruction_loss: 2.6702 - kl_loss: 5.7859 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.8048 - reconstruction_loss: 2.6281 - kl_loss: 5.7632 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.7828 - reconstruction_loss: 2.5965 - kl_loss: 5.7674 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.7576 - reconstruction_loss: 2.5860 - kl_loss: 5.7729 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.7227 - reconstruction_loss: 2.5713 - kl_loss: 5.7877 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.7645 - reconstruction_loss: 2.5788 - kl_loss: 5.7587 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.7753 - reconstruction_loss: 2.6130 - kl_loss: 5.7904 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.7256 - reconstruction_loss: 2.5823 - kl_loss: 5.7763\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.7257 - reconstruction_loss: 2.5736 - kl_loss: 5.7759 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6968 - reconstruction_loss: 2.5164 - kl_loss: 5.7884 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6798 - reconstruction_loss: 2.5195 - kl_loss: 5.8105 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.7099 - reconstruction_loss: 2.5595 - kl_loss: 5.8019 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.7041 - reconstruction_loss: 2.5251 - kl_loss: 5.7235 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6631 - reconstruction_loss: 2.4962 - kl_loss: 5.7282 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6459 - reconstruction_loss: 2.4856 - kl_loss: 5.7405 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6148 - reconstruction_loss: 2.4684 - kl_loss: 5.7144 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6324 - reconstruction_loss: 2.4724 - kl_loss: 5.7210 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6211 - reconstruction_loss: 2.4622 - kl_loss: 5.7048 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6152 - reconstruction_loss: 2.4595 - kl_loss: 5.7096 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6096 - reconstruction_loss: 2.4429 - kl_loss: 5.7108 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5892 - reconstruction_loss: 2.4467 - kl_loss: 5.7055 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6171 - reconstruction_loss: 2.4437 - kl_loss: 5.7138 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5897 - reconstruction_loss: 2.4345 - kl_loss: 5.7131 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5773 - reconstruction_loss: 2.4207 - kl_loss: 5.7078 - lr: 5.0000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6030 - reconstruction_loss: 2.4338 - kl_loss: 5.7361 - lr: 5.0000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5571 - reconstruction_loss: 2.4065 - kl_loss: 5.7241 - lr: 5.0000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6009 - reconstruction_loss: 2.4384 - kl_loss: 5.7875 - lr: 5.0000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6106 - reconstruction_loss: 2.4573 - kl_loss: 5.7951 - lr: 5.0000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5852 - reconstruction_loss: 2.4260 - kl_loss: 5.7719\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5852 - reconstruction_loss: 2.4260 - kl_loss: 5.7719 - lr: 5.0000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5722 - reconstruction_loss: 2.3939 - kl_loss: 5.7675 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5381 - reconstruction_loss: 2.3755 - kl_loss: 5.7558 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5224 - reconstruction_loss: 2.3729 - kl_loss: 5.7356 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5315 - reconstruction_loss: 2.3626 - kl_loss: 5.7387 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5090 - reconstruction_loss: 2.3567 - kl_loss: 5.7575 - lr: 2.5000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5164 - reconstruction_loss: 2.3563 - kl_loss: 5.7608 - lr: 2.5000e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5243 - reconstruction_loss: 2.3612 - kl_loss: 5.7505 - lr: 2.5000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5021 - reconstruction_loss: 2.3554 - kl_loss: 5.7348 - lr: 2.5000e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4898 - reconstruction_loss: 2.3366 - kl_loss: 5.7183 - lr: 2.5000e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4811 - reconstruction_loss: 2.3354 - kl_loss: 5.7386 - lr: 2.5000e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4877 - reconstruction_loss: 2.3322 - kl_loss: 5.7411 - lr: 2.5000e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4826 - reconstruction_loss: 2.3315 - kl_loss: 5.7168 - lr: 2.5000e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4918 - reconstruction_loss: 2.3361 - kl_loss: 5.7236 - lr: 2.5000e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4787 - reconstruction_loss: 2.3317 - kl_loss: 5.7298 - lr: 2.5000e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4647 - reconstruction_loss: 2.3293 - kl_loss: 5.7242 - lr: 2.5000e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4755 - reconstruction_loss: 2.3137 - kl_loss: 5.7235 - lr: 2.5000e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4602 - reconstruction_loss: 2.3099 - kl_loss: 5.7377 - lr: 2.5000e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4770 - reconstruction_loss: 2.3184 - kl_loss: 5.7335 - lr: 2.5000e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4602 - reconstruction_loss: 2.3166 - kl_loss: 5.7307 - lr: 2.5000e-04\n",
      "Epoch 73/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4726 - reconstruction_loss: 2.3222 - kl_loss: 5.7430\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4724 - reconstruction_loss: 2.3136 - kl_loss: 5.7428 - lr: 2.5000e-04\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4539 - reconstruction_loss: 2.2981 - kl_loss: 5.7379 - lr: 1.2500e-04\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4389 - reconstruction_loss: 2.2862 - kl_loss: 5.7446 - lr: 1.2500e-04\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4348 - reconstruction_loss: 2.2840 - kl_loss: 5.7555 - lr: 1.2500e-04\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4496 - reconstruction_loss: 2.2960 - kl_loss: 5.7465 - lr: 1.2500e-04\n",
      "Epoch 78/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4323 - reconstruction_loss: 2.2972 - kl_loss: 5.7402\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4324 - reconstruction_loss: 2.2890 - kl_loss: 5.7402 - lr: 1.2500e-04\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4393 - reconstruction_loss: 2.2815 - kl_loss: 5.7470 - lr: 6.2500e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4319 - reconstruction_loss: 2.2762 - kl_loss: 5.7320 - lr: 6.2500e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4296 - reconstruction_loss: 2.2740 - kl_loss: 5.7459 - lr: 6.2500e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4226 - reconstruction_loss: 2.2691 - kl_loss: 5.7521 - lr: 6.2500e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4259 - reconstruction_loss: 2.2712 - kl_loss: 5.7488 - lr: 6.2500e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4326 - reconstruction_loss: 2.2740 - kl_loss: 5.7511 - lr: 6.2500e-05\n",
      "Epoch 85/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4311 - reconstruction_loss: 2.2811 - kl_loss: 5.7535\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4310 - reconstruction_loss: 2.2736 - kl_loss: 5.7537 - lr: 6.2500e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4182 - reconstruction_loss: 2.2660 - kl_loss: 5.7439 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4277 - reconstruction_loss: 2.2657 - kl_loss: 5.7485 - lr: 3.1250e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4325 - reconstruction_loss: 2.2636 - kl_loss: 5.7559 - lr: 3.1250e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4213 - reconstruction_loss: 2.2601 - kl_loss: 5.7588 - lr: 3.1250e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4169 - reconstruction_loss: 2.2579 - kl_loss: 5.7645 - lr: 3.1250e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4074 - reconstruction_loss: 2.2589 - kl_loss: 5.7513 - lr: 3.1250e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4079 - reconstruction_loss: 2.2553 - kl_loss: 5.7552 - lr: 3.1250e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4028 - reconstruction_loss: 2.2570 - kl_loss: 5.7559 - lr: 3.1250e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4199 - reconstruction_loss: 2.2571 - kl_loss: 5.7552 - lr: 3.1250e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4135 - reconstruction_loss: 2.2559 - kl_loss: 5.7506 - lr: 3.1250e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4174 - reconstruction_loss: 2.2513 - kl_loss: 5.7706 - lr: 3.1250e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4163 - reconstruction_loss: 2.2557 - kl_loss: 5.7613 - lr: 3.1250e-05\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4139 - reconstruction_loss: 2.2603 - kl_loss: 5.7630 - lr: 3.1250e-05\n",
      "Epoch 99/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4213 - reconstruction_loss: 2.2608 - kl_loss: 5.7624\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4211 - reconstruction_loss: 2.2534 - kl_loss: 5.7617 - lr: 3.1250e-05\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4127 - reconstruction_loss: 2.2494 - kl_loss: 5.7544 - lr: 1.5625e-05\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4055 - reconstruction_loss: 2.2456 - kl_loss: 5.7560 - lr: 1.5625e-05\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4276 - reconstruction_loss: 2.2493 - kl_loss: 5.7476 - lr: 1.5625e-05\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4010 - reconstruction_loss: 2.2486 - kl_loss: 5.7523 - lr: 1.5625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4007 - reconstruction_loss: 2.2504 - kl_loss: 5.7575\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4007 - reconstruction_loss: 2.2504 - kl_loss: 5.7575 - lr: 1.5625e-05\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4067 - reconstruction_loss: 2.2442 - kl_loss: 5.7578 - lr: 7.8125e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4037 - reconstruction_loss: 2.2463 - kl_loss: 5.7535 - lr: 7.8125e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3972 - reconstruction_loss: 2.2438 - kl_loss: 5.7573 - lr: 7.8125e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4086 - reconstruction_loss: 2.2456 - kl_loss: 5.7577 - lr: 7.8125e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4045 - reconstruction_loss: 2.2433 - kl_loss: 5.7634 - lr: 7.8125e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4000 - reconstruction_loss: 2.2399 - kl_loss: 5.7560 - lr: 7.8125e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4138 - reconstruction_loss: 2.2448 - kl_loss: 5.7585 - lr: 7.8125e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3890 - reconstruction_loss: 2.2438 - kl_loss: 5.7572 - lr: 7.8125e-06\n",
      "Epoch 113/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.3935 - reconstruction_loss: 2.2497 - kl_loss: 5.7557\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3935 - reconstruction_loss: 2.2419 - kl_loss: 5.7558 - lr: 7.8125e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4128 - reconstruction_loss: 2.2399 - kl_loss: 5.7639 - lr: 3.9063e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3996 - reconstruction_loss: 2.2411 - kl_loss: 5.7600 - lr: 3.9063e-06\n",
      "Epoch 116/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.3898 - reconstruction_loss: 2.2481 - kl_loss: 5.7645\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3899 - reconstruction_loss: 2.2414 - kl_loss: 5.7648 - lr: 3.9063e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.3901 - reconstruction_loss: 2.2412 - kl_loss: 5.7654 - lr: 1.9531e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3923 - reconstruction_loss: 2.2380 - kl_loss: 5.7647 - lr: 1.9531e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3964 - reconstruction_loss: 2.2385 - kl_loss: 5.7624 - lr: 1.9531e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4026 - reconstruction_loss: 2.2391 - kl_loss: 5.7633 - lr: 1.9531e-06\n",
      "Epoch 121/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.3961 - reconstruction_loss: 2.2476 - kl_loss: 5.7625\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3961 - reconstruction_loss: 2.2397 - kl_loss: 5.7625 - lr: 1.9531e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3940 - reconstruction_loss: 2.2368 - kl_loss: 5.7613 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3974 - reconstruction_loss: 2.2426 - kl_loss: 5.7618 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3946 - reconstruction_loss: 2.2420 - kl_loss: 5.7652 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4036 - reconstruction_loss: 2.2405 - kl_loss: 5.7655 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4010 - reconstruction_loss: 2.2383 - kl_loss: 5.7671 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4002 - reconstruction_loss: 2.2396 - kl_loss: 5.7676 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4014 - reconstruction_loss: 2.2392 - kl_loss: 5.7675 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3882 - reconstruction_loss: 2.2363 - kl_loss: 5.7664 - lr: 1.0000e-06\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4006 - reconstruction_loss: 2.2372 - kl_loss: 5.7657 - lr: 1.0000e-06\n",
      "Epoch 131/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4082 - reconstruction_loss: 2.2383 - kl_loss: 5.7642 - lr: 1.0000e-06\n",
      "Epoch 132/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4002 - reconstruction_loss: 2.2382 - kl_loss: 5.7634 - lr: 1.0000e-06\n",
      "Epoch 132: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 14:43:24,126]\u001b[0m Trial 60 finished with value: 0.004385454877985728 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 35, 'encoder_units_l2': 693, 'encoder_units_l3': 911, 'decoder_layers': 5, 'decoder_units_l1': 112, 'decoder_units_l2': 65, 'decoder_units_l3': 894, 'decoder_units_l4': 49, 'decoder_units_l5': 54, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 60 with value: 0.004385454877985728.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 12ms/step - loss: 13.0723 - reconstruction_loss: 8.9080 - kl_loss: 3.9250 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.6197 - reconstruction_loss: 6.3741 - kl_loss: 4.6138 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 6.3143 - reconstruction_loss: 4.9534 - kl_loss: 5.1413 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 5.5083 - reconstruction_loss: 4.3203 - kl_loss: 5.4379 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 5.1968 - reconstruction_loss: 4.0160 - kl_loss: 5.4901 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.9754 - reconstruction_loss: 3.8172 - kl_loss: 5.6049 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.8051 - reconstruction_loss: 3.6297 - kl_loss: 5.6756 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.7510 - reconstruction_loss: 3.5208 - kl_loss: 5.7497 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.5548 - reconstruction_loss: 3.3836 - kl_loss: 5.7986 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.5110 - reconstruction_loss: 3.3195 - kl_loss: 5.8204 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.4086 - reconstruction_loss: 3.1980 - kl_loss: 5.8388 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.3161 - reconstruction_loss: 3.0977 - kl_loss: 5.8773 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.2990 - reconstruction_loss: 3.0792 - kl_loss: 5.8601 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.1707 - reconstruction_loss: 2.9784 - kl_loss: 5.8922 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.1780 - reconstruction_loss: 2.9603 - kl_loss: 5.9047 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0786 - reconstruction_loss: 2.9295 - kl_loss: 5.8680 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.0702 - reconstruction_loss: 2.9022 - kl_loss: 5.8618 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.0414 - reconstruction_loss: 2.8412 - kl_loss: 5.8733 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.0224 - reconstruction_loss: 2.7933 - kl_loss: 5.8759 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9503 - reconstruction_loss: 2.7252 - kl_loss: 5.8648 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9045 - reconstruction_loss: 2.7117 - kl_loss: 5.8935 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9051 - reconstruction_loss: 2.7005 - kl_loss: 5.8683 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8452 - reconstruction_loss: 2.6547 - kl_loss: 5.8458 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8387 - reconstruction_loss: 2.6650 - kl_loss: 5.8289 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8508 - reconstruction_loss: 2.6664 - kl_loss: 5.8316 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8060 - reconstruction_loss: 2.6295 - kl_loss: 5.7936 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8879 - reconstruction_loss: 2.6506 - kl_loss: 5.8083 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8158 - reconstruction_loss: 2.6557 - kl_loss: 5.8005 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7754 - reconstruction_loss: 2.5979 - kl_loss: 5.7909 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7373 - reconstruction_loss: 2.5711 - kl_loss: 5.7751 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7523 - reconstruction_loss: 2.5888 - kl_loss: 5.7822 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7372 - reconstruction_loss: 2.5732 - kl_loss: 5.7595 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7467 - reconstruction_loss: 2.5692 - kl_loss: 5.7435 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7579 - reconstruction_loss: 2.5920 - kl_loss: 5.7712 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7815 - reconstruction_loss: 2.5940 - kl_loss: 5.7552 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7278 - reconstruction_loss: 2.5486 - kl_loss: 5.7199 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6585 - reconstruction_loss: 2.5179 - kl_loss: 5.7035 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6886 - reconstruction_loss: 2.5159 - kl_loss: 5.7076 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6346 - reconstruction_loss: 2.4897 - kl_loss: 5.7000 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6402 - reconstruction_loss: 2.4788 - kl_loss: 5.7000 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6549 - reconstruction_loss: 2.4856 - kl_loss: 5.7106 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5995 - reconstruction_loss: 2.4676 - kl_loss: 5.7020 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6176 - reconstruction_loss: 2.4484 - kl_loss: 5.6960 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5847 - reconstruction_loss: 2.4379 - kl_loss: 5.7037 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5943 - reconstruction_loss: 2.4388 - kl_loss: 5.6901 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6272 - reconstruction_loss: 2.4718 - kl_loss: 5.7284 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5737 - reconstruction_loss: 2.4209 - kl_loss: 5.6934 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5593 - reconstruction_loss: 2.4339 - kl_loss: 5.6972 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6604 - reconstruction_loss: 2.5415 - kl_loss: 5.7328 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.6682 - reconstruction_loss: 2.5392 - kl_loss: 5.8070\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6683 - reconstruction_loss: 2.5392 - kl_loss: 5.8070 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6801 - reconstruction_loss: 2.4805 - kl_loss: 5.8000 - lr: 5.0000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5882 - reconstruction_loss: 2.4206 - kl_loss: 5.7859 - lr: 5.0000e-04\n",
      "Epoch 53/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.5805 - reconstruction_loss: 2.4507 - kl_loss: 5.7786\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5812 - reconstruction_loss: 2.4426 - kl_loss: 5.7796 - lr: 5.0000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5885 - reconstruction_loss: 2.4012 - kl_loss: 5.8103 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5946 - reconstruction_loss: 2.4001 - kl_loss: 5.8265 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5551 - reconstruction_loss: 2.3827 - kl_loss: 5.7848 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5479 - reconstruction_loss: 2.3897 - kl_loss: 5.7974 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5683 - reconstruction_loss: 2.3974 - kl_loss: 5.7954 - lr: 2.5000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5315 - reconstruction_loss: 2.3715 - kl_loss: 5.7476 - lr: 2.5000e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5220 - reconstruction_loss: 2.3526 - kl_loss: 5.7217 - lr: 2.5000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5011 - reconstruction_loss: 2.3511 - kl_loss: 5.7445 - lr: 2.5000e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5008 - reconstruction_loss: 2.3452 - kl_loss: 5.7323 - lr: 2.5000e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5013 - reconstruction_loss: 2.3400 - kl_loss: 5.7021 - lr: 2.5000e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4717 - reconstruction_loss: 2.3351 - kl_loss: 5.7201 - lr: 2.5000e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5291 - reconstruction_loss: 2.3696 - kl_loss: 5.7472 - lr: 2.5000e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5149 - reconstruction_loss: 2.3455 - kl_loss: 5.7395 - lr: 2.5000e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4759 - reconstruction_loss: 2.3284 - kl_loss: 5.7123 - lr: 2.5000e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4722 - reconstruction_loss: 2.3271 - kl_loss: 5.7130 - lr: 2.5000e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4811 - reconstruction_loss: 2.3153 - kl_loss: 5.7263 - lr: 2.5000e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4663 - reconstruction_loss: 2.3221 - kl_loss: 5.7120 - lr: 2.5000e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4731 - reconstruction_loss: 2.3130 - kl_loss: 5.6935 - lr: 2.5000e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4522 - reconstruction_loss: 2.3000 - kl_loss: 5.7226 - lr: 2.5000e-04\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4541 - reconstruction_loss: 2.2988 - kl_loss: 5.7106 - lr: 2.5000e-04\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4430 - reconstruction_loss: 2.2804 - kl_loss: 5.7181 - lr: 2.5000e-04\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4470 - reconstruction_loss: 2.2967 - kl_loss: 5.7444 - lr: 2.5000e-04\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4504 - reconstruction_loss: 2.2968 - kl_loss: 5.7486 - lr: 2.5000e-04\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4603 - reconstruction_loss: 2.3122 - kl_loss: 5.7579\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4603 - reconstruction_loss: 2.3122 - kl_loss: 5.7579 - lr: 2.5000e-04\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4681 - reconstruction_loss: 2.2891 - kl_loss: 5.7408 - lr: 1.2500e-04\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4351 - reconstruction_loss: 2.2791 - kl_loss: 5.7483 - lr: 1.2500e-04\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4379 - reconstruction_loss: 2.2800 - kl_loss: 5.7543\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4379 - reconstruction_loss: 2.2800 - kl_loss: 5.7543 - lr: 1.2500e-04\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4220 - reconstruction_loss: 2.2633 - kl_loss: 5.7497 - lr: 6.2500e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4112 - reconstruction_loss: 2.2584 - kl_loss: 5.7338 - lr: 6.2500e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3980 - reconstruction_loss: 2.2579 - kl_loss: 5.7432 - lr: 6.2500e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4228 - reconstruction_loss: 2.2610 - kl_loss: 5.7408 - lr: 6.2500e-05\n",
      "Epoch 85/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.4182 - reconstruction_loss: 2.2681 - kl_loss: 5.7347\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4180 - reconstruction_loss: 2.2596 - kl_loss: 5.7347 - lr: 6.2500e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4038 - reconstruction_loss: 2.2524 - kl_loss: 5.7360 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4136 - reconstruction_loss: 2.2540 - kl_loss: 5.7425 - lr: 3.1250e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4130 - reconstruction_loss: 2.2491 - kl_loss: 5.7452 - lr: 3.1250e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3872 - reconstruction_loss: 2.2459 - kl_loss: 5.7351 - lr: 3.1250e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3928 - reconstruction_loss: 2.2453 - kl_loss: 5.7344 - lr: 3.1250e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4236 - reconstruction_loss: 2.2457 - kl_loss: 5.7572 - lr: 3.1250e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4142 - reconstruction_loss: 2.2436 - kl_loss: 5.7395 - lr: 3.1250e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3989 - reconstruction_loss: 2.2435 - kl_loss: 5.7511 - lr: 3.1250e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3902 - reconstruction_loss: 2.2443 - kl_loss: 5.7524 - lr: 3.1250e-05\n",
      "Epoch 95/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.3949 - reconstruction_loss: 2.2516 - kl_loss: 5.7524\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3949 - reconstruction_loss: 2.2437 - kl_loss: 5.7527 - lr: 3.1250e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3950 - reconstruction_loss: 2.2374 - kl_loss: 5.7498 - lr: 1.5625e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4107 - reconstruction_loss: 2.2416 - kl_loss: 5.7447 - lr: 1.5625e-05\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3979 - reconstruction_loss: 2.2403 - kl_loss: 5.7500 - lr: 1.5625e-05\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4035 - reconstruction_loss: 2.2407 - kl_loss: 5.7484\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4034 - reconstruction_loss: 2.2407 - kl_loss: 5.7484 - lr: 1.5625e-05\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3873 - reconstruction_loss: 2.2338 - kl_loss: 5.7538 - lr: 7.8125e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3821 - reconstruction_loss: 2.2340 - kl_loss: 5.7486 - lr: 7.8125e-06\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3959 - reconstruction_loss: 2.2321 - kl_loss: 5.7550 - lr: 7.8125e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3962 - reconstruction_loss: 2.2329 - kl_loss: 5.7452 - lr: 7.8125e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3835 - reconstruction_loss: 2.2351 - kl_loss: 5.7418 - lr: 7.8125e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3921 - reconstruction_loss: 2.2327 - kl_loss: 5.7456 - lr: 7.8125e-06\n",
      "Epoch 106/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.3934 - reconstruction_loss: 2.2395 - kl_loss: 5.7586\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3933 - reconstruction_loss: 2.2316 - kl_loss: 5.7585 - lr: 7.8125e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3860 - reconstruction_loss: 2.2294 - kl_loss: 5.7507 - lr: 3.9063e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3731 - reconstruction_loss: 2.2326 - kl_loss: 5.7482 - lr: 3.9063e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3871 - reconstruction_loss: 2.2310 - kl_loss: 5.7433 - lr: 3.9063e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4061 - reconstruction_loss: 2.2322 - kl_loss: 5.7419\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4060 - reconstruction_loss: 2.2322 - kl_loss: 5.7419 - lr: 3.9063e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3973 - reconstruction_loss: 2.2328 - kl_loss: 5.7453 - lr: 1.9531e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3868 - reconstruction_loss: 2.2313 - kl_loss: 5.7448 - lr: 1.9531e-06\n",
      "Epoch 113/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.3841 - reconstruction_loss: 2.2380 - kl_loss: 5.7450\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3841 - reconstruction_loss: 2.2319 - kl_loss: 5.7447 - lr: 1.9531e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3812 - reconstruction_loss: 2.2298 - kl_loss: 5.7474 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3939 - reconstruction_loss: 2.2303 - kl_loss: 5.7470 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3794 - reconstruction_loss: 2.2273 - kl_loss: 5.7482 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3792 - reconstruction_loss: 2.2272 - kl_loss: 5.7486 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3924 - reconstruction_loss: 2.2312 - kl_loss: 5.7471 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3801 - reconstruction_loss: 2.2295 - kl_loss: 5.7476 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3805 - reconstruction_loss: 2.2297 - kl_loss: 5.7450 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4030 - reconstruction_loss: 2.2304 - kl_loss: 5.7445 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3925 - reconstruction_loss: 2.2300 - kl_loss: 5.7467 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3937 - reconstruction_loss: 2.2295 - kl_loss: 5.7439 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3813 - reconstruction_loss: 2.2306 - kl_loss: 5.7444 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3839 - reconstruction_loss: 2.2314 - kl_loss: 5.7468 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3823 - reconstruction_loss: 2.2279 - kl_loss: 5.7471 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3927 - reconstruction_loss: 2.2279 - kl_loss: 5.7456 - lr: 1.0000e-06\n",
      "Epoch 127: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 14:48:04,115]\u001b[0m Trial 61 finished with value: 0.004372707378647208 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 28, 'encoder_units_l2': 718, 'encoder_units_l3': 871, 'decoder_layers': 4, 'decoder_units_l1': 191, 'decoder_units_l2': 65, 'decoder_units_l3': 954, 'decoder_units_l4': 28, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 12.7057 - reconstruction_loss: 8.7993 - kl_loss: 3.8538 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 6.9996 - reconstruction_loss: 5.4888 - kl_loss: 4.8011 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 5.6280 - reconstruction_loss: 4.4570 - kl_loss: 5.1214 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 5.2358 - reconstruction_loss: 4.1077 - kl_loss: 5.2771 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.9889 - reconstruction_loss: 3.8608 - kl_loss: 5.3692 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.8134 - reconstruction_loss: 3.6440 - kl_loss: 5.4754 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.7067 - reconstruction_loss: 3.5183 - kl_loss: 5.5201 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.5141 - reconstruction_loss: 3.3812 - kl_loss: 5.5763 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.4931 - reconstruction_loss: 3.3131 - kl_loss: 5.5772 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.3687 - reconstruction_loss: 3.2561 - kl_loss: 5.5828 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.3483 - reconstruction_loss: 3.2240 - kl_loss: 5.6059 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.2888 - reconstruction_loss: 3.1248 - kl_loss: 5.6418 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.1826 - reconstruction_loss: 3.0459 - kl_loss: 5.6598 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.1504 - reconstruction_loss: 3.0042 - kl_loss: 5.6638 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.1235 - reconstruction_loss: 2.9679 - kl_loss: 5.6752 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.1378 - reconstruction_loss: 2.9968 - kl_loss: 5.6693 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.0700 - reconstruction_loss: 2.9132 - kl_loss: 5.7086 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.9896 - reconstruction_loss: 2.8451 - kl_loss: 5.7163 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.0006 - reconstruction_loss: 2.8114 - kl_loss: 5.7192 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.9423 - reconstruction_loss: 2.7800 - kl_loss: 5.7481 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.9018 - reconstruction_loss: 2.7478 - kl_loss: 5.7300 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.8914 - reconstruction_loss: 2.7217 - kl_loss: 5.7347 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.8793 - reconstruction_loss: 2.7107 - kl_loss: 5.7024 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.8832 - reconstruction_loss: 2.7186 - kl_loss: 5.7421 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.8670 - reconstruction_loss: 2.7153 - kl_loss: 5.7343 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.8404 - reconstruction_loss: 2.6763 - kl_loss: 5.7303 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.8040 - reconstruction_loss: 2.6696 - kl_loss: 5.7341 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.7813 - reconstruction_loss: 2.6209 - kl_loss: 5.7171 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.8165 - reconstruction_loss: 2.6522 - kl_loss: 5.7433 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.7977 - reconstruction_loss: 2.6654 - kl_loss: 5.7486 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 4.0179 - reconstruction_loss: 2.7825 - kl_loss: 5.8195\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 4.0152 - reconstruction_loss: 2.7686 - kl_loss: 5.8195 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.7885 - reconstruction_loss: 2.5883 - kl_loss: 5.7946 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.7195 - reconstruction_loss: 2.5527 - kl_loss: 5.8017 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.7132 - reconstruction_loss: 2.5427 - kl_loss: 5.7748 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6746 - reconstruction_loss: 2.5270 - kl_loss: 5.7544 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6737 - reconstruction_loss: 2.5216 - kl_loss: 5.7760 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6708 - reconstruction_loss: 2.5410 - kl_loss: 5.8105 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.7514 - reconstruction_loss: 2.5669 - kl_loss: 5.8047 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.7181 - reconstruction_loss: 2.5439 - kl_loss: 5.7667\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.7172 - reconstruction_loss: 2.5333 - kl_loss: 5.7683 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.6556 - reconstruction_loss: 2.4842 - kl_loss: 5.7574 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.6454 - reconstruction_loss: 2.4739 - kl_loss: 5.7522 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.6487 - reconstruction_loss: 2.4778 - kl_loss: 5.7913 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.6548 - reconstruction_loss: 2.4801 - kl_loss: 5.7805 - lr: 2.5000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.6234 - reconstruction_loss: 2.4644 - kl_loss: 5.7678 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.6189 - reconstruction_loss: 2.4498 - kl_loss: 5.7783 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.6173 - reconstruction_loss: 2.4555 - kl_loss: 5.7687 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.6264 - reconstruction_loss: 2.4516 - kl_loss: 5.7646 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5963 - reconstruction_loss: 2.4429 - kl_loss: 5.7552 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6183 - reconstruction_loss: 2.4518 - kl_loss: 5.7506 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5988 - reconstruction_loss: 2.4544 - kl_loss: 5.7444 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.6080 - reconstruction_loss: 2.4376 - kl_loss: 5.7613 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5874 - reconstruction_loss: 2.4382 - kl_loss: 5.7445 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.6097 - reconstruction_loss: 2.4296 - kl_loss: 5.7438 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5832 - reconstruction_loss: 2.4215 - kl_loss: 5.7261 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5865 - reconstruction_loss: 2.4239 - kl_loss: 5.7420 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5865 - reconstruction_loss: 2.4178 - kl_loss: 5.7360 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5724 - reconstruction_loss: 2.4109 - kl_loss: 5.7416 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5670 - reconstruction_loss: 2.4120 - kl_loss: 5.7433 - lr: 2.5000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5828 - reconstruction_loss: 2.4069 - kl_loss: 5.7374 - lr: 2.5000e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5693 - reconstruction_loss: 2.4089 - kl_loss: 5.7580 - lr: 2.5000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5628 - reconstruction_loss: 2.3997 - kl_loss: 5.7492 - lr: 2.5000e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5396 - reconstruction_loss: 2.3861 - kl_loss: 5.7492 - lr: 2.5000e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5744 - reconstruction_loss: 2.3985 - kl_loss: 5.7662 - lr: 2.5000e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5382 - reconstruction_loss: 2.3825 - kl_loss: 5.7731 - lr: 2.5000e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5450 - reconstruction_loss: 2.3689 - kl_loss: 5.7700 - lr: 2.5000e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5228 - reconstruction_loss: 2.3660 - kl_loss: 5.7720 - lr: 2.5000e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5336 - reconstruction_loss: 2.3760 - kl_loss: 5.7742 - lr: 2.5000e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5372 - reconstruction_loss: 2.3711 - kl_loss: 5.7981 - lr: 2.5000e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5371 - reconstruction_loss: 2.3571 - kl_loss: 5.7984 - lr: 2.5000e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5125 - reconstruction_loss: 2.3524 - kl_loss: 5.7886 - lr: 2.5000e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5418 - reconstruction_loss: 2.3836 - kl_loss: 5.7902 - lr: 2.5000e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5523 - reconstruction_loss: 2.3712 - kl_loss: 5.7971 - lr: 2.5000e-04\n",
      "Epoch 73/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.5420 - reconstruction_loss: 2.3920 - kl_loss: 5.8022\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5423 - reconstruction_loss: 2.3834 - kl_loss: 5.8027 - lr: 2.5000e-04\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5585 - reconstruction_loss: 2.3764 - kl_loss: 5.7804 - lr: 1.2500e-04\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5076 - reconstruction_loss: 2.3466 - kl_loss: 5.8151 - lr: 1.2500e-04\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5102 - reconstruction_loss: 2.3398 - kl_loss: 5.8118 - lr: 1.2500e-04\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5073 - reconstruction_loss: 2.3372 - kl_loss: 5.8101 - lr: 1.2500e-04\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5316 - reconstruction_loss: 2.3535 - kl_loss: 5.8099 - lr: 1.2500e-04\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5301 - reconstruction_loss: 2.3567 - kl_loss: 5.8098 - lr: 1.2500e-04\n",
      "Epoch 80/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5128 - reconstruction_loss: 2.3553 - kl_loss: 5.8032\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5127 - reconstruction_loss: 2.3468 - kl_loss: 5.8030 - lr: 1.2500e-04\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4991 - reconstruction_loss: 2.3320 - kl_loss: 5.8178 - lr: 6.2500e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4890 - reconstruction_loss: 2.3259 - kl_loss: 5.8143 - lr: 6.2500e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4910 - reconstruction_loss: 2.3246 - kl_loss: 5.8307 - lr: 6.2500e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4949 - reconstruction_loss: 2.3226 - kl_loss: 5.8272 - lr: 6.2500e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4945 - reconstruction_loss: 2.3207 - kl_loss: 5.8273 - lr: 6.2500e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4904 - reconstruction_loss: 2.3245 - kl_loss: 5.8206 - lr: 6.2500e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5054 - reconstruction_loss: 2.3222 - kl_loss: 5.8285 - lr: 6.2500e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4825 - reconstruction_loss: 2.3173 - kl_loss: 5.8325 - lr: 6.2500e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4909 - reconstruction_loss: 2.3112 - kl_loss: 5.8179 - lr: 6.2500e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4748 - reconstruction_loss: 2.3114 - kl_loss: 5.8250 - lr: 6.2500e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4804 - reconstruction_loss: 2.3049 - kl_loss: 5.8345 - lr: 6.2500e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4703 - reconstruction_loss: 2.3124 - kl_loss: 5.8315 - lr: 6.2500e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4774 - reconstruction_loss: 2.3076 - kl_loss: 5.8186 - lr: 6.2500e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4673 - reconstruction_loss: 2.3012 - kl_loss: 5.8435 - lr: 6.2500e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4828 - reconstruction_loss: 2.3025 - kl_loss: 5.8267 - lr: 6.2500e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4863 - reconstruction_loss: 2.3044 - kl_loss: 5.8506 - lr: 6.2500e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4884 - reconstruction_loss: 2.3064 - kl_loss: 5.8376 - lr: 6.2500e-05\n",
      "Epoch 98/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.4748 - reconstruction_loss: 2.3089 - kl_loss: 5.8353\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4748 - reconstruction_loss: 2.3035 - kl_loss: 5.8352 - lr: 6.2500e-05\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4710 - reconstruction_loss: 2.2954 - kl_loss: 5.8352 - lr: 3.1250e-05\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4576 - reconstruction_loss: 2.2951 - kl_loss: 5.8357 - lr: 3.1250e-05\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4695 - reconstruction_loss: 2.2920 - kl_loss: 5.8410 - lr: 3.1250e-05\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4714 - reconstruction_loss: 2.2912 - kl_loss: 5.8485 - lr: 3.1250e-05\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4754 - reconstruction_loss: 2.2912 - kl_loss: 5.8365 - lr: 3.1250e-05\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4527 - reconstruction_loss: 2.2899 - kl_loss: 5.8276 - lr: 3.1250e-05\n",
      "Epoch 105/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4696 - reconstruction_loss: 2.2938 - kl_loss: 5.8529 - lr: 3.1250e-05\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4746 - reconstruction_loss: 2.2982 - kl_loss: 5.8296 - lr: 3.1250e-05\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4647 - reconstruction_loss: 2.2884 - kl_loss: 5.8311 - lr: 3.1250e-05\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4570 - reconstruction_loss: 2.2850 - kl_loss: 5.8389 - lr: 3.1250e-05\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4593 - reconstruction_loss: 2.2871 - kl_loss: 5.8435 - lr: 3.1250e-05\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4549 - reconstruction_loss: 2.2839 - kl_loss: 5.8468 - lr: 3.1250e-05\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4562 - reconstruction_loss: 2.2886 - kl_loss: 5.8329\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4562 - reconstruction_loss: 2.2886 - kl_loss: 5.8329 - lr: 3.1250e-05\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4490 - reconstruction_loss: 2.2785 - kl_loss: 5.8588 - lr: 1.5625e-05\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4578 - reconstruction_loss: 2.2802 - kl_loss: 5.8422 - lr: 1.5625e-05\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4492 - reconstruction_loss: 2.2803 - kl_loss: 5.8349 - lr: 1.5625e-05\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4580 - reconstruction_loss: 2.2801 - kl_loss: 5.8448 - lr: 1.5625e-05\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4445 - reconstruction_loss: 2.2771 - kl_loss: 5.8381 - lr: 1.5625e-05\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4594 - reconstruction_loss: 2.2788 - kl_loss: 5.8362 - lr: 1.5625e-05\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4593 - reconstruction_loss: 2.2841 - kl_loss: 5.8275 - lr: 1.5625e-05\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4578 - reconstruction_loss: 2.2831 - kl_loss: 5.8348\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4577 - reconstruction_loss: 2.2831 - kl_loss: 5.8348 - lr: 1.5625e-05\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4567 - reconstruction_loss: 2.2786 - kl_loss: 5.8392 - lr: 7.8125e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4658 - reconstruction_loss: 2.2778 - kl_loss: 5.8455 - lr: 7.8125e-06\n",
      "Epoch 122/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4680 - reconstruction_loss: 2.2864 - kl_loss: 5.8432\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4678 - reconstruction_loss: 2.2784 - kl_loss: 5.8431 - lr: 7.8125e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4433 - reconstruction_loss: 2.2759 - kl_loss: 5.8368 - lr: 3.9063e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4539 - reconstruction_loss: 2.2770 - kl_loss: 5.8347 - lr: 3.9063e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4437 - reconstruction_loss: 2.2745 - kl_loss: 5.8363 - lr: 3.9063e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4449 - reconstruction_loss: 2.2742 - kl_loss: 5.8369 - lr: 3.9063e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4560 - reconstruction_loss: 2.2751 - kl_loss: 5.8401 - lr: 3.9063e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4526 - reconstruction_loss: 2.2711 - kl_loss: 5.8434 - lr: 3.9063e-06\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4477 - reconstruction_loss: 2.2727 - kl_loss: 5.8407 - lr: 3.9063e-06\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4516 - reconstruction_loss: 2.2711 - kl_loss: 5.8426 - lr: 3.9063e-06\n",
      "Epoch 131/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4436 - reconstruction_loss: 2.2736 - kl_loss: 5.8429 - lr: 3.9063e-06\n",
      "Epoch 132/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4474 - reconstruction_loss: 2.2714 - kl_loss: 5.8388 - lr: 3.9063e-06\n",
      "Epoch 133/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4527 - reconstruction_loss: 2.2721 - kl_loss: 5.8363 - lr: 3.9063e-06\n",
      "Epoch 134/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4392 - reconstruction_loss: 2.2707 - kl_loss: 5.8377 - lr: 3.9063e-06\n",
      "Epoch 135/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4520 - reconstruction_loss: 2.2738 - kl_loss: 5.8393 - lr: 3.9063e-06\n",
      "Epoch 136/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4362 - reconstruction_loss: 2.2701 - kl_loss: 5.8336 - lr: 3.9063e-06\n",
      "Epoch 137/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4489 - reconstruction_loss: 2.2706 - kl_loss: 5.8343 - lr: 3.9063e-06\n",
      "Epoch 138/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4450 - reconstruction_loss: 2.2744 - kl_loss: 5.8399 - lr: 3.9063e-06\n",
      "Epoch 139/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4569 - reconstruction_loss: 2.2805 - kl_loss: 5.8465\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4568 - reconstruction_loss: 2.2726 - kl_loss: 5.8465 - lr: 3.9063e-06\n",
      "Epoch 140/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4514 - reconstruction_loss: 2.2688 - kl_loss: 5.8450 - lr: 1.9531e-06\n",
      "Epoch 141/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4554 - reconstruction_loss: 2.2742 - kl_loss: 5.8442 - lr: 1.9531e-06\n",
      "Epoch 142/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.4542 - reconstruction_loss: 2.2788 - kl_loss: 5.8455\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4540 - reconstruction_loss: 2.2725 - kl_loss: 5.8458 - lr: 1.9531e-06\n",
      "Epoch 143/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4539 - reconstruction_loss: 2.2717 - kl_loss: 5.8451 - lr: 1.0000e-06\n",
      "Epoch 144/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4505 - reconstruction_loss: 2.2734 - kl_loss: 5.8452 - lr: 1.0000e-06\n",
      "Epoch 145/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4460 - reconstruction_loss: 2.2674 - kl_loss: 5.8449 - lr: 1.0000e-06\n",
      "Epoch 146/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4483 - reconstruction_loss: 2.2718 - kl_loss: 5.8419 - lr: 1.0000e-06\n",
      "Epoch 147/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4572 - reconstruction_loss: 2.2695 - kl_loss: 5.8419 - lr: 1.0000e-06\n",
      "Epoch 148/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4324 - reconstruction_loss: 2.2674 - kl_loss: 5.8401 - lr: 1.0000e-06\n",
      "Epoch 149/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4424 - reconstruction_loss: 2.2735 - kl_loss: 5.8402 - lr: 1.0000e-06\n",
      "Epoch 150/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4424 - reconstruction_loss: 2.2701 - kl_loss: 5.8420 - lr: 1.0000e-06\n",
      "Epoch 151/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4612 - reconstruction_loss: 2.2719 - kl_loss: 5.8410 - lr: 1.0000e-06\n",
      "Epoch 152/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4510 - reconstruction_loss: 2.2690 - kl_loss: 5.8385 - lr: 1.0000e-06\n",
      "Epoch 153/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4433 - reconstruction_loss: 2.2702 - kl_loss: 5.8380 - lr: 1.0000e-06\n",
      "Epoch 154/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4457 - reconstruction_loss: 2.2684 - kl_loss: 5.8386 - lr: 1.0000e-06\n",
      "Epoch 155/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4508 - reconstruction_loss: 2.2705 - kl_loss: 5.8402 - lr: 1.0000e-06\n",
      "Epoch 156/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4515 - reconstruction_loss: 2.2740 - kl_loss: 5.8406 - lr: 1.0000e-06\n",
      "Epoch 157/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4547 - reconstruction_loss: 2.2701 - kl_loss: 5.8427 - lr: 1.0000e-06\n",
      "Epoch 158/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4501 - reconstruction_loss: 2.2679 - kl_loss: 5.8415 - lr: 1.0000e-06\n",
      "Epoch 158: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 14:52:04,508]\u001b[0m Trial 62 finished with value: 0.004446477181296346 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 24, 'encoder_units_l2': 710, 'encoder_units_l3': 885, 'decoder_layers': 4, 'decoder_units_l1': 111, 'decoder_units_l2': 36, 'decoder_units_l3': 1008, 'decoder_units_l4': 30, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 7ms/step - loss: 13.1097 - reconstruction_loss: 9.0110 - kl_loss: 3.8099 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.0029 - reconstruction_loss: 5.4770 - kl_loss: 4.7720 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.6390 - reconstruction_loss: 4.4841 - kl_loss: 5.1219 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.2483 - reconstruction_loss: 4.1226 - kl_loss: 5.2730 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.0024 - reconstruction_loss: 3.8594 - kl_loss: 5.4311 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.8139 - reconstruction_loss: 3.6559 - kl_loss: 5.5515 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.7435 - reconstruction_loss: 3.5396 - kl_loss: 5.6024 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 4.5957 - reconstruction_loss: 3.4449 - kl_loss: 5.6294 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.4991 - reconstruction_loss: 3.3283 - kl_loss: 5.6625 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.4119 - reconstruction_loss: 3.2637 - kl_loss: 5.6672 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.3780 - reconstruction_loss: 3.1919 - kl_loss: 5.7110 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.3038 - reconstruction_loss: 3.1253 - kl_loss: 5.7680 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.3150 - reconstruction_loss: 3.0739 - kl_loss: 5.7805 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.1746 - reconstruction_loss: 3.0017 - kl_loss: 5.8040 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.1031 - reconstruction_loss: 2.9253 - kl_loss: 5.8194 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.1340 - reconstruction_loss: 2.9604 - kl_loss: 5.8029 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0704 - reconstruction_loss: 2.8889 - kl_loss: 5.8156 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0560 - reconstruction_loss: 2.8309 - kl_loss: 5.8485 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0330 - reconstruction_loss: 2.8650 - kl_loss: 5.8557 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9834 - reconstruction_loss: 2.7847 - kl_loss: 5.8422 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9289 - reconstruction_loss: 2.7526 - kl_loss: 5.8131 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9239 - reconstruction_loss: 2.7533 - kl_loss: 5.8243 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9200 - reconstruction_loss: 2.7614 - kl_loss: 5.8051 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9222 - reconstruction_loss: 2.7082 - kl_loss: 5.8257 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8589 - reconstruction_loss: 2.6885 - kl_loss: 5.8321 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8829 - reconstruction_loss: 2.7185 - kl_loss: 5.7950 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8708 - reconstruction_loss: 2.6937 - kl_loss: 5.8259 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8197 - reconstruction_loss: 2.6415 - kl_loss: 5.8249 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8263 - reconstruction_loss: 2.6456 - kl_loss: 5.8076 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8118 - reconstruction_loss: 2.6131 - kl_loss: 5.8377 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7957 - reconstruction_loss: 2.6192 - kl_loss: 5.8368 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7959 - reconstruction_loss: 2.5956 - kl_loss: 5.8315 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7779 - reconstruction_loss: 2.6064 - kl_loss: 5.8351 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7554 - reconstruction_loss: 2.5916 - kl_loss: 5.8751 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.7773 - reconstruction_loss: 2.6149 - kl_loss: 5.8844\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7776 - reconstruction_loss: 2.6066 - kl_loss: 5.8824 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7522 - reconstruction_loss: 2.5378 - kl_loss: 5.8553 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7151 - reconstruction_loss: 2.5105 - kl_loss: 5.8601 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6482 - reconstruction_loss: 2.4797 - kl_loss: 5.8461 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6713 - reconstruction_loss: 2.4960 - kl_loss: 5.8554 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6326 - reconstruction_loss: 2.4555 - kl_loss: 5.8477 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6413 - reconstruction_loss: 2.4637 - kl_loss: 5.8421 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6187 - reconstruction_loss: 2.4546 - kl_loss: 5.8317 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6409 - reconstruction_loss: 2.4752 - kl_loss: 5.8288 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6164 - reconstruction_loss: 2.4419 - kl_loss: 5.8218 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6016 - reconstruction_loss: 2.4322 - kl_loss: 5.8323 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6051 - reconstruction_loss: 2.4346 - kl_loss: 5.8317 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6053 - reconstruction_loss: 2.4379 - kl_loss: 5.8450 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6011 - reconstruction_loss: 2.4211 - kl_loss: 5.8246 - lr: 5.0000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6010 - reconstruction_loss: 2.4137 - kl_loss: 5.8340 - lr: 5.0000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6100 - reconstruction_loss: 2.4327 - kl_loss: 5.8242 - lr: 5.0000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6192 - reconstruction_loss: 2.4219 - kl_loss: 5.8328 - lr: 5.0000e-04\n",
      "Epoch 52/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.6134 - reconstruction_loss: 2.4326 - kl_loss: 5.8337\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6129 - reconstruction_loss: 2.4246 - kl_loss: 5.8342 - lr: 5.0000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5747 - reconstruction_loss: 2.4097 - kl_loss: 5.8470 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5648 - reconstruction_loss: 2.3764 - kl_loss: 5.8259 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5406 - reconstruction_loss: 2.3652 - kl_loss: 5.8255 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5289 - reconstruction_loss: 2.3688 - kl_loss: 5.8303 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5502 - reconstruction_loss: 2.3887 - kl_loss: 5.8541 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.5448 - reconstruction_loss: 2.3792 - kl_loss: 5.8323\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5447 - reconstruction_loss: 2.3718 - kl_loss: 5.8327 - lr: 2.5000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5394 - reconstruction_loss: 2.3605 - kl_loss: 5.8265 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5219 - reconstruction_loss: 2.3441 - kl_loss: 5.8368 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5057 - reconstruction_loss: 2.3444 - kl_loss: 5.8303 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5231 - reconstruction_loss: 2.3363 - kl_loss: 5.8253 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5165 - reconstruction_loss: 2.3426 - kl_loss: 5.8216 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5192 - reconstruction_loss: 2.3371 - kl_loss: 5.8276 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5103 - reconstruction_loss: 2.3311 - kl_loss: 5.8242 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4885 - reconstruction_loss: 2.3258 - kl_loss: 5.8207 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4966 - reconstruction_loss: 2.3247 - kl_loss: 5.8314 - lr: 1.2500e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4877 - reconstruction_loss: 2.3142 - kl_loss: 5.8212 - lr: 1.2500e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4949 - reconstruction_loss: 2.3150 - kl_loss: 5.8272 - lr: 1.2500e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4734 - reconstruction_loss: 2.3213 - kl_loss: 5.8270 - lr: 1.2500e-04\n",
      "Epoch 71/300\n",
      "189/196 [===========================>..] - ETA: 0s - loss: 3.5007 - reconstruction_loss: 2.3356 - kl_loss: 5.8295\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5007 - reconstruction_loss: 2.3279 - kl_loss: 5.8301 - lr: 1.2500e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4850 - reconstruction_loss: 2.3107 - kl_loss: 5.8337 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4697 - reconstruction_loss: 2.3045 - kl_loss: 5.8353 - lr: 6.2500e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4714 - reconstruction_loss: 2.3017 - kl_loss: 5.8339 - lr: 6.2500e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4743 - reconstruction_loss: 2.3030 - kl_loss: 5.8228 - lr: 6.2500e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4803 - reconstruction_loss: 2.2977 - kl_loss: 5.8272 - lr: 6.2500e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4641 - reconstruction_loss: 2.2941 - kl_loss: 5.8232 - lr: 6.2500e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4569 - reconstruction_loss: 2.3025 - kl_loss: 5.8259 - lr: 6.2500e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4727 - reconstruction_loss: 2.3037 - kl_loss: 5.8285 - lr: 6.2500e-05\n",
      "Epoch 80/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4852 - reconstruction_loss: 2.3082 - kl_loss: 5.8359\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4850 - reconstruction_loss: 2.3000 - kl_loss: 5.8357 - lr: 6.2500e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4637 - reconstruction_loss: 2.2916 - kl_loss: 5.8203 - lr: 3.1250e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4663 - reconstruction_loss: 2.2910 - kl_loss: 5.8298 - lr: 3.1250e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4690 - reconstruction_loss: 2.2883 - kl_loss: 5.8312 - lr: 3.1250e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4620 - reconstruction_loss: 2.2888 - kl_loss: 5.8340 - lr: 3.1250e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4712 - reconstruction_loss: 2.2892 - kl_loss: 5.8330 - lr: 3.1250e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4663 - reconstruction_loss: 2.2878 - kl_loss: 5.8301 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4492 - reconstruction_loss: 2.2838 - kl_loss: 5.8375 - lr: 3.1250e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4616 - reconstruction_loss: 2.2862 - kl_loss: 5.8420 - lr: 3.1250e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4492 - reconstruction_loss: 2.2835 - kl_loss: 5.8269 - lr: 3.1250e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4517 - reconstruction_loss: 2.2855 - kl_loss: 5.8314 - lr: 3.1250e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4611 - reconstruction_loss: 2.2841 - kl_loss: 5.8351 - lr: 3.1250e-05\n",
      "Epoch 92/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4754 - reconstruction_loss: 2.2897 - kl_loss: 5.8366\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4750 - reconstruction_loss: 2.2823 - kl_loss: 5.8365 - lr: 3.1250e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4566 - reconstruction_loss: 2.2808 - kl_loss: 5.8214 - lr: 1.5625e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4491 - reconstruction_loss: 2.2785 - kl_loss: 5.8284 - lr: 1.5625e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4459 - reconstruction_loss: 2.2768 - kl_loss: 5.8304 - lr: 1.5625e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4510 - reconstruction_loss: 2.2786 - kl_loss: 5.8249 - lr: 1.5625e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4491 - reconstruction_loss: 2.2766 - kl_loss: 5.8329 - lr: 1.5625e-05\n",
      "Epoch 98/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.4482 - reconstruction_loss: 2.2848 - kl_loss: 5.8379\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4483 - reconstruction_loss: 2.2767 - kl_loss: 5.8376 - lr: 1.5625e-05\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4451 - reconstruction_loss: 2.2732 - kl_loss: 5.8267 - lr: 7.8125e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4651 - reconstruction_loss: 2.2716 - kl_loss: 5.8315 - lr: 7.8125e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4535 - reconstruction_loss: 2.2749 - kl_loss: 5.8265 - lr: 7.8125e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4374 - reconstruction_loss: 2.2720 - kl_loss: 5.8266 - lr: 7.8125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4476 - reconstruction_loss: 2.2716 - kl_loss: 5.8315 - lr: 7.8125e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4379 - reconstruction_loss: 2.2721 - kl_loss: 5.8300 - lr: 7.8125e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4405 - reconstruction_loss: 2.2720 - kl_loss: 5.8263 - lr: 7.8125e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4537 - reconstruction_loss: 2.2726 - kl_loss: 5.8232 - lr: 7.8125e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4537 - reconstruction_loss: 2.2719 - kl_loss: 5.8267 - lr: 7.8125e-06\n",
      "Epoch 108/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.4339 - reconstruction_loss: 2.2796 - kl_loss: 5.8303\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4342 - reconstruction_loss: 2.2716 - kl_loss: 5.8314 - lr: 7.8125e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4494 - reconstruction_loss: 2.2707 - kl_loss: 5.8321 - lr: 3.9063e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4357 - reconstruction_loss: 2.2711 - kl_loss: 5.8348 - lr: 3.9063e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4392 - reconstruction_loss: 2.2690 - kl_loss: 5.8349 - lr: 3.9063e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4370 - reconstruction_loss: 2.2697 - kl_loss: 5.8308 - lr: 3.9063e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4429 - reconstruction_loss: 2.2707 - kl_loss: 5.8347 - lr: 3.9063e-06\n",
      "Epoch 114/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.4521 - reconstruction_loss: 2.2779 - kl_loss: 5.8345\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4518 - reconstruction_loss: 2.2688 - kl_loss: 5.8354 - lr: 3.9063e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4295 - reconstruction_loss: 2.2674 - kl_loss: 5.8310 - lr: 1.9531e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4637 - reconstruction_loss: 2.2711 - kl_loss: 5.8323 - lr: 1.9531e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4544 - reconstruction_loss: 2.2704 - kl_loss: 5.8367 - lr: 1.9531e-06\n",
      "Epoch 118/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.4491 - reconstruction_loss: 2.2761 - kl_loss: 5.8349\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4489 - reconstruction_loss: 2.2680 - kl_loss: 5.8346 - lr: 1.9531e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4423 - reconstruction_loss: 2.2668 - kl_loss: 5.8338 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4549 - reconstruction_loss: 2.2697 - kl_loss: 5.8346 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4439 - reconstruction_loss: 2.2699 - kl_loss: 5.8359 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4333 - reconstruction_loss: 2.2667 - kl_loss: 5.8353 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4457 - reconstruction_loss: 2.2674 - kl_loss: 5.8344 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4483 - reconstruction_loss: 2.2709 - kl_loss: 5.8353 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4344 - reconstruction_loss: 2.2676 - kl_loss: 5.8353 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4451 - reconstruction_loss: 2.2672 - kl_loss: 5.8367 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4303 - reconstruction_loss: 2.2697 - kl_loss: 5.8350 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4476 - reconstruction_loss: 2.2667 - kl_loss: 5.8359 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4296 - reconstruction_loss: 2.2671 - kl_loss: 5.8347 - lr: 1.0000e-06\n",
      "Epoch 129: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 14:55:09,713]\u001b[0m Trial 63 finished with value: 0.004448370314765113 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 26, 'encoder_units_l2': 732, 'encoder_units_l3': 716, 'decoder_layers': 4, 'decoder_units_l1': 117, 'decoder_units_l2': 34, 'decoder_units_l3': 949, 'decoder_units_l4': 32, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 7ms/step - loss: 13.5236 - reconstruction_loss: 9.3024 - kl_loss: 3.7492 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.8467 - reconstruction_loss: 6.5705 - kl_loss: 4.5051 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 6.4034 - reconstruction_loss: 5.1142 - kl_loss: 5.1142 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.6816 - reconstruction_loss: 4.5148 - kl_loss: 5.3389 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.2858 - reconstruction_loss: 4.1036 - kl_loss: 5.5592 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.1324 - reconstruction_loss: 3.9100 - kl_loss: 5.5915 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.9297 - reconstruction_loss: 3.7803 - kl_loss: 5.6374 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.8129 - reconstruction_loss: 3.6381 - kl_loss: 5.7039 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.7425 - reconstruction_loss: 3.5353 - kl_loss: 5.7205 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.6073 - reconstruction_loss: 3.4257 - kl_loss: 5.7633 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.5316 - reconstruction_loss: 3.3523 - kl_loss: 5.7918 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.4600 - reconstruction_loss: 3.2861 - kl_loss: 5.8175 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.3990 - reconstruction_loss: 3.1809 - kl_loss: 5.8500 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.3273 - reconstruction_loss: 3.1405 - kl_loss: 5.8547 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.3364 - reconstruction_loss: 3.1242 - kl_loss: 5.8729 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.2836 - reconstruction_loss: 3.0715 - kl_loss: 5.8892 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.2261 - reconstruction_loss: 3.0306 - kl_loss: 5.8698 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.1965 - reconstruction_loss: 2.9852 - kl_loss: 5.8805 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.1056 - reconstruction_loss: 2.9196 - kl_loss: 5.9066 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0886 - reconstruction_loss: 2.8967 - kl_loss: 5.9167 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.1113 - reconstruction_loss: 2.8718 - kl_loss: 5.9239 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0600 - reconstruction_loss: 2.8873 - kl_loss: 5.9324 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0624 - reconstruction_loss: 2.8566 - kl_loss: 5.9124 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0074 - reconstruction_loss: 2.8023 - kl_loss: 5.9012 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9932 - reconstruction_loss: 2.7991 - kl_loss: 5.9203 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9464 - reconstruction_loss: 2.7683 - kl_loss: 5.9093 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0054 - reconstruction_loss: 2.7869 - kl_loss: 5.9272 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9537 - reconstruction_loss: 2.7534 - kl_loss: 5.9285 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9585 - reconstruction_loss: 2.7578 - kl_loss: 5.9162 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9745 - reconstruction_loss: 2.7602 - kl_loss: 5.9525 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9632 - reconstruction_loss: 2.7518 - kl_loss: 5.9356 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9540 - reconstruction_loss: 2.7407 - kl_loss: 5.9549 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9059 - reconstruction_loss: 2.7041 - kl_loss: 5.9244 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9841 - reconstruction_loss: 2.7360 - kl_loss: 5.9094 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8905 - reconstruction_loss: 2.7019 - kl_loss: 5.8930 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8437 - reconstruction_loss: 2.6583 - kl_loss: 5.8754 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8433 - reconstruction_loss: 2.6732 - kl_loss: 5.9077 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8224 - reconstruction_loss: 2.6308 - kl_loss: 5.9290 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8053 - reconstruction_loss: 2.6265 - kl_loss: 5.8918 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7942 - reconstruction_loss: 2.6288 - kl_loss: 5.8688 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8173 - reconstruction_loss: 2.6590 - kl_loss: 5.9040 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8218 - reconstruction_loss: 2.6840 - kl_loss: 5.9307 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "189/196 [===========================>..] - ETA: 0s - loss: 3.8904 - reconstruction_loss: 2.6743 - kl_loss: 5.8829\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8887 - reconstruction_loss: 2.6636 - kl_loss: 5.8801 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7395 - reconstruction_loss: 2.5546 - kl_loss: 5.8495 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7046 - reconstruction_loss: 2.5237 - kl_loss: 5.8443 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6841 - reconstruction_loss: 2.4995 - kl_loss: 5.8510 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7170 - reconstruction_loss: 2.5259 - kl_loss: 5.8628 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6982 - reconstruction_loss: 2.5024 - kl_loss: 5.8375 - lr: 5.0000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6638 - reconstruction_loss: 2.4935 - kl_loss: 5.8287 - lr: 5.0000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6739 - reconstruction_loss: 2.4913 - kl_loss: 5.8404 - lr: 5.0000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6739 - reconstruction_loss: 2.4966 - kl_loss: 5.8297 - lr: 5.0000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6826 - reconstruction_loss: 2.4846 - kl_loss: 5.8374 - lr: 5.0000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6614 - reconstruction_loss: 2.4765 - kl_loss: 5.8469 - lr: 5.0000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6698 - reconstruction_loss: 2.4884 - kl_loss: 5.8706 - lr: 5.0000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7000 - reconstruction_loss: 2.4896 - kl_loss: 5.8942 - lr: 5.0000e-04\n",
      "Epoch 56/300\n",
      "189/196 [===========================>..] - ETA: 0s - loss: 3.6461 - reconstruction_loss: 2.4865 - kl_loss: 5.8593\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6465 - reconstruction_loss: 2.4750 - kl_loss: 5.8611 - lr: 5.0000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6296 - reconstruction_loss: 2.4446 - kl_loss: 5.8490 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6039 - reconstruction_loss: 2.4350 - kl_loss: 5.8497 - lr: 2.5000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6009 - reconstruction_loss: 2.4209 - kl_loss: 5.8470 - lr: 2.5000e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5888 - reconstruction_loss: 2.4106 - kl_loss: 5.8446 - lr: 2.5000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5909 - reconstruction_loss: 2.4265 - kl_loss: 5.8684 - lr: 2.5000e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6484 - reconstruction_loss: 2.4653 - kl_loss: 5.8911 - lr: 2.5000e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.6274 - reconstruction_loss: 2.4398 - kl_loss: 5.8854\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6274 - reconstruction_loss: 2.4398 - kl_loss: 5.8854 - lr: 2.5000e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5966 - reconstruction_loss: 2.4162 - kl_loss: 5.8711 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5994 - reconstruction_loss: 2.4167 - kl_loss: 5.9023 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.6286 - reconstruction_loss: 2.4442 - kl_loss: 5.8862\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6282 - reconstruction_loss: 2.4332 - kl_loss: 5.8860 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6053 - reconstruction_loss: 2.4089 - kl_loss: 5.8936 - lr: 6.2500e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5982 - reconstruction_loss: 2.4105 - kl_loss: 5.8827 - lr: 6.2500e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5733 - reconstruction_loss: 2.3945 - kl_loss: 5.8886 - lr: 6.2500e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5898 - reconstruction_loss: 2.4085 - kl_loss: 5.8937 - lr: 6.2500e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5848 - reconstruction_loss: 2.4067 - kl_loss: 5.8986 - lr: 6.2500e-05\n",
      "Epoch 72/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5826 - reconstruction_loss: 2.4088 - kl_loss: 5.8932\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5826 - reconstruction_loss: 2.4003 - kl_loss: 5.8930 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5777 - reconstruction_loss: 2.3902 - kl_loss: 5.8894 - lr: 3.1250e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5719 - reconstruction_loss: 2.3829 - kl_loss: 5.8771 - lr: 3.1250e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5720 - reconstruction_loss: 2.3828 - kl_loss: 5.8743 - lr: 3.1250e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5613 - reconstruction_loss: 2.3756 - kl_loss: 5.8837 - lr: 3.1250e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5637 - reconstruction_loss: 2.3730 - kl_loss: 5.8808 - lr: 3.1250e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5587 - reconstruction_loss: 2.3728 - kl_loss: 5.8823 - lr: 3.1250e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5628 - reconstruction_loss: 2.3690 - kl_loss: 5.8793 - lr: 3.1250e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5432 - reconstruction_loss: 2.3682 - kl_loss: 5.8890 - lr: 3.1250e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5403 - reconstruction_loss: 2.3658 - kl_loss: 5.8779 - lr: 3.1250e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5374 - reconstruction_loss: 2.3635 - kl_loss: 5.8586 - lr: 3.1250e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5393 - reconstruction_loss: 2.3582 - kl_loss: 5.8907 - lr: 3.1250e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5391 - reconstruction_loss: 2.3570 - kl_loss: 5.8758 - lr: 3.1250e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5438 - reconstruction_loss: 2.3584 - kl_loss: 5.8864 - lr: 3.1250e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5386 - reconstruction_loss: 2.3571 - kl_loss: 5.8730 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5435 - reconstruction_loss: 2.3609 - kl_loss: 5.8769 - lr: 3.1250e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5436 - reconstruction_loss: 2.3598 - kl_loss: 5.8831 - lr: 3.1250e-05\n",
      "Epoch 89/300\n",
      "189/196 [===========================>..] - ETA: 0s - loss: 3.5432 - reconstruction_loss: 2.3643 - kl_loss: 5.8774\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5430 - reconstruction_loss: 2.3569 - kl_loss: 5.8773 - lr: 3.1250e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5360 - reconstruction_loss: 2.3494 - kl_loss: 5.8928 - lr: 1.5625e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5356 - reconstruction_loss: 2.3503 - kl_loss: 5.8844 - lr: 1.5625e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5212 - reconstruction_loss: 2.3494 - kl_loss: 5.8754 - lr: 1.5625e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5302 - reconstruction_loss: 2.3489 - kl_loss: 5.8810 - lr: 1.5625e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5313 - reconstruction_loss: 2.3491 - kl_loss: 5.8808 - lr: 1.5625e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5333 - reconstruction_loss: 2.3447 - kl_loss: 5.8805 - lr: 1.5625e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5436 - reconstruction_loss: 2.3475 - kl_loss: 5.8738 - lr: 1.5625e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5251 - reconstruction_loss: 2.3515 - kl_loss: 5.8728 - lr: 1.5625e-05\n",
      "Epoch 98/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.5259 - reconstruction_loss: 2.3560 - kl_loss: 5.8862\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5260 - reconstruction_loss: 2.3475 - kl_loss: 5.8856 - lr: 1.5625e-05\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5356 - reconstruction_loss: 2.3433 - kl_loss: 5.8926 - lr: 7.8125e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5351 - reconstruction_loss: 2.3452 - kl_loss: 5.8939 - lr: 7.8125e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5145 - reconstruction_loss: 2.3430 - kl_loss: 5.8835 - lr: 7.8125e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5321 - reconstruction_loss: 2.3456 - kl_loss: 5.8883 - lr: 7.8125e-06\n",
      "Epoch 103/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5345 - reconstruction_loss: 2.3436 - kl_loss: 5.8848 - lr: 7.8125e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5209 - reconstruction_loss: 2.3430 - kl_loss: 5.8730 - lr: 7.8125e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5327 - reconstruction_loss: 2.3409 - kl_loss: 5.8847 - lr: 7.8125e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5205 - reconstruction_loss: 2.3421 - kl_loss: 5.8775 - lr: 7.8125e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5252 - reconstruction_loss: 2.3404 - kl_loss: 5.8878\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5252 - reconstruction_loss: 2.3404 - kl_loss: 5.8878 - lr: 7.8125e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5261 - reconstruction_loss: 2.3391 - kl_loss: 5.8862 - lr: 3.9063e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5236 - reconstruction_loss: 2.3398 - kl_loss: 5.8871 - lr: 3.9063e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5309 - reconstruction_loss: 2.3359 - kl_loss: 5.8904 - lr: 3.9063e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5268 - reconstruction_loss: 2.3401 - kl_loss: 5.8864 - lr: 3.9063e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5253 - reconstruction_loss: 2.3376 - kl_loss: 5.8847 - lr: 3.9063e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5147 - reconstruction_loss: 2.3364 - kl_loss: 5.8803 - lr: 3.9063e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5354 - reconstruction_loss: 2.3393 - kl_loss: 5.8821 - lr: 3.9063e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5173 - reconstruction_loss: 2.3387 - kl_loss: 5.8910 - lr: 3.9063e-06\n",
      "Epoch 116/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.5386 - reconstruction_loss: 2.3472 - kl_loss: 5.8882\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5380 - reconstruction_loss: 2.3400 - kl_loss: 5.8887 - lr: 3.9063e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5166 - reconstruction_loss: 2.3344 - kl_loss: 5.8930 - lr: 1.9531e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5229 - reconstruction_loss: 2.3367 - kl_loss: 5.8876 - lr: 1.9531e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5190 - reconstruction_loss: 2.3343 - kl_loss: 5.8874 - lr: 1.9531e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5303 - reconstruction_loss: 2.3390 - kl_loss: 5.8842 - lr: 1.9531e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5294 - reconstruction_loss: 2.3366 - kl_loss: 5.8820 - lr: 1.9531e-06\n",
      "Epoch 122/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.5369 - reconstruction_loss: 2.3455 - kl_loss: 5.8810\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5364 - reconstruction_loss: 2.3382 - kl_loss: 5.8807 - lr: 1.9531e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5312 - reconstruction_loss: 2.3382 - kl_loss: 5.8803 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5254 - reconstruction_loss: 2.3390 - kl_loss: 5.8839 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5269 - reconstruction_loss: 2.3348 - kl_loss: 5.8859 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5143 - reconstruction_loss: 2.3349 - kl_loss: 5.8874 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5232 - reconstruction_loss: 2.3383 - kl_loss: 5.8875 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5160 - reconstruction_loss: 2.3371 - kl_loss: 5.8869 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5233 - reconstruction_loss: 2.3339 - kl_loss: 5.8858 - lr: 1.0000e-06\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5171 - reconstruction_loss: 2.3369 - kl_loss: 5.8847 - lr: 1.0000e-06\n",
      "Epoch 131/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5245 - reconstruction_loss: 2.3366 - kl_loss: 5.8849 - lr: 1.0000e-06\n",
      "Epoch 132/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5214 - reconstruction_loss: 2.3344 - kl_loss: 5.8841 - lr: 1.0000e-06\n",
      "Epoch 133/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5257 - reconstruction_loss: 2.3360 - kl_loss: 5.8821 - lr: 1.0000e-06\n",
      "Epoch 134/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5180 - reconstruction_loss: 2.3367 - kl_loss: 5.8809 - lr: 1.0000e-06\n",
      "Epoch 135/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5187 - reconstruction_loss: 2.3373 - kl_loss: 5.8796 - lr: 1.0000e-06\n",
      "Epoch 136/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5265 - reconstruction_loss: 2.3401 - kl_loss: 5.8795 - lr: 1.0000e-06\n",
      "Epoch 137/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5361 - reconstruction_loss: 2.3347 - kl_loss: 5.8812 - lr: 1.0000e-06\n",
      "Epoch 138/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5194 - reconstruction_loss: 2.3380 - kl_loss: 5.8818 - lr: 1.0000e-06\n",
      "Epoch 139/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5157 - reconstruction_loss: 2.3380 - kl_loss: 5.8820 - lr: 1.0000e-06\n",
      "Epoch 140/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5299 - reconstruction_loss: 2.3417 - kl_loss: 5.8820 - lr: 1.0000e-06\n",
      "Epoch 141/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5204 - reconstruction_loss: 2.3374 - kl_loss: 5.8826 - lr: 1.0000e-06\n",
      "Epoch 142/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5207 - reconstruction_loss: 2.3363 - kl_loss: 5.8840 - lr: 1.0000e-06\n",
      "Epoch 143/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5355 - reconstruction_loss: 2.3370 - kl_loss: 5.8844 - lr: 1.0000e-06\n",
      "Epoch 144/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5159 - reconstruction_loss: 2.3343 - kl_loss: 5.8854 - lr: 1.0000e-06\n",
      "Epoch 145/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5052 - reconstruction_loss: 2.3350 - kl_loss: 5.8868 - lr: 1.0000e-06\n",
      "Epoch 146/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5218 - reconstruction_loss: 2.3357 - kl_loss: 5.8843 - lr: 1.0000e-06\n",
      "Epoch 147/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5192 - reconstruction_loss: 2.3380 - kl_loss: 5.8852 - lr: 1.0000e-06\n",
      "Epoch 147: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 14:58:33,531]\u001b[0m Trial 64 finished with value: 0.004578703720047682 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 26, 'encoder_units_l2': 728, 'encoder_units_l3': 735, 'decoder_layers': 4, 'decoder_units_l1': 106, 'decoder_units_l2': 37, 'decoder_units_l3': 996, 'decoder_units_l4': 30, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 7ms/step - loss: 13.1272 - reconstruction_loss: 8.9818 - kl_loss: 3.7994 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.4417 - reconstruction_loss: 6.0783 - kl_loss: 4.6752 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.9584 - reconstruction_loss: 4.7018 - kl_loss: 5.1649 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.4238 - reconstruction_loss: 4.2248 - kl_loss: 5.3623 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.1120 - reconstruction_loss: 3.8996 - kl_loss: 5.5898 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.8689 - reconstruction_loss: 3.6830 - kl_loss: 5.6972 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.7612 - reconstruction_loss: 3.5672 - kl_loss: 5.7298 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.6119 - reconstruction_loss: 3.4452 - kl_loss: 5.7978 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.5790 - reconstruction_loss: 3.3608 - kl_loss: 5.8346 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.4946 - reconstruction_loss: 3.2792 - kl_loss: 5.8848 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.3813 - reconstruction_loss: 3.1773 - kl_loss: 5.9039 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.3261 - reconstruction_loss: 3.1350 - kl_loss: 5.9254 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.2569 - reconstruction_loss: 3.0486 - kl_loss: 5.9314 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.2690 - reconstruction_loss: 3.0244 - kl_loss: 5.9518 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0739 - reconstruction_loss: 2.8752 - kl_loss: 5.9433 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0407 - reconstruction_loss: 2.8276 - kl_loss: 5.9383 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9934 - reconstruction_loss: 2.8079 - kl_loss: 5.9117 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9782 - reconstruction_loss: 2.8002 - kl_loss: 5.9387 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9923 - reconstruction_loss: 2.7541 - kl_loss: 5.9321 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9755 - reconstruction_loss: 2.7718 - kl_loss: 5.9271 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0115 - reconstruction_loss: 2.7650 - kl_loss: 5.9429 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9277 - reconstruction_loss: 2.6803 - kl_loss: 5.8761 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8473 - reconstruction_loss: 2.6326 - kl_loss: 5.8916 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8275 - reconstruction_loss: 2.6372 - kl_loss: 5.8663 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8513 - reconstruction_loss: 2.5996 - kl_loss: 5.8565 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7728 - reconstruction_loss: 2.5844 - kl_loss: 5.8662 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7044 - reconstruction_loss: 2.5585 - kl_loss: 5.8287 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7248 - reconstruction_loss: 2.5515 - kl_loss: 5.8165 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6990 - reconstruction_loss: 2.5205 - kl_loss: 5.8094 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7027 - reconstruction_loss: 2.5237 - kl_loss: 5.8066 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7013 - reconstruction_loss: 2.5515 - kl_loss: 5.8197 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.7676 - reconstruction_loss: 2.5826 - kl_loss: 5.7867\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7670 - reconstruction_loss: 2.5732 - kl_loss: 5.7876 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6311 - reconstruction_loss: 2.4663 - kl_loss: 5.7935 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6059 - reconstruction_loss: 2.4595 - kl_loss: 5.8147 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6428 - reconstruction_loss: 2.4592 - kl_loss: 5.7902 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6006 - reconstruction_loss: 2.4337 - kl_loss: 5.7724 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5996 - reconstruction_loss: 2.4615 - kl_loss: 5.7484 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6105 - reconstruction_loss: 2.4390 - kl_loss: 5.7557 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.6137 - reconstruction_loss: 2.4648 - kl_loss: 5.7679\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6137 - reconstruction_loss: 2.4568 - kl_loss: 5.7680 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5686 - reconstruction_loss: 2.4209 - kl_loss: 5.7549 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5601 - reconstruction_loss: 2.4048 - kl_loss: 5.7425 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5530 - reconstruction_loss: 2.4007 - kl_loss: 5.7217 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5510 - reconstruction_loss: 2.4007 - kl_loss: 5.7382 - lr: 2.5000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5465 - reconstruction_loss: 2.4032 - kl_loss: 5.7492 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5603 - reconstruction_loss: 2.4349 - kl_loss: 5.7513\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5605 - reconstruction_loss: 2.4265 - kl_loss: 5.7510 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5593 - reconstruction_loss: 2.4042 - kl_loss: 5.7356 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5445 - reconstruction_loss: 2.3904 - kl_loss: 5.7239 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5369 - reconstruction_loss: 2.3893 - kl_loss: 5.7124 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5538 - reconstruction_loss: 2.3923 - kl_loss: 5.7168 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5374 - reconstruction_loss: 2.3855 - kl_loss: 5.7215 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5341 - reconstruction_loss: 2.3821 - kl_loss: 5.7002 - lr: 1.2500e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5259 - reconstruction_loss: 2.3803 - kl_loss: 5.6816 - lr: 1.2500e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5464 - reconstruction_loss: 2.3869 - kl_loss: 5.7064 - lr: 1.2500e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5251 - reconstruction_loss: 2.3854 - kl_loss: 5.7013 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5336 - reconstruction_loss: 2.3848 - kl_loss: 5.7088\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5334 - reconstruction_loss: 2.3754 - kl_loss: 5.7082 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5312 - reconstruction_loss: 2.3763 - kl_loss: 5.7020 - lr: 6.2500e-05\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5218 - reconstruction_loss: 2.3808 - kl_loss: 5.6680 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5255 - reconstruction_loss: 2.3698 - kl_loss: 5.6843 - lr: 6.2500e-05\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5161 - reconstruction_loss: 2.3713 - kl_loss: 5.6764 - lr: 6.2500e-05\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5062 - reconstruction_loss: 2.3674 - kl_loss: 5.6792 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5190 - reconstruction_loss: 2.3630 - kl_loss: 5.6805 - lr: 6.2500e-05\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4973 - reconstruction_loss: 2.3627 - kl_loss: 5.6919 - lr: 6.2500e-05\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4984 - reconstruction_loss: 2.3574 - kl_loss: 5.6799 - lr: 6.2500e-05\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5031 - reconstruction_loss: 2.3581 - kl_loss: 5.6790 - lr: 6.2500e-05\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4971 - reconstruction_loss: 2.3586 - kl_loss: 5.6882 - lr: 6.2500e-05\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4963 - reconstruction_loss: 2.3510 - kl_loss: 5.6796 - lr: 6.2500e-05\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5013 - reconstruction_loss: 2.3537 - kl_loss: 5.6819 - lr: 6.2500e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4932 - reconstruction_loss: 2.3545 - kl_loss: 5.6906 - lr: 6.2500e-05\n",
      "Epoch 69/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.4774 - reconstruction_loss: 2.3625 - kl_loss: 5.6723\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4779 - reconstruction_loss: 2.3544 - kl_loss: 5.6730 - lr: 6.2500e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4968 - reconstruction_loss: 2.3510 - kl_loss: 5.6693 - lr: 3.1250e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4847 - reconstruction_loss: 2.3472 - kl_loss: 5.6804 - lr: 3.1250e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4799 - reconstruction_loss: 2.3486 - kl_loss: 5.6761 - lr: 3.1250e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4978 - reconstruction_loss: 2.3507 - kl_loss: 5.6587 - lr: 3.1250e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4958 - reconstruction_loss: 2.3485 - kl_loss: 5.6730 - lr: 3.1250e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4786 - reconstruction_loss: 2.3465 - kl_loss: 5.6759 - lr: 3.1250e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4864 - reconstruction_loss: 2.3458 - kl_loss: 5.6793 - lr: 3.1250e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4776 - reconstruction_loss: 2.3469 - kl_loss: 5.6787 - lr: 3.1250e-05\n",
      "Epoch 78/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.4872 - reconstruction_loss: 2.3566 - kl_loss: 5.6746\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4872 - reconstruction_loss: 2.3480 - kl_loss: 5.6747 - lr: 3.1250e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4883 - reconstruction_loss: 2.3404 - kl_loss: 5.6693 - lr: 1.5625e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4786 - reconstruction_loss: 2.3414 - kl_loss: 5.6715 - lr: 1.5625e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4806 - reconstruction_loss: 2.3435 - kl_loss: 5.6703 - lr: 1.5625e-05\n",
      "Epoch 82/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4839 - reconstruction_loss: 2.3485 - kl_loss: 5.6781\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4838 - reconstruction_loss: 2.3396 - kl_loss: 5.6786 - lr: 1.5625e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4979 - reconstruction_loss: 2.3402 - kl_loss: 5.6877 - lr: 7.8125e-06\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4835 - reconstruction_loss: 2.3368 - kl_loss: 5.6802 - lr: 7.8125e-06\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4791 - reconstruction_loss: 2.3379 - kl_loss: 5.6714 - lr: 7.8125e-06\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4939 - reconstruction_loss: 2.3392 - kl_loss: 5.6749 - lr: 7.8125e-06\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4789 - reconstruction_loss: 2.3365 - kl_loss: 5.6773 - lr: 7.8125e-06\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4743 - reconstruction_loss: 2.3369 - kl_loss: 5.6739 - lr: 7.8125e-06\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4705 - reconstruction_loss: 2.3383 - kl_loss: 5.6724 - lr: 7.8125e-06\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4797 - reconstruction_loss: 2.3376 - kl_loss: 5.6736 - lr: 7.8125e-06\n",
      "Epoch 91/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4841 - reconstruction_loss: 2.3471 - kl_loss: 5.6738\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4840 - reconstruction_loss: 2.3391 - kl_loss: 5.6734 - lr: 7.8125e-06\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4867 - reconstruction_loss: 2.3377 - kl_loss: 5.6683 - lr: 3.9063e-06\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4798 - reconstruction_loss: 2.3391 - kl_loss: 5.6694 - lr: 3.9063e-06\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4855 - reconstruction_loss: 2.3337 - kl_loss: 5.6824 - lr: 3.9063e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4688 - reconstruction_loss: 2.3362 - kl_loss: 5.6808 - lr: 3.9063e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4766 - reconstruction_loss: 2.3326 - kl_loss: 5.6834 - lr: 3.9063e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4795 - reconstruction_loss: 2.3368 - kl_loss: 5.6744 - lr: 3.9063e-06\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4766 - reconstruction_loss: 2.3364 - kl_loss: 5.6725 - lr: 3.9063e-06\n",
      "Epoch 99/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.4825 - reconstruction_loss: 2.3458 - kl_loss: 5.6766\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4823 - reconstruction_loss: 2.3363 - kl_loss: 5.6768 - lr: 3.9063e-06\n",
      "Epoch 100/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4874 - reconstruction_loss: 2.3339 - kl_loss: 5.6793 - lr: 1.9531e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4833 - reconstruction_loss: 2.3348 - kl_loss: 5.6821 - lr: 1.9531e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4793 - reconstruction_loss: 2.3314 - kl_loss: 5.6798 - lr: 1.9531e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4777 - reconstruction_loss: 2.3362 - kl_loss: 5.6779 - lr: 1.9531e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4866 - reconstruction_loss: 2.3342 - kl_loss: 5.6780 - lr: 1.9531e-06\n",
      "Epoch 105/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.4733 - reconstruction_loss: 2.3424 - kl_loss: 5.6733\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4734 - reconstruction_loss: 2.3346 - kl_loss: 5.6749 - lr: 1.9531e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4886 - reconstruction_loss: 2.3360 - kl_loss: 5.6782 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4612 - reconstruction_loss: 2.3342 - kl_loss: 5.6766 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4663 - reconstruction_loss: 2.3340 - kl_loss: 5.6773 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4786 - reconstruction_loss: 2.3345 - kl_loss: 5.6792 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4677 - reconstruction_loss: 2.3345 - kl_loss: 5.6766 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4721 - reconstruction_loss: 2.3335 - kl_loss: 5.6763 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4885 - reconstruction_loss: 2.3328 - kl_loss: 5.6750 - lr: 1.0000e-06\n",
      "Epoch 112: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 15:01:09,875]\u001b[0m Trial 65 finished with value: 0.004575067037489491 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 26, 'encoder_units_l2': 737, 'encoder_units_l3': 759, 'decoder_layers': 4, 'decoder_units_l1': 111, 'decoder_units_l2': 33, 'decoder_units_l3': 937, 'decoder_units_l4': 30, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 7ms/step - loss: 12.8915 - reconstruction_loss: 8.8224 - kl_loss: 3.8970 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.6735 - reconstruction_loss: 6.2944 - kl_loss: 4.5604 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.9983 - reconstruction_loss: 4.7489 - kl_loss: 5.0991 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.4159 - reconstruction_loss: 4.2131 - kl_loss: 5.3943 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.0705 - reconstruction_loss: 3.8902 - kl_loss: 5.5098 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.9227 - reconstruction_loss: 3.7255 - kl_loss: 5.5816 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.7336 - reconstruction_loss: 3.5792 - kl_loss: 5.6015 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.5924 - reconstruction_loss: 3.4139 - kl_loss: 5.6474 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.5012 - reconstruction_loss: 3.3334 - kl_loss: 5.6538 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.3829 - reconstruction_loss: 3.2169 - kl_loss: 5.6577 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.3015 - reconstruction_loss: 3.1309 - kl_loss: 5.6886 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.2226 - reconstruction_loss: 3.0657 - kl_loss: 5.7315 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.1744 - reconstruction_loss: 2.9952 - kl_loss: 5.7246 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0873 - reconstruction_loss: 2.8970 - kl_loss: 5.7472 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0126 - reconstruction_loss: 2.8478 - kl_loss: 5.7644 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0052 - reconstruction_loss: 2.8183 - kl_loss: 5.7852 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9837 - reconstruction_loss: 2.7896 - kl_loss: 5.7539 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9563 - reconstruction_loss: 2.7688 - kl_loss: 5.7900 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9084 - reconstruction_loss: 2.7289 - kl_loss: 5.7814 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9032 - reconstruction_loss: 2.7535 - kl_loss: 5.7736 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9229 - reconstruction_loss: 2.7279 - kl_loss: 5.8345 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8966 - reconstruction_loss: 2.7044 - kl_loss: 5.8085 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8712 - reconstruction_loss: 2.7063 - kl_loss: 5.8085 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8760 - reconstruction_loss: 2.6477 - kl_loss: 5.7864 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7988 - reconstruction_loss: 2.6160 - kl_loss: 5.7686 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7249 - reconstruction_loss: 2.5792 - kl_loss: 5.7653 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7689 - reconstruction_loss: 2.6096 - kl_loss: 5.7733 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7867 - reconstruction_loss: 2.6319 - kl_loss: 5.7927 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7232 - reconstruction_loss: 2.5509 - kl_loss: 5.7546 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7765 - reconstruction_loss: 2.6329 - kl_loss: 5.7851 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7128 - reconstruction_loss: 2.5831 - kl_loss: 5.7671 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7194 - reconstruction_loss: 2.5430 - kl_loss: 5.7590 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7092 - reconstruction_loss: 2.5515 - kl_loss: 5.7721 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6606 - reconstruction_loss: 2.5009 - kl_loss: 5.7325 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6787 - reconstruction_loss: 2.5303 - kl_loss: 5.7334 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6722 - reconstruction_loss: 2.5209 - kl_loss: 5.7189 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6484 - reconstruction_loss: 2.5036 - kl_loss: 5.6987 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6260 - reconstruction_loss: 2.5092 - kl_loss: 5.7144 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6807 - reconstruction_loss: 2.5322 - kl_loss: 5.7087 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "189/196 [===========================>..] - ETA: 0s - loss: 3.6872 - reconstruction_loss: 2.5323 - kl_loss: 5.7049\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6865 - reconstruction_loss: 2.5223 - kl_loss: 5.7052 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6236 - reconstruction_loss: 2.4595 - kl_loss: 5.7142 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5663 - reconstruction_loss: 2.4215 - kl_loss: 5.7180 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5827 - reconstruction_loss: 2.4260 - kl_loss: 5.7052 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5802 - reconstruction_loss: 2.4262 - kl_loss: 5.7001 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.5716 - reconstruction_loss: 2.4362 - kl_loss: 5.7048\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5717 - reconstruction_loss: 2.4275 - kl_loss: 5.7063 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5529 - reconstruction_loss: 2.3961 - kl_loss: 5.7074 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5429 - reconstruction_loss: 2.3918 - kl_loss: 5.6950 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5496 - reconstruction_loss: 2.3977 - kl_loss: 5.6890 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5136 - reconstruction_loss: 2.3779 - kl_loss: 5.6808 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5242 - reconstruction_loss: 2.3771 - kl_loss: 5.7031 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5127 - reconstruction_loss: 2.3674 - kl_loss: 5.6809 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5248 - reconstruction_loss: 2.3690 - kl_loss: 5.6807 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5066 - reconstruction_loss: 2.3746 - kl_loss: 5.6983 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5128 - reconstruction_loss: 2.3733 - kl_loss: 5.6909\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5127 - reconstruction_loss: 2.3670 - kl_loss: 5.6903 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4855 - reconstruction_loss: 2.3513 - kl_loss: 5.6808 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4954 - reconstruction_loss: 2.3467 - kl_loss: 5.6871 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4886 - reconstruction_loss: 2.3419 - kl_loss: 5.6904 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4743 - reconstruction_loss: 2.3447 - kl_loss: 5.6883 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5036 - reconstruction_loss: 2.3508 - kl_loss: 5.6772 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4817 - reconstruction_loss: 2.3391 - kl_loss: 5.6724 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4888 - reconstruction_loss: 2.3417 - kl_loss: 5.6870 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5166 - reconstruction_loss: 2.3561 - kl_loss: 5.6870 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.5059 - reconstruction_loss: 2.3563 - kl_loss: 5.6747\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5052 - reconstruction_loss: 2.3455 - kl_loss: 5.6741 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4775 - reconstruction_loss: 2.3324 - kl_loss: 5.6773 - lr: 6.2500e-05\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4788 - reconstruction_loss: 2.3361 - kl_loss: 5.6939 - lr: 6.2500e-05\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4805 - reconstruction_loss: 2.3290 - kl_loss: 5.6778 - lr: 6.2500e-05\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4643 - reconstruction_loss: 2.3260 - kl_loss: 5.6759 - lr: 6.2500e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4734 - reconstruction_loss: 2.3281 - kl_loss: 5.6771 - lr: 6.2500e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4634 - reconstruction_loss: 2.3226 - kl_loss: 5.6790 - lr: 6.2500e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4548 - reconstruction_loss: 2.3245 - kl_loss: 5.6780 - lr: 6.2500e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4671 - reconstruction_loss: 2.3242 - kl_loss: 5.6784 - lr: 6.2500e-05\n",
      "Epoch 72/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4661 - reconstruction_loss: 2.3343 - kl_loss: 5.6687\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4661 - reconstruction_loss: 2.3263 - kl_loss: 5.6687 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4514 - reconstruction_loss: 2.3158 - kl_loss: 5.6681 - lr: 3.1250e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4572 - reconstruction_loss: 2.3139 - kl_loss: 5.6746 - lr: 3.1250e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4442 - reconstruction_loss: 2.3108 - kl_loss: 5.6818 - lr: 3.1250e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4570 - reconstruction_loss: 2.3165 - kl_loss: 5.6651 - lr: 3.1250e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4591 - reconstruction_loss: 2.3127 - kl_loss: 5.6676 - lr: 3.1250e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4674 - reconstruction_loss: 2.3177 - kl_loss: 5.6726 - lr: 3.1250e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4548 - reconstruction_loss: 2.3153 - kl_loss: 5.6599 - lr: 3.1250e-05\n",
      "Epoch 80/300\n",
      "189/196 [===========================>..] - ETA: 0s - loss: 3.4582 - reconstruction_loss: 2.3240 - kl_loss: 5.6683\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4582 - reconstruction_loss: 2.3158 - kl_loss: 5.6688 - lr: 3.1250e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4566 - reconstruction_loss: 2.3124 - kl_loss: 5.6716 - lr: 1.5625e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4611 - reconstruction_loss: 2.3108 - kl_loss: 5.6751 - lr: 1.5625e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4559 - reconstruction_loss: 2.3123 - kl_loss: 5.6704 - lr: 1.5625e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4526 - reconstruction_loss: 2.3077 - kl_loss: 5.6749 - lr: 1.5625e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4489 - reconstruction_loss: 2.3101 - kl_loss: 5.6642 - lr: 1.5625e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4645 - reconstruction_loss: 2.3104 - kl_loss: 5.6714 - lr: 1.5625e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4439 - reconstruction_loss: 2.3100 - kl_loss: 5.6614 - lr: 1.5625e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4484 - reconstruction_loss: 2.3105 - kl_loss: 5.6643 - lr: 1.5625e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4632 - reconstruction_loss: 2.3090 - kl_loss: 5.6723 - lr: 1.5625e-05\n",
      "Epoch 90/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.4416 - reconstruction_loss: 2.3181 - kl_loss: 5.6729\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4418 - reconstruction_loss: 2.3102 - kl_loss: 5.6732 - lr: 1.5625e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4483 - reconstruction_loss: 2.3078 - kl_loss: 5.6705 - lr: 7.8125e-06\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4540 - reconstruction_loss: 2.3040 - kl_loss: 5.6705 - lr: 7.8125e-06\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4520 - reconstruction_loss: 2.3048 - kl_loss: 5.6674 - lr: 7.8125e-06\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4475 - reconstruction_loss: 2.3026 - kl_loss: 5.6729 - lr: 7.8125e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4511 - reconstruction_loss: 2.3032 - kl_loss: 5.6739 - lr: 7.8125e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4351 - reconstruction_loss: 2.3037 - kl_loss: 5.6703 - lr: 7.8125e-06\n",
      "Epoch 97/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.4434 - reconstruction_loss: 2.3085 - kl_loss: 5.6833\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4434 - reconstruction_loss: 2.3015 - kl_loss: 5.6828 - lr: 7.8125e-06\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4430 - reconstruction_loss: 2.3001 - kl_loss: 5.6792 - lr: 3.9063e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4438 - reconstruction_loss: 2.3004 - kl_loss: 5.6715 - lr: 3.9063e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4425 - reconstruction_loss: 2.3001 - kl_loss: 5.6686 - lr: 3.9063e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4434 - reconstruction_loss: 2.3013 - kl_loss: 5.6724 - lr: 3.9063e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4458 - reconstruction_loss: 2.3025 - kl_loss: 5.6743 - lr: 3.9063e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.4451 - reconstruction_loss: 2.3079 - kl_loss: 5.6760\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4450 - reconstruction_loss: 2.3001 - kl_loss: 5.6763 - lr: 3.9063e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4445 - reconstruction_loss: 2.3001 - kl_loss: 5.6759 - lr: 1.9531e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4401 - reconstruction_loss: 2.3003 - kl_loss: 5.6745 - lr: 1.9531e-06\n",
      "Epoch 106/300\n",
      "189/196 [===========================>..] - ETA: 0s - loss: 3.4470 - reconstruction_loss: 2.3095 - kl_loss: 5.6777\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4468 - reconstruction_loss: 2.3016 - kl_loss: 5.6780 - lr: 1.9531e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4385 - reconstruction_loss: 2.2988 - kl_loss: 5.6780 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4428 - reconstruction_loss: 2.2974 - kl_loss: 5.6760 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4318 - reconstruction_loss: 2.3002 - kl_loss: 5.6750 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4304 - reconstruction_loss: 2.3000 - kl_loss: 5.6773 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4469 - reconstruction_loss: 2.2993 - kl_loss: 5.6744 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4379 - reconstruction_loss: 2.2998 - kl_loss: 5.6704 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4436 - reconstruction_loss: 2.2982 - kl_loss: 5.6700 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4416 - reconstruction_loss: 2.2994 - kl_loss: 5.6691 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4452 - reconstruction_loss: 2.3003 - kl_loss: 5.6675 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4416 - reconstruction_loss: 2.3001 - kl_loss: 5.6669 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4429 - reconstruction_loss: 2.2998 - kl_loss: 5.6678 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4388 - reconstruction_loss: 2.3008 - kl_loss: 5.6667 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4311 - reconstruction_loss: 2.3001 - kl_loss: 5.6672 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4354 - reconstruction_loss: 2.3012 - kl_loss: 5.6667 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4362 - reconstruction_loss: 2.3005 - kl_loss: 5.6698 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4429 - reconstruction_loss: 2.2974 - kl_loss: 5.6695 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4483 - reconstruction_loss: 2.3021 - kl_loss: 5.6686 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4488 - reconstruction_loss: 2.3004 - kl_loss: 5.6713 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4390 - reconstruction_loss: 2.2991 - kl_loss: 5.6712 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4361 - reconstruction_loss: 2.2999 - kl_loss: 5.6710 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4388 - reconstruction_loss: 2.3007 - kl_loss: 5.6719 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4437 - reconstruction_loss: 2.2994 - kl_loss: 5.6706 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4378 - reconstruction_loss: 2.3009 - kl_loss: 5.6721 - lr: 1.0000e-06\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4442 - reconstruction_loss: 2.3003 - kl_loss: 5.6717 - lr: 1.0000e-06\n",
      "Epoch 131/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4469 - reconstruction_loss: 2.2998 - kl_loss: 5.6722 - lr: 1.0000e-06\n",
      "Epoch 132/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4388 - reconstruction_loss: 2.2978 - kl_loss: 5.6714 - lr: 1.0000e-06\n",
      "Epoch 132: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 15:04:13,366]\u001b[0m Trial 66 finished with value: 0.004509543889042279 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 25, 'encoder_units_l2': 754, 'encoder_units_l3': 735, 'decoder_layers': 4, 'decoder_units_l1': 115, 'decoder_units_l2': 37, 'decoder_units_l3': 1008, 'decoder_units_l4': 30, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 4s 15ms/step - loss: 18.8146 - reconstruction_loss: 13.7641 - kl_loss: 0.7819 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 16.9600 - reconstruction_loss: 11.7910 - kl_loss: 1.0169 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.9526 - reconstruction_loss: 11.7545 - kl_loss: 1.0187 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 16.8957 - reconstruction_loss: 11.7437 - kl_loss: 1.0135 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 16.8610 - reconstruction_loss: 11.7491 - kl_loss: 1.0216 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 16.9091 - reconstruction_loss: 11.7318 - kl_loss: 1.0262 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 16.9373 - reconstruction_loss: 11.7354 - kl_loss: 1.0252\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.9352 - reconstruction_loss: 11.6886 - kl_loss: 1.0250 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.8196 - reconstruction_loss: 11.6485 - kl_loss: 1.0257 - lr: 5.0000e-04\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 16.8206 - reconstruction_loss: 11.6290 - kl_loss: 1.0309 - lr: 5.0000e-04\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 16.7711 - reconstruction_loss: 11.5469 - kl_loss: 1.0363 - lr: 5.0000e-04\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 16.7642 - reconstruction_loss: 11.3316 - kl_loss: 1.0688 - lr: 5.0000e-04\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 16.6148 - reconstruction_loss: 10.9096 - kl_loss: 1.1289 - lr: 5.0000e-04\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.4700 - reconstruction_loss: 10.5779 - kl_loss: 1.1775 - lr: 5.0000e-04\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.5392 - reconstruction_loss: 10.4087 - kl_loss: 1.2033 - lr: 5.0000e-04\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.4664 - reconstruction_loss: 10.3180 - kl_loss: 1.2158 - lr: 5.0000e-04\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.4159 - reconstruction_loss: 10.2244 - kl_loss: 1.2251 - lr: 5.0000e-04\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.3984 - reconstruction_loss: 10.1342 - kl_loss: 1.2393 - lr: 5.0000e-04\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.3951 - reconstruction_loss: 10.0869 - kl_loss: 1.2482 - lr: 5.0000e-04\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.3717 - reconstruction_loss: 10.0143 - kl_loss: 1.2656 - lr: 5.0000e-04\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.3332 - reconstruction_loss: 10.0014 - kl_loss: 1.2649 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.4036 - reconstruction_loss: 9.9748 - kl_loss: 1.2744 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 16.3514 - reconstruction_loss: 9.9228 - kl_loss: 1.2783 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.3507 - reconstruction_loss: 9.9276 - kl_loss: 1.2776 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.3432 - reconstruction_loss: 9.9017 - kl_loss: 1.2797 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.2785 - reconstruction_loss: 9.8948 - kl_loss: 1.2770 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.3123 - reconstruction_loss: 9.8308 - kl_loss: 1.2859 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.3779 - reconstruction_loss: 9.8496 - kl_loss: 1.2888 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.2642 - reconstruction_loss: 9.8284 - kl_loss: 1.2869 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 16.3188 - reconstruction_loss: 9.8549 - kl_loss: 1.2920\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.3184 - reconstruction_loss: 9.8195 - kl_loss: 1.2924 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 16.2953 - reconstruction_loss: 9.8270 - kl_loss: 1.2917 - lr: 2.5000e-04\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 16.2656 - reconstruction_loss: 9.7526 - kl_loss: 1.2970 - lr: 2.5000e-04\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.3190 - reconstruction_loss: 9.7801 - kl_loss: 1.2977 - lr: 2.5000e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.2903 - reconstruction_loss: 9.7442 - kl_loss: 1.2992 - lr: 2.5000e-04\n",
      "Epoch 34/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 16.2534 - reconstruction_loss: 9.8023 - kl_loss: 1.2972\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.2536 - reconstruction_loss: 9.7601 - kl_loss: 1.2973 - lr: 2.5000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.2411 - reconstruction_loss: 9.7436 - kl_loss: 1.2994 - lr: 1.2500e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.2669 - reconstruction_loss: 9.7367 - kl_loss: 1.3062 - lr: 1.2500e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.2417 - reconstruction_loss: 9.7298 - kl_loss: 1.2977 - lr: 1.2500e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.3206 - reconstruction_loss: 9.7367 - kl_loss: 1.3022 - lr: 1.2500e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.3082 - reconstruction_loss: 9.7304 - kl_loss: 1.3052 - lr: 1.2500e-04\n",
      "Epoch 40/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 16.3099 - reconstruction_loss: 9.7646 - kl_loss: 1.3018\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.3087 - reconstruction_loss: 9.7268 - kl_loss: 1.3021 - lr: 1.2500e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.2575 - reconstruction_loss: 9.7167 - kl_loss: 1.3021 - lr: 6.2500e-05\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.2219 - reconstruction_loss: 9.6979 - kl_loss: 1.3041 - lr: 6.2500e-05\n",
      "Epoch 43/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 16.2877 - reconstruction_loss: 9.7599 - kl_loss: 1.3081\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.2875 - reconstruction_loss: 9.7267 - kl_loss: 1.3079 - lr: 6.2500e-05\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.2649 - reconstruction_loss: 9.6991 - kl_loss: 1.3060 - lr: 3.1250e-05\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.2270 - reconstruction_loss: 9.6992 - kl_loss: 1.3052 - lr: 3.1250e-05\n",
      "Epoch 46/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 16.2886 - reconstruction_loss: 9.7195 - kl_loss: 1.3075\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.2879 - reconstruction_loss: 9.6856 - kl_loss: 1.3074 - lr: 3.1250e-05\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 16.2467 - reconstruction_loss: 9.6790 - kl_loss: 1.3106 - lr: 1.5625e-05\n",
      "Epoch 47: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 15:06:31,472]\u001b[0m Trial 67 finished with value: 0.01898854244546 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 17, 'encoder_units_l2': 1014, 'encoder_units_l3': 530, 'decoder_layers': 4, 'decoder_units_l1': 173, 'decoder_units_l2': 27, 'decoder_units_l3': 794, 'decoder_units_l4': 28, 'beta': 5, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 19.7085 - reconstruction_loss: 16.2746 - kl_loss: 0.9939 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 9.1196 - reconstruction_loss: 7.6742 - kl_loss: 3.9796 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.0431 - reconstruction_loss: 7.1192 - kl_loss: 4.2668 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.9251 - reconstruction_loss: 6.9911 - kl_loss: 4.2830 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.5530 - reconstruction_loss: 6.4415 - kl_loss: 4.4287 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 6.7842 - reconstruction_loss: 5.6885 - kl_loss: 4.7439 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 6.3951 - reconstruction_loss: 5.3030 - kl_loss: 4.9537 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 6.1298 - reconstruction_loss: 5.0402 - kl_loss: 5.0781 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.9595 - reconstruction_loss: 4.8479 - kl_loss: 5.1585 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.7870 - reconstruction_loss: 4.6918 - kl_loss: 5.2110 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.6949 - reconstruction_loss: 4.5773 - kl_loss: 5.2734 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.5833 - reconstruction_loss: 4.4716 - kl_loss: 5.3166 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.4820 - reconstruction_loss: 4.3825 - kl_loss: 5.3519 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 5.3877 - reconstruction_loss: 4.2976 - kl_loss: 5.3776 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 5.3470 - reconstruction_loss: 4.2342 - kl_loss: 5.4090 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 5.2594 - reconstruction_loss: 4.1650 - kl_loss: 5.4624 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 5.2119 - reconstruction_loss: 4.1083 - kl_loss: 5.4880 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 5.1693 - reconstruction_loss: 4.0520 - kl_loss: 5.5240 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 5.1288 - reconstruction_loss: 3.9985 - kl_loss: 5.5680 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.1010 - reconstruction_loss: 3.9464 - kl_loss: 5.5951 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 5.0520 - reconstruction_loss: 3.8953 - kl_loss: 5.6200 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.0226 - reconstruction_loss: 3.8554 - kl_loss: 5.6577 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.9882 - reconstruction_loss: 3.8069 - kl_loss: 5.6904 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.9343 - reconstruction_loss: 3.7666 - kl_loss: 5.6978 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.9353 - reconstruction_loss: 3.7429 - kl_loss: 5.7371 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.8584 - reconstruction_loss: 3.7007 - kl_loss: 5.7496 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.8272 - reconstruction_loss: 3.6672 - kl_loss: 5.7556 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.8077 - reconstruction_loss: 3.6389 - kl_loss: 5.7686 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.7960 - reconstruction_loss: 3.6139 - kl_loss: 5.7865 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.7647 - reconstruction_loss: 3.5886 - kl_loss: 5.7736 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.7565 - reconstruction_loss: 3.5730 - kl_loss: 5.7818 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.7196 - reconstruction_loss: 3.5476 - kl_loss: 5.8028 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.7144 - reconstruction_loss: 3.5277 - kl_loss: 5.8037 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.6916 - reconstruction_loss: 3.5102 - kl_loss: 5.8005 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.6585 - reconstruction_loss: 3.4932 - kl_loss: 5.8194 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.6618 - reconstruction_loss: 3.4754 - kl_loss: 5.8214 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.6391 - reconstruction_loss: 3.4617 - kl_loss: 5.8401 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.6241 - reconstruction_loss: 3.4380 - kl_loss: 5.8375 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.5947 - reconstruction_loss: 3.4256 - kl_loss: 5.8397 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.6109 - reconstruction_loss: 3.4145 - kl_loss: 5.8437 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.5945 - reconstruction_loss: 3.3956 - kl_loss: 5.8512 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.5760 - reconstruction_loss: 3.3889 - kl_loss: 5.8471 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.5655 - reconstruction_loss: 3.3706 - kl_loss: 5.8514 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.5388 - reconstruction_loss: 3.3547 - kl_loss: 5.8646 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.5153 - reconstruction_loss: 3.3498 - kl_loss: 5.8361 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.5219 - reconstruction_loss: 3.3371 - kl_loss: 5.8488 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.4859 - reconstruction_loss: 3.3209 - kl_loss: 5.8424 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.4824 - reconstruction_loss: 3.3105 - kl_loss: 5.8453 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.4937 - reconstruction_loss: 3.3072 - kl_loss: 5.8403 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.4751 - reconstruction_loss: 3.2947 - kl_loss: 5.8395 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.4604 - reconstruction_loss: 3.2784 - kl_loss: 5.8338 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.4634 - reconstruction_loss: 3.2675 - kl_loss: 5.8584 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.4321 - reconstruction_loss: 3.2562 - kl_loss: 5.8443 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.4362 - reconstruction_loss: 3.2508 - kl_loss: 5.8485 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.4214 - reconstruction_loss: 3.2398 - kl_loss: 5.8658 - lr: 1.0000e-04\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 3s 13ms/step - loss: 4.4264 - reconstruction_loss: 3.2345 - kl_loss: 5.8576 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.4028 - reconstruction_loss: 3.2171 - kl_loss: 5.8633 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.3808 - reconstruction_loss: 3.2004 - kl_loss: 5.8598 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.3678 - reconstruction_loss: 3.1984 - kl_loss: 5.8622 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.3889 - reconstruction_loss: 3.1917 - kl_loss: 5.8659 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.3389 - reconstruction_loss: 3.1764 - kl_loss: 5.8467 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.3711 - reconstruction_loss: 3.1700 - kl_loss: 5.8457 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.3530 - reconstruction_loss: 3.1600 - kl_loss: 5.8579 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.3319 - reconstruction_loss: 3.1558 - kl_loss: 5.8632 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.3259 - reconstruction_loss: 3.1374 - kl_loss: 5.8592 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.3047 - reconstruction_loss: 3.1318 - kl_loss: 5.8606 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.2949 - reconstruction_loss: 3.1211 - kl_loss: 5.8641 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.3053 - reconstruction_loss: 3.1163 - kl_loss: 5.8647 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.2736 - reconstruction_loss: 3.1013 - kl_loss: 5.8619 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.2785 - reconstruction_loss: 3.0983 - kl_loss: 5.8673 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.2830 - reconstruction_loss: 3.0885 - kl_loss: 5.8579 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.2565 - reconstruction_loss: 3.0856 - kl_loss: 5.8582 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.2559 - reconstruction_loss: 3.0781 - kl_loss: 5.8601 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.2498 - reconstruction_loss: 3.0628 - kl_loss: 5.8633 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.2452 - reconstruction_loss: 3.0618 - kl_loss: 5.8623 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.2395 - reconstruction_loss: 3.0469 - kl_loss: 5.8641 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.2397 - reconstruction_loss: 3.0393 - kl_loss: 5.8640 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.2348 - reconstruction_loss: 3.0372 - kl_loss: 5.8749 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.2203 - reconstruction_loss: 3.0367 - kl_loss: 5.8639 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.1894 - reconstruction_loss: 3.0177 - kl_loss: 5.8795 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.2036 - reconstruction_loss: 3.0165 - kl_loss: 5.8629 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.1840 - reconstruction_loss: 2.9998 - kl_loss: 5.8622 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.2000 - reconstruction_loss: 2.9976 - kl_loss: 5.8775 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.1869 - reconstruction_loss: 2.9908 - kl_loss: 5.8558 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.1498 - reconstruction_loss: 2.9867 - kl_loss: 5.8590 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.1447 - reconstruction_loss: 2.9626 - kl_loss: 5.8750 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.1740 - reconstruction_loss: 2.9642 - kl_loss: 5.8817 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.1285 - reconstruction_loss: 2.9539 - kl_loss: 5.8772 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.1383 - reconstruction_loss: 2.9446 - kl_loss: 5.8644 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.1216 - reconstruction_loss: 2.9411 - kl_loss: 5.8910 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.1362 - reconstruction_loss: 2.9298 - kl_loss: 5.8776 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.1291 - reconstruction_loss: 2.9171 - kl_loss: 5.8876 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0997 - reconstruction_loss: 2.9116 - kl_loss: 5.8887 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0799 - reconstruction_loss: 2.8987 - kl_loss: 5.8812 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0791 - reconstruction_loss: 2.9086 - kl_loss: 5.8927 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0665 - reconstruction_loss: 2.8946 - kl_loss: 5.8862 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.1008 - reconstruction_loss: 2.8888 - kl_loss: 5.8778 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0905 - reconstruction_loss: 2.8872 - kl_loss: 5.8753 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0591 - reconstruction_loss: 2.8749 - kl_loss: 5.8857 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0410 - reconstruction_loss: 2.8682 - kl_loss: 5.8816 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0742 - reconstruction_loss: 2.8628 - kl_loss: 5.9024 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0432 - reconstruction_loss: 2.8661 - kl_loss: 5.8898 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0474 - reconstruction_loss: 2.8583 - kl_loss: 5.8852 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0325 - reconstruction_loss: 2.8405 - kl_loss: 5.8895 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0272 - reconstruction_loss: 2.8450 - kl_loss: 5.8833 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.0126 - reconstruction_loss: 2.8364 - kl_loss: 5.8819 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.0421 - reconstruction_loss: 2.8400 - kl_loss: 5.8908 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.0119 - reconstruction_loss: 2.8328 - kl_loss: 5.8816 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0072 - reconstruction_loss: 2.8264 - kl_loss: 5.8786 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0036 - reconstruction_loss: 2.8225 - kl_loss: 5.8756 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9872 - reconstruction_loss: 2.8214 - kl_loss: 5.8776 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0043 - reconstruction_loss: 2.8122 - kl_loss: 5.8879 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9904 - reconstruction_loss: 2.8085 - kl_loss: 5.8805 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0096 - reconstruction_loss: 2.8077 - kl_loss: 5.8727 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9737 - reconstruction_loss: 2.7947 - kl_loss: 5.8803 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9746 - reconstruction_loss: 2.7992 - kl_loss: 5.8794 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9757 - reconstruction_loss: 2.7886 - kl_loss: 5.8635 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9827 - reconstruction_loss: 2.7917 - kl_loss: 5.8618 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9839 - reconstruction_loss: 2.7820 - kl_loss: 5.8687 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9800 - reconstruction_loss: 2.7907 - kl_loss: 5.8580 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9538 - reconstruction_loss: 2.7722 - kl_loss: 5.8593 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9652 - reconstruction_loss: 2.7693 - kl_loss: 5.8755 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9430 - reconstruction_loss: 2.7723 - kl_loss: 5.8722 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9536 - reconstruction_loss: 2.7611 - kl_loss: 5.8756 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9657 - reconstruction_loss: 2.7694 - kl_loss: 5.8561 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9282 - reconstruction_loss: 2.7587 - kl_loss: 5.8631 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.9461 - reconstruction_loss: 2.7530 - kl_loss: 5.8636 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.9335 - reconstruction_loss: 2.7557 - kl_loss: 5.8634 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.9195 - reconstruction_loss: 2.7451 - kl_loss: 5.8609 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9260 - reconstruction_loss: 2.7427 - kl_loss: 5.8714 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.9108 - reconstruction_loss: 2.7378 - kl_loss: 5.8526 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.9279 - reconstruction_loss: 2.7379 - kl_loss: 5.8631 - lr: 1.0000e-04\n",
      "Epoch 133/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.9369 - reconstruction_loss: 2.7392 - kl_loss: 5.8585 - lr: 1.0000e-04\n",
      "Epoch 134/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.9292 - reconstruction_loss: 2.7358 - kl_loss: 5.8564 - lr: 1.0000e-04\n",
      "Epoch 135/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.9020 - reconstruction_loss: 2.7322 - kl_loss: 5.8586 - lr: 1.0000e-04\n",
      "Epoch 136/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.9135 - reconstruction_loss: 2.7252 - kl_loss: 5.8742 - lr: 1.0000e-04\n",
      "Epoch 137/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.9150 - reconstruction_loss: 2.7176 - kl_loss: 5.8554 - lr: 1.0000e-04\n",
      "Epoch 138/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8936 - reconstruction_loss: 2.7187 - kl_loss: 5.8525 - lr: 1.0000e-04\n",
      "Epoch 139/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.9015 - reconstruction_loss: 2.7216 - kl_loss: 5.8632 - lr: 1.0000e-04\n",
      "Epoch 140/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8944 - reconstruction_loss: 2.7119 - kl_loss: 5.8607 - lr: 1.0000e-04\n",
      "Epoch 141/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8954 - reconstruction_loss: 2.7077 - kl_loss: 5.8541 - lr: 1.0000e-04\n",
      "Epoch 142/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8730 - reconstruction_loss: 2.7072 - kl_loss: 5.8475 - lr: 1.0000e-04\n",
      "Epoch 143/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8665 - reconstruction_loss: 2.6993 - kl_loss: 5.8692 - lr: 1.0000e-04\n",
      "Epoch 144/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8972 - reconstruction_loss: 2.7040 - kl_loss: 5.8650 - lr: 1.0000e-04\n",
      "Epoch 145/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8738 - reconstruction_loss: 2.7023 - kl_loss: 5.8440 - lr: 1.0000e-04\n",
      "Epoch 146/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8779 - reconstruction_loss: 2.6937 - kl_loss: 5.8578 - lr: 1.0000e-04\n",
      "Epoch 147/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8666 - reconstruction_loss: 2.6855 - kl_loss: 5.8651 - lr: 1.0000e-04\n",
      "Epoch 148/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8785 - reconstruction_loss: 2.6871 - kl_loss: 5.8448 - lr: 1.0000e-04\n",
      "Epoch 149/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8611 - reconstruction_loss: 2.6868 - kl_loss: 5.8657 - lr: 1.0000e-04\n",
      "Epoch 150/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8647 - reconstruction_loss: 2.6818 - kl_loss: 5.8578 - lr: 1.0000e-04\n",
      "Epoch 151/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8551 - reconstruction_loss: 2.6769 - kl_loss: 5.8490 - lr: 1.0000e-04\n",
      "Epoch 152/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8460 - reconstruction_loss: 2.6809 - kl_loss: 5.8558 - lr: 1.0000e-04\n",
      "Epoch 153/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8576 - reconstruction_loss: 2.6691 - kl_loss: 5.8590 - lr: 1.0000e-04\n",
      "Epoch 154/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8663 - reconstruction_loss: 2.6757 - kl_loss: 5.8449 - lr: 1.0000e-04\n",
      "Epoch 155/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8452 - reconstruction_loss: 2.6599 - kl_loss: 5.8586 - lr: 1.0000e-04\n",
      "Epoch 156/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8477 - reconstruction_loss: 2.6627 - kl_loss: 5.8648 - lr: 1.0000e-04\n",
      "Epoch 157/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8554 - reconstruction_loss: 2.6600 - kl_loss: 5.8631 - lr: 1.0000e-04\n",
      "Epoch 158/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.8513 - reconstruction_loss: 2.6722 - kl_loss: 5.8601\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8511 - reconstruction_loss: 2.6631 - kl_loss: 5.8600 - lr: 1.0000e-04\n",
      "Epoch 159/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8132 - reconstruction_loss: 2.6425 - kl_loss: 5.8558 - lr: 5.0000e-05\n",
      "Epoch 160/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8096 - reconstruction_loss: 2.6363 - kl_loss: 5.8445 - lr: 5.0000e-05\n",
      "Epoch 161/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8189 - reconstruction_loss: 2.6328 - kl_loss: 5.8697 - lr: 5.0000e-05\n",
      "Epoch 162/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8069 - reconstruction_loss: 2.6347 - kl_loss: 5.8698 - lr: 5.0000e-05\n",
      "Epoch 163/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8139 - reconstruction_loss: 2.6262 - kl_loss: 5.8727 - lr: 5.0000e-05\n",
      "Epoch 164/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8063 - reconstruction_loss: 2.6278 - kl_loss: 5.8712 - lr: 5.0000e-05\n",
      "Epoch 165/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7923 - reconstruction_loss: 2.6215 - kl_loss: 5.8678 - lr: 5.0000e-05\n",
      "Epoch 166/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8127 - reconstruction_loss: 2.6293 - kl_loss: 5.8654 - lr: 5.0000e-05\n",
      "Epoch 167/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8138 - reconstruction_loss: 2.6233 - kl_loss: 5.8674 - lr: 5.0000e-05\n",
      "Epoch 168/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.8147 - reconstruction_loss: 2.6350 - kl_loss: 5.8797\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8145 - reconstruction_loss: 2.6259 - kl_loss: 5.8792 - lr: 5.0000e-05\n",
      "Epoch 169/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7984 - reconstruction_loss: 2.6101 - kl_loss: 5.8829 - lr: 2.5000e-05\n",
      "Epoch 170/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7857 - reconstruction_loss: 2.6044 - kl_loss: 5.8664 - lr: 2.5000e-05\n",
      "Epoch 171/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7774 - reconstruction_loss: 2.6047 - kl_loss: 5.8715 - lr: 2.5000e-05\n",
      "Epoch 172/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7774 - reconstruction_loss: 2.6060 - kl_loss: 5.8754 - lr: 2.5000e-05\n",
      "Epoch 173/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.8090 - reconstruction_loss: 2.6157 - kl_loss: 5.8879\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8087 - reconstruction_loss: 2.6056 - kl_loss: 5.8878 - lr: 2.5000e-05\n",
      "Epoch 174/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7736 - reconstruction_loss: 2.5964 - kl_loss: 5.8818 - lr: 1.2500e-05\n",
      "Epoch 175/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7835 - reconstruction_loss: 2.5987 - kl_loss: 5.8789 - lr: 1.2500e-05\n",
      "Epoch 176/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7680 - reconstruction_loss: 2.5959 - kl_loss: 5.8788 - lr: 1.2500e-05\n",
      "Epoch 177/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7758 - reconstruction_loss: 2.5985 - kl_loss: 5.8823 - lr: 1.2500e-05\n",
      "Epoch 178/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7827 - reconstruction_loss: 2.5966 - kl_loss: 5.8826 - lr: 1.2500e-05\n",
      "Epoch 179/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.7811 - reconstruction_loss: 2.5984 - kl_loss: 5.8835\n",
      "Epoch 179: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7811 - reconstruction_loss: 2.5984 - kl_loss: 5.8835 - lr: 1.2500e-05\n",
      "Epoch 180/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7801 - reconstruction_loss: 2.5922 - kl_loss: 5.8891 - lr: 6.2500e-06\n",
      "Epoch 181/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7849 - reconstruction_loss: 2.5891 - kl_loss: 5.8837 - lr: 6.2500e-06\n",
      "Epoch 182/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7781 - reconstruction_loss: 2.5947 - kl_loss: 5.8820 - lr: 6.2500e-06\n",
      "Epoch 183/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7642 - reconstruction_loss: 2.5912 - kl_loss: 5.8837 - lr: 6.2500e-06\n",
      "Epoch 184/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.7783 - reconstruction_loss: 2.6013 - kl_loss: 5.8859\n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7782 - reconstruction_loss: 2.5906 - kl_loss: 5.8859 - lr: 6.2500e-06\n",
      "Epoch 185/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7972 - reconstruction_loss: 2.5936 - kl_loss: 5.8863 - lr: 3.1250e-06\n",
      "Epoch 186/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7637 - reconstruction_loss: 2.5889 - kl_loss: 5.8856 - lr: 3.1250e-06\n",
      "Epoch 187/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.7977 - reconstruction_loss: 2.5897 - kl_loss: 5.8851\n",
      "Epoch 187: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7976 - reconstruction_loss: 2.5897 - kl_loss: 5.8851 - lr: 3.1250e-06\n",
      "Epoch 188/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7816 - reconstruction_loss: 2.5862 - kl_loss: 5.8859 - lr: 1.5625e-06\n",
      "Epoch 189/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7729 - reconstruction_loss: 2.5849 - kl_loss: 5.8859 - lr: 1.5625e-06\n",
      "Epoch 190/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.7842 - reconstruction_loss: 2.5889 - kl_loss: 5.8848 - lr: 1.5625e-06\n",
      "Epoch 191/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7723 - reconstruction_loss: 2.5891 - kl_loss: 5.8860 - lr: 1.5625e-06\n",
      "Epoch 192/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.7883 - reconstruction_loss: 2.5997 - kl_loss: 5.8868\n",
      "Epoch 192: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7881 - reconstruction_loss: 2.5899 - kl_loss: 5.8869 - lr: 1.5625e-06\n",
      "Epoch 193/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7843 - reconstruction_loss: 2.5888 - kl_loss: 5.8883 - lr: 1.0000e-06\n",
      "Epoch 194/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7705 - reconstruction_loss: 2.5854 - kl_loss: 5.8875 - lr: 1.0000e-06\n",
      "Epoch 195/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7656 - reconstruction_loss: 2.5867 - kl_loss: 5.8868 - lr: 1.0000e-06\n",
      "Epoch 196/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7677 - reconstruction_loss: 2.5864 - kl_loss: 5.8864 - lr: 1.0000e-06\n",
      "Epoch 197/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7666 - reconstruction_loss: 2.5880 - kl_loss: 5.8864 - lr: 1.0000e-06\n",
      "Epoch 198/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7735 - reconstruction_loss: 2.5864 - kl_loss: 5.8864 - lr: 1.0000e-06\n",
      "Epoch 199/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7776 - reconstruction_loss: 2.5878 - kl_loss: 5.8858 - lr: 1.0000e-06\n",
      "Epoch 199: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 15:14:52,037]\u001b[0m Trial 68 finished with value: 0.005068226770307079 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 26, 'encoder_units_l2': 866, 'encoder_units_l3': 748, 'decoder_layers': 4, 'decoder_units_l1': 203, 'decoder_units_l2': 20, 'decoder_units_l3': 795, 'decoder_units_l4': 39, 'beta': 0.2, 'lr': 0.0001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 11ms/step - loss: 14.6857 - reconstruction_loss: 9.7824 - kl_loss: 2.2193 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 10.3589 - reconstruction_loss: 7.3101 - kl_loss: 2.7138 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 8.9412 - reconstruction_loss: 5.6395 - kl_loss: 3.0921 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 8.4148 - reconstruction_loss: 5.1974 - kl_loss: 3.1544 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 8.2665 - reconstruction_loss: 5.0373 - kl_loss: 3.1951 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 8.1752 - reconstruction_loss: 4.9025 - kl_loss: 3.2258 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 8.0948 - reconstruction_loss: 4.8076 - kl_loss: 3.2583 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 8.0689 - reconstruction_loss: 4.7421 - kl_loss: 3.2783 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 8.0117 - reconstruction_loss: 4.6582 - kl_loss: 3.3124 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.9560 - reconstruction_loss: 4.5849 - kl_loss: 3.3408 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.9522 - reconstruction_loss: 4.5706 - kl_loss: 3.3546 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.8834 - reconstruction_loss: 4.4709 - kl_loss: 3.3801 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.8817 - reconstruction_loss: 4.4446 - kl_loss: 3.3978 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.7975 - reconstruction_loss: 4.3496 - kl_loss: 3.4243 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.7789 - reconstruction_loss: 4.3217 - kl_loss: 3.4405 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.7896 - reconstruction_loss: 4.2786 - kl_loss: 3.4605 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7199 - reconstruction_loss: 4.2104 - kl_loss: 3.4855 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7336 - reconstruction_loss: 4.1839 - kl_loss: 3.4990 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7079 - reconstruction_loss: 4.1342 - kl_loss: 3.5280 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.6848 - reconstruction_loss: 4.0802 - kl_loss: 3.5590 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.6430 - reconstruction_loss: 4.0505 - kl_loss: 3.5702 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.5980 - reconstruction_loss: 3.9765 - kl_loss: 3.6034 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.5548 - reconstruction_loss: 3.9147 - kl_loss: 3.6286 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.5285 - reconstruction_loss: 3.8511 - kl_loss: 3.6515 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.5220 - reconstruction_loss: 3.8193 - kl_loss: 3.6607 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.4972 - reconstruction_loss: 3.7821 - kl_loss: 3.6775 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4879 - reconstruction_loss: 3.7601 - kl_loss: 3.6935 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.4147 - reconstruction_loss: 3.7004 - kl_loss: 3.7122 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.4087 - reconstruction_loss: 3.6793 - kl_loss: 3.7111 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.4090 - reconstruction_loss: 3.6557 - kl_loss: 3.7229 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.3859 - reconstruction_loss: 3.6230 - kl_loss: 3.7312 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.3634 - reconstruction_loss: 3.6085 - kl_loss: 3.7416 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.3534 - reconstruction_loss: 3.5760 - kl_loss: 3.7500 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.3285 - reconstruction_loss: 3.5717 - kl_loss: 3.7484 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.3365 - reconstruction_loss: 3.5740 - kl_loss: 3.7526 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.3012 - reconstruction_loss: 3.5309 - kl_loss: 3.7555 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.3532 - reconstruction_loss: 3.5594 - kl_loss: 3.7650 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2765 - reconstruction_loss: 3.4971 - kl_loss: 3.7582 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.3013 - reconstruction_loss: 3.4771 - kl_loss: 3.7803 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2745 - reconstruction_loss: 3.5091 - kl_loss: 3.7617 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2732 - reconstruction_loss: 3.4805 - kl_loss: 3.7705 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2656 - reconstruction_loss: 3.4838 - kl_loss: 3.7759 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2454 - reconstruction_loss: 3.4624 - kl_loss: 3.7826 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2880 - reconstruction_loss: 3.4576 - kl_loss: 3.7956 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2627 - reconstruction_loss: 3.4367 - kl_loss: 3.7944 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2516 - reconstruction_loss: 3.4903 - kl_loss: 3.7680 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2507 - reconstruction_loss: 3.4474 - kl_loss: 3.7892 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2409 - reconstruction_loss: 3.4168 - kl_loss: 3.7984 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2313 - reconstruction_loss: 3.4129 - kl_loss: 3.7966 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.1949 - reconstruction_loss: 3.4122 - kl_loss: 3.7878 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2151 - reconstruction_loss: 3.4381 - kl_loss: 3.7890 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2097 - reconstruction_loss: 3.4181 - kl_loss: 3.7846 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2309 - reconstruction_loss: 3.3985 - kl_loss: 3.7938 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2403 - reconstruction_loss: 3.4251 - kl_loss: 3.7886 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2357 - reconstruction_loss: 3.3843 - kl_loss: 3.8021 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2419 - reconstruction_loss: 3.4392 - kl_loss: 3.8097 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.1906 - reconstruction_loss: 3.3799 - kl_loss: 3.7902 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.1804 - reconstruction_loss: 3.3568 - kl_loss: 3.7967 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2283 - reconstruction_loss: 3.4280 - kl_loss: 3.7911 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.1910 - reconstruction_loss: 3.3790 - kl_loss: 3.8041 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 7.2450 - reconstruction_loss: 3.4266 - kl_loss: 3.7955\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2442 - reconstruction_loss: 3.4113 - kl_loss: 3.7963 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.1747 - reconstruction_loss: 3.3287 - kl_loss: 3.8073 - lr: 5.0000e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.1203 - reconstruction_loss: 3.3082 - kl_loss: 3.8056 - lr: 5.0000e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.1309 - reconstruction_loss: 3.3515 - kl_loss: 3.8138 - lr: 5.0000e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.2029 - reconstruction_loss: 3.3447 - kl_loss: 3.8283 - lr: 5.0000e-04\n",
      "Epoch 66/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 7.1813 - reconstruction_loss: 3.3712 - kl_loss: 3.8139\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.1812 - reconstruction_loss: 3.3594 - kl_loss: 3.8131 - lr: 5.0000e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.1663 - reconstruction_loss: 3.3385 - kl_loss: 3.8126 - lr: 2.5000e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.1481 - reconstruction_loss: 3.3070 - kl_loss: 3.8122 - lr: 2.5000e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.1000 - reconstruction_loss: 3.2797 - kl_loss: 3.8122 - lr: 2.5000e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0880 - reconstruction_loss: 3.2787 - kl_loss: 3.8135 - lr: 2.5000e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0945 - reconstruction_loss: 3.2710 - kl_loss: 3.8076 - lr: 2.5000e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0705 - reconstruction_loss: 3.2607 - kl_loss: 3.8078 - lr: 2.5000e-04\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0928 - reconstruction_loss: 3.2663 - kl_loss: 3.8180 - lr: 2.5000e-04\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0783 - reconstruction_loss: 3.2484 - kl_loss: 3.8196 - lr: 2.5000e-04\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0695 - reconstruction_loss: 3.2310 - kl_loss: 3.8229 - lr: 2.5000e-04\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0720 - reconstruction_loss: 3.2400 - kl_loss: 3.8177 - lr: 2.5000e-04\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0656 - reconstruction_loss: 3.2371 - kl_loss: 3.8157 - lr: 2.5000e-04\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0847 - reconstruction_loss: 3.2497 - kl_loss: 3.8293 - lr: 2.5000e-04\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0784 - reconstruction_loss: 3.2486 - kl_loss: 3.8200 - lr: 2.5000e-04\n",
      "Epoch 80/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 7.0729 - reconstruction_loss: 3.2579 - kl_loss: 3.8144\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0728 - reconstruction_loss: 3.2483 - kl_loss: 3.8144 - lr: 2.5000e-04\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0396 - reconstruction_loss: 3.2151 - kl_loss: 3.8153 - lr: 1.2500e-04\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0384 - reconstruction_loss: 3.2147 - kl_loss: 3.8159 - lr: 1.2500e-04\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0416 - reconstruction_loss: 3.2116 - kl_loss: 3.8172 - lr: 1.2500e-04\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0341 - reconstruction_loss: 3.2156 - kl_loss: 3.8140 - lr: 1.2500e-04\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0333 - reconstruction_loss: 3.2107 - kl_loss: 3.8180 - lr: 1.2500e-04\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0411 - reconstruction_loss: 3.2171 - kl_loss: 3.8105 - lr: 1.2500e-04\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0327 - reconstruction_loss: 3.2176 - kl_loss: 3.8134 - lr: 1.2500e-04\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0179 - reconstruction_loss: 3.2061 - kl_loss: 3.8103 - lr: 1.2500e-04\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0433 - reconstruction_loss: 3.2015 - kl_loss: 3.8272 - lr: 1.2500e-04\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0386 - reconstruction_loss: 3.1987 - kl_loss: 3.8198 - lr: 1.2500e-04\n",
      "Epoch 91/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 7.0433 - reconstruction_loss: 3.2246 - kl_loss: 3.8141\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0430 - reconstruction_loss: 3.2117 - kl_loss: 3.8141 - lr: 1.2500e-04\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0063 - reconstruction_loss: 3.1951 - kl_loss: 3.8094 - lr: 6.2500e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0209 - reconstruction_loss: 3.1962 - kl_loss: 3.8154 - lr: 6.2500e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0108 - reconstruction_loss: 3.1998 - kl_loss: 3.8132 - lr: 6.2500e-05\n",
      "Epoch 95/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 7.0034 - reconstruction_loss: 3.1993 - kl_loss: 3.8251\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0038 - reconstruction_loss: 3.1866 - kl_loss: 3.8248 - lr: 6.2500e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0167 - reconstruction_loss: 3.1807 - kl_loss: 3.8233 - lr: 3.1250e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0038 - reconstruction_loss: 3.1904 - kl_loss: 3.8160 - lr: 3.1250e-05\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0204 - reconstruction_loss: 3.1795 - kl_loss: 3.8246 - lr: 3.1250e-05\n",
      "Epoch 99/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 7.0053 - reconstruction_loss: 3.1893 - kl_loss: 3.8265\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0054 - reconstruction_loss: 3.1778 - kl_loss: 3.8263 - lr: 3.1250e-05\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0080 - reconstruction_loss: 3.1826 - kl_loss: 3.8288 - lr: 1.5625e-05\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0119 - reconstruction_loss: 3.1751 - kl_loss: 3.8300 - lr: 1.5625e-05\n",
      "Epoch 102/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 7.0182 - reconstruction_loss: 3.1934 - kl_loss: 3.8241\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0181 - reconstruction_loss: 3.1805 - kl_loss: 3.8236 - lr: 1.5625e-05\n",
      "Epoch 103/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0148 - reconstruction_loss: 3.1726 - kl_loss: 3.8269 - lr: 7.8125e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0190 - reconstruction_loss: 3.1736 - kl_loss: 3.8279 - lr: 7.8125e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0147 - reconstruction_loss: 3.1764 - kl_loss: 3.8195 - lr: 7.8125e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0107 - reconstruction_loss: 3.1786 - kl_loss: 3.8204 - lr: 7.8125e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0170 - reconstruction_loss: 3.1751 - kl_loss: 3.8225 - lr: 7.8125e-06\n",
      "Epoch 108/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 7.0175 - reconstruction_loss: 3.1908 - kl_loss: 3.8176\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0172 - reconstruction_loss: 3.1813 - kl_loss: 3.8178 - lr: 7.8125e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 6.9981 - reconstruction_loss: 3.1715 - kl_loss: 3.8207 - lr: 3.9063e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0181 - reconstruction_loss: 3.1756 - kl_loss: 3.8229 - lr: 3.9063e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0136 - reconstruction_loss: 3.1777 - kl_loss: 3.8222 - lr: 3.9063e-06\n",
      "Epoch 112/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 6.9956 - reconstruction_loss: 3.1820 - kl_loss: 3.8227\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 6.9957 - reconstruction_loss: 3.1708 - kl_loss: 3.8231 - lr: 3.9063e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0089 - reconstruction_loss: 3.1722 - kl_loss: 3.8197 - lr: 1.9531e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0134 - reconstruction_loss: 3.1714 - kl_loss: 3.8190 - lr: 1.9531e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0040 - reconstruction_loss: 3.1786 - kl_loss: 3.8190 - lr: 1.9531e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0107 - reconstruction_loss: 3.1739 - kl_loss: 3.8219 - lr: 1.9531e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0028 - reconstruction_loss: 3.1683 - kl_loss: 3.8209 - lr: 1.9531e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0092 - reconstruction_loss: 3.1688 - kl_loss: 3.8160 - lr: 1.9531e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0140 - reconstruction_loss: 3.1798 - kl_loss: 3.8167 - lr: 1.9531e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0026 - reconstruction_loss: 3.1777 - kl_loss: 3.8155 - lr: 1.9531e-06\n",
      "Epoch 121/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 7.0080 - reconstruction_loss: 3.1889 - kl_loss: 3.8186\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0079 - reconstruction_loss: 3.1778 - kl_loss: 3.8190 - lr: 1.9531e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0076 - reconstruction_loss: 3.1719 - kl_loss: 3.8211 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0073 - reconstruction_loss: 3.1718 - kl_loss: 3.8199 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0020 - reconstruction_loss: 3.1795 - kl_loss: 3.8206 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0110 - reconstruction_loss: 3.1727 - kl_loss: 3.8201 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 7.0173 - reconstruction_loss: 3.1750 - kl_loss: 3.8194 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0113 - reconstruction_loss: 3.1767 - kl_loss: 3.8203 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.0106 - reconstruction_loss: 3.1724 - kl_loss: 3.8208 - lr: 1.0000e-06\n",
      "Epoch 128: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 15:19:13,549]\u001b[0m Trial 69 finished with value: 0.006216904541708762 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 37, 'encoder_units_l2': 710, 'encoder_units_l3': 270, 'decoder_layers': 4, 'decoder_units_l1': 80, 'decoder_units_l2': 44, 'decoder_units_l3': 968, 'decoder_units_l4': 21, 'beta': 1, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 13.4290 - reconstruction_loss: 9.1404 - kl_loss: 3.8067 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 7.9337 - reconstruction_loss: 7.0306 - kl_loss: 4.2405 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.2827 - reconstruction_loss: 5.9699 - kl_loss: 4.6661 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 6.0173 - reconstruction_loss: 4.8057 - kl_loss: 5.0243 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.3441 - reconstruction_loss: 4.2225 - kl_loss: 5.2707 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.0957 - reconstruction_loss: 3.9476 - kl_loss: 5.3706 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.8839 - reconstruction_loss: 3.7385 - kl_loss: 5.4573 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.7199 - reconstruction_loss: 3.5819 - kl_loss: 5.5155 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 4.6750 - reconstruction_loss: 3.5032 - kl_loss: 5.5511 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.5439 - reconstruction_loss: 3.4225 - kl_loss: 5.5662 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.5122 - reconstruction_loss: 3.3578 - kl_loss: 5.5844 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.4216 - reconstruction_loss: 3.2822 - kl_loss: 5.6011 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.4037 - reconstruction_loss: 3.2313 - kl_loss: 5.6139 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.3313 - reconstruction_loss: 3.1768 - kl_loss: 5.6442 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.2728 - reconstruction_loss: 3.1129 - kl_loss: 5.6574 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.2249 - reconstruction_loss: 3.0703 - kl_loss: 5.6737 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.1706 - reconstruction_loss: 3.0058 - kl_loss: 5.6724 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0833 - reconstruction_loss: 2.9367 - kl_loss: 5.6983 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0874 - reconstruction_loss: 2.9505 - kl_loss: 5.7099 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0548 - reconstruction_loss: 2.8886 - kl_loss: 5.7243 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0168 - reconstruction_loss: 2.8482 - kl_loss: 5.7284 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0311 - reconstruction_loss: 2.8639 - kl_loss: 5.7105 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0108 - reconstruction_loss: 2.8160 - kl_loss: 5.7216 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9641 - reconstruction_loss: 2.7851 - kl_loss: 5.7266 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9008 - reconstruction_loss: 2.7436 - kl_loss: 5.7188 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9052 - reconstruction_loss: 2.7763 - kl_loss: 5.7423 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8600 - reconstruction_loss: 2.6975 - kl_loss: 5.7210 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8370 - reconstruction_loss: 2.7049 - kl_loss: 5.7383 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8630 - reconstruction_loss: 2.7304 - kl_loss: 5.7587 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8695 - reconstruction_loss: 2.6808 - kl_loss: 5.7353 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.7976 - reconstruction_loss: 2.6414 - kl_loss: 5.7459 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.7542 - reconstruction_loss: 2.6155 - kl_loss: 5.7394 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7616 - reconstruction_loss: 2.5955 - kl_loss: 5.7528 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7592 - reconstruction_loss: 2.6051 - kl_loss: 5.7522 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8018 - reconstruction_loss: 2.6336 - kl_loss: 5.7250 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7074 - reconstruction_loss: 2.5673 - kl_loss: 5.7277 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7603 - reconstruction_loss: 2.6026 - kl_loss: 5.7322 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8013 - reconstruction_loss: 2.5952 - kl_loss: 5.7456 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.7548 - reconstruction_loss: 2.6308 - kl_loss: 5.7810\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7556 - reconstruction_loss: 2.6210 - kl_loss: 5.7813 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7168 - reconstruction_loss: 2.5314 - kl_loss: 5.7419 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7024 - reconstruction_loss: 2.5130 - kl_loss: 5.7418 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6714 - reconstruction_loss: 2.5035 - kl_loss: 5.7567 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6509 - reconstruction_loss: 2.5008 - kl_loss: 5.7528 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6399 - reconstruction_loss: 2.4831 - kl_loss: 5.7287 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6250 - reconstruction_loss: 2.4776 - kl_loss: 5.7369 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6091 - reconstruction_loss: 2.4532 - kl_loss: 5.7395 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6200 - reconstruction_loss: 2.4490 - kl_loss: 5.7421 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6065 - reconstruction_loss: 2.4561 - kl_loss: 5.7341 - lr: 5.0000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6017 - reconstruction_loss: 2.4614 - kl_loss: 5.7656 - lr: 5.0000e-04\n",
      "Epoch 50/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.6481 - reconstruction_loss: 2.4752 - kl_loss: 5.7687\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6476 - reconstruction_loss: 2.4648 - kl_loss: 5.7691 - lr: 5.0000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5745 - reconstruction_loss: 2.4049 - kl_loss: 5.7404 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5526 - reconstruction_loss: 2.4014 - kl_loss: 5.7676 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5619 - reconstruction_loss: 2.4041 - kl_loss: 5.7550 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5479 - reconstruction_loss: 2.4048 - kl_loss: 5.7326 - lr: 2.5000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5625 - reconstruction_loss: 2.4006 - kl_loss: 5.7298 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5299 - reconstruction_loss: 2.3833 - kl_loss: 5.7302 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5216 - reconstruction_loss: 2.3813 - kl_loss: 5.7302 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5629 - reconstruction_loss: 2.3899 - kl_loss: 5.7501 - lr: 2.5000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5307 - reconstruction_loss: 2.3852 - kl_loss: 5.7376 - lr: 2.5000e-04\n",
      "Epoch 60/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5346 - reconstruction_loss: 2.3896 - kl_loss: 5.7384\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5346 - reconstruction_loss: 2.3806 - kl_loss: 5.7396 - lr: 2.5000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5336 - reconstruction_loss: 2.3756 - kl_loss: 5.7246 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5255 - reconstruction_loss: 2.3632 - kl_loss: 5.7278 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5259 - reconstruction_loss: 2.3579 - kl_loss: 5.7469 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5326 - reconstruction_loss: 2.3593 - kl_loss: 5.7459 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5132 - reconstruction_loss: 2.3655 - kl_loss: 5.7475 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5029 - reconstruction_loss: 2.3582 - kl_loss: 5.7369 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5114 - reconstruction_loss: 2.3581 - kl_loss: 5.7335 - lr: 1.2500e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5277 - reconstruction_loss: 2.3708 - kl_loss: 5.7287 - lr: 1.2500e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5301 - reconstruction_loss: 2.3630 - kl_loss: 5.7429 - lr: 1.2500e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5106 - reconstruction_loss: 2.3587 - kl_loss: 5.7291 - lr: 1.2500e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5011 - reconstruction_loss: 2.3541 - kl_loss: 5.7291 - lr: 1.2500e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5165 - reconstruction_loss: 2.3558 - kl_loss: 5.7300 - lr: 1.2500e-04\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5146 - reconstruction_loss: 2.3554 - kl_loss: 5.7245 - lr: 1.2500e-04\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4966 - reconstruction_loss: 2.3458 - kl_loss: 5.7318 - lr: 1.2500e-04\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5157 - reconstruction_loss: 2.3513 - kl_loss: 5.7241 - lr: 1.2500e-04\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5011 - reconstruction_loss: 2.3459 - kl_loss: 5.7403 - lr: 1.2500e-04\n",
      "Epoch 77/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5038 - reconstruction_loss: 2.3586 - kl_loss: 5.7323\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5037 - reconstruction_loss: 2.3509 - kl_loss: 5.7323 - lr: 1.2500e-04\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5002 - reconstruction_loss: 2.3392 - kl_loss: 5.7271 - lr: 6.2500e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4914 - reconstruction_loss: 2.3389 - kl_loss: 5.7426 - lr: 6.2500e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4857 - reconstruction_loss: 2.3372 - kl_loss: 5.7400 - lr: 6.2500e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5041 - reconstruction_loss: 2.3334 - kl_loss: 5.7377 - lr: 6.2500e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4792 - reconstruction_loss: 2.3310 - kl_loss: 5.7382 - lr: 6.2500e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4824 - reconstruction_loss: 2.3356 - kl_loss: 5.7365 - lr: 6.2500e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4881 - reconstruction_loss: 2.3295 - kl_loss: 5.7457 - lr: 6.2500e-05\n",
      "Epoch 85/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.4998 - reconstruction_loss: 2.3406 - kl_loss: 5.7473\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4994 - reconstruction_loss: 2.3316 - kl_loss: 5.7473 - lr: 6.2500e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4922 - reconstruction_loss: 2.3248 - kl_loss: 5.7504 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4733 - reconstruction_loss: 2.3271 - kl_loss: 5.7438 - lr: 3.1250e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4852 - reconstruction_loss: 2.3232 - kl_loss: 5.7399 - lr: 3.1250e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4728 - reconstruction_loss: 2.3227 - kl_loss: 5.7493 - lr: 3.1250e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4929 - reconstruction_loss: 2.3256 - kl_loss: 5.7449 - lr: 3.1250e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4841 - reconstruction_loss: 2.3201 - kl_loss: 5.7512 - lr: 3.1250e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4652 - reconstruction_loss: 2.3226 - kl_loss: 5.7506 - lr: 3.1250e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4766 - reconstruction_loss: 2.3227 - kl_loss: 5.7376 - lr: 3.1250e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4624 - reconstruction_loss: 2.3201 - kl_loss: 5.7391 - lr: 3.1250e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4729 - reconstruction_loss: 2.3193 - kl_loss: 5.7409 - lr: 3.1250e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4766 - reconstruction_loss: 2.3204 - kl_loss: 5.7369 - lr: 3.1250e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4829 - reconstruction_loss: 2.3222 - kl_loss: 5.7415 - lr: 3.1250e-05\n",
      "Epoch 98/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.4793 - reconstruction_loss: 2.3258 - kl_loss: 5.7530\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4791 - reconstruction_loss: 2.3177 - kl_loss: 5.7532 - lr: 3.1250e-05\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4788 - reconstruction_loss: 2.3160 - kl_loss: 5.7389 - lr: 1.5625e-05\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4589 - reconstruction_loss: 2.3153 - kl_loss: 5.7393 - lr: 1.5625e-05\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4726 - reconstruction_loss: 2.3138 - kl_loss: 5.7475 - lr: 1.5625e-05\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4863 - reconstruction_loss: 2.3141 - kl_loss: 5.7490 - lr: 1.5625e-05\n",
      "Epoch 103/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4772 - reconstruction_loss: 2.3212 - kl_loss: 5.7506\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4770 - reconstruction_loss: 2.3132 - kl_loss: 5.7504 - lr: 1.5625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4702 - reconstruction_loss: 2.3095 - kl_loss: 5.7418 - lr: 7.8125e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4560 - reconstruction_loss: 2.3116 - kl_loss: 5.7375 - lr: 7.8125e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4677 - reconstruction_loss: 2.3122 - kl_loss: 5.7430 - lr: 7.8125e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4751 - reconstruction_loss: 2.3080 - kl_loss: 5.7526\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4750 - reconstruction_loss: 2.3080 - kl_loss: 5.7526 - lr: 7.8125e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4827 - reconstruction_loss: 2.3095 - kl_loss: 5.7435 - lr: 3.9063e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4608 - reconstruction_loss: 2.3096 - kl_loss: 5.7460 - lr: 3.9063e-06\n",
      "Epoch 110/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.4560 - reconstruction_loss: 2.3168 - kl_loss: 5.7454\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4563 - reconstruction_loss: 2.3091 - kl_loss: 5.7455 - lr: 3.9063e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4650 - reconstruction_loss: 2.3090 - kl_loss: 5.7442 - lr: 1.9531e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4713 - reconstruction_loss: 2.3082 - kl_loss: 5.7476 - lr: 1.9531e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4675 - reconstruction_loss: 2.3092 - kl_loss: 5.7470 - lr: 1.9531e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4664 - reconstruction_loss: 2.3064 - kl_loss: 5.7515 - lr: 1.9531e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4693 - reconstruction_loss: 2.3084 - kl_loss: 5.7480 - lr: 1.9531e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4544 - reconstruction_loss: 2.3065 - kl_loss: 5.7481 - lr: 1.9531e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4506 - reconstruction_loss: 2.3076 - kl_loss: 5.7463 - lr: 1.9531e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4718 - reconstruction_loss: 2.3064 - kl_loss: 5.7482 - lr: 1.9531e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4571 - reconstruction_loss: 2.3049 - kl_loss: 5.7487 - lr: 1.9531e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4531 - reconstruction_loss: 2.3079 - kl_loss: 5.7463 - lr: 1.9531e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4589 - reconstruction_loss: 2.3069 - kl_loss: 5.7475 - lr: 1.9531e-06\n",
      "Epoch 122/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.4720 - reconstruction_loss: 2.3132 - kl_loss: 5.7490\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4717 - reconstruction_loss: 2.3059 - kl_loss: 5.7489 - lr: 1.9531e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4724 - reconstruction_loss: 2.3091 - kl_loss: 5.7485 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4682 - reconstruction_loss: 2.3070 - kl_loss: 5.7503 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4651 - reconstruction_loss: 2.3049 - kl_loss: 5.7484 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4629 - reconstruction_loss: 2.3057 - kl_loss: 5.7457 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4681 - reconstruction_loss: 2.3075 - kl_loss: 5.7463 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4625 - reconstruction_loss: 2.3054 - kl_loss: 5.7468 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4599 - reconstruction_loss: 2.3067 - kl_loss: 5.7456 - lr: 1.0000e-06\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4703 - reconstruction_loss: 2.3069 - kl_loss: 5.7459 - lr: 1.0000e-06\n",
      "Epoch 131/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4757 - reconstruction_loss: 2.3079 - kl_loss: 5.7464 - lr: 1.0000e-06\n",
      "Epoch 132/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4580 - reconstruction_loss: 2.3064 - kl_loss: 5.7454 - lr: 1.0000e-06\n",
      "Epoch 133/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4690 - reconstruction_loss: 2.3098 - kl_loss: 5.7485 - lr: 1.0000e-06\n",
      "Epoch 134/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4681 - reconstruction_loss: 2.3065 - kl_loss: 5.7503 - lr: 1.0000e-06\n",
      "Epoch 135/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4635 - reconstruction_loss: 2.3061 - kl_loss: 5.7503 - lr: 1.0000e-06\n",
      "Epoch 135: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 15:22:26,052]\u001b[0m Trial 70 finished with value: 0.004521700471375792 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 22, 'encoder_units_l2': 831, 'encoder_units_l3': 363, 'decoder_layers': 3, 'decoder_units_l1': 148, 'decoder_units_l2': 58, 'decoder_units_l3': 828, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 6ms/step - loss: 14.0630 - reconstruction_loss: 9.5238 - kl_loss: 3.8138 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 7.5861 - reconstruction_loss: 6.3917 - kl_loss: 4.5971 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 6.5583 - reconstruction_loss: 5.3074 - kl_loss: 5.0767 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 5.8203 - reconstruction_loss: 4.6122 - kl_loss: 5.2831 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 5.3903 - reconstruction_loss: 4.1997 - kl_loss: 5.3939 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 5.1315 - reconstruction_loss: 3.9319 - kl_loss: 5.5072 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 4.8980 - reconstruction_loss: 3.7176 - kl_loss: 5.6060 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 4.7092 - reconstruction_loss: 3.5311 - kl_loss: 5.6633 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 4.6016 - reconstruction_loss: 3.4268 - kl_loss: 5.6902 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 4.5041 - reconstruction_loss: 3.2998 - kl_loss: 5.7094 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 4.4613 - reconstruction_loss: 3.2468 - kl_loss: 5.7264 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 4.3272 - reconstruction_loss: 3.1614 - kl_loss: 5.7491 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 4.2297 - reconstruction_loss: 3.0695 - kl_loss: 5.7591 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 4.1524 - reconstruction_loss: 2.9652 - kl_loss: 5.7762 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.0999 - reconstruction_loss: 2.9532 - kl_loss: 5.7732 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 4.0656 - reconstruction_loss: 2.8793 - kl_loss: 5.7784 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 4.0161 - reconstruction_loss: 2.8481 - kl_loss: 5.8027 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.9793 - reconstruction_loss: 2.7987 - kl_loss: 5.8073 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.9695 - reconstruction_loss: 2.7948 - kl_loss: 5.7997 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.9716 - reconstruction_loss: 2.7695 - kl_loss: 5.7865 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.9218 - reconstruction_loss: 2.7205 - kl_loss: 5.7892 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.8860 - reconstruction_loss: 2.7073 - kl_loss: 5.7845 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.8586 - reconstruction_loss: 2.7005 - kl_loss: 5.7774 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.8732 - reconstruction_loss: 2.7000 - kl_loss: 5.7697 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.9219 - reconstruction_loss: 2.7257 - kl_loss: 5.7602 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.8116 - reconstruction_loss: 2.6374 - kl_loss: 5.7416 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.7996 - reconstruction_loss: 2.6314 - kl_loss: 5.7545 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.7619 - reconstruction_loss: 2.6128 - kl_loss: 5.7503 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.7681 - reconstruction_loss: 2.6181 - kl_loss: 5.7480 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.7927 - reconstruction_loss: 2.6183 - kl_loss: 5.7287 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.7359 - reconstruction_loss: 2.5690 - kl_loss: 5.7177 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.7851 - reconstruction_loss: 2.6049 - kl_loss: 5.7272 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.6861 - reconstruction_loss: 2.5461 - kl_loss: 5.7192 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.7578 - reconstruction_loss: 2.6035 - kl_loss: 5.7137 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.7479 - reconstruction_loss: 2.6268 - kl_loss: 5.7471 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.7918 - reconstruction_loss: 2.6033 - kl_loss: 5.7526\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.7916 - reconstruction_loss: 2.6033 - kl_loss: 5.7526 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.6729 - reconstruction_loss: 2.5091 - kl_loss: 5.7397 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.6482 - reconstruction_loss: 2.4863 - kl_loss: 5.7063 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.6429 - reconstruction_loss: 2.4830 - kl_loss: 5.6981 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.6939 - reconstruction_loss: 2.5286 - kl_loss: 5.6947 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.6728 - reconstruction_loss: 2.5087 - kl_loss: 5.7041 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "188/196 [===========================>..] - ETA: 0s - loss: 3.6615 - reconstruction_loss: 2.4965 - kl_loss: 5.7197\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.6606 - reconstruction_loss: 2.4912 - kl_loss: 5.7220 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.6142 - reconstruction_loss: 2.4558 - kl_loss: 5.7002 - lr: 2.5000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5962 - reconstruction_loss: 2.4440 - kl_loss: 5.6906 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5915 - reconstruction_loss: 2.4378 - kl_loss: 5.6836 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5887 - reconstruction_loss: 2.4373 - kl_loss: 5.6994 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5833 - reconstruction_loss: 2.4255 - kl_loss: 5.6707 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5604 - reconstruction_loss: 2.4274 - kl_loss: 5.6810 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5853 - reconstruction_loss: 2.4387 - kl_loss: 5.6688 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5613 - reconstruction_loss: 2.4234 - kl_loss: 5.6742 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5525 - reconstruction_loss: 2.4231 - kl_loss: 5.6797 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5794 - reconstruction_loss: 2.4242 - kl_loss: 5.6972 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5749 - reconstruction_loss: 2.4206 - kl_loss: 5.6768 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5746 - reconstruction_loss: 2.4285 - kl_loss: 5.6614 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5868 - reconstruction_loss: 2.4464 - kl_loss: 5.7185 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.6011 - reconstruction_loss: 2.4423 - kl_loss: 5.7179\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.6009 - reconstruction_loss: 2.4338 - kl_loss: 5.7180 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5775 - reconstruction_loss: 2.4205 - kl_loss: 5.7009 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5621 - reconstruction_loss: 2.4023 - kl_loss: 5.6944 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5554 - reconstruction_loss: 2.4084 - kl_loss: 5.6946 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5462 - reconstruction_loss: 2.4016 - kl_loss: 5.6759 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5349 - reconstruction_loss: 2.3873 - kl_loss: 5.6769 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5272 - reconstruction_loss: 2.3927 - kl_loss: 5.6776 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5264 - reconstruction_loss: 2.3900 - kl_loss: 5.6699 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.5270 - reconstruction_loss: 2.3982 - kl_loss: 5.6636\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5269 - reconstruction_loss: 2.3901 - kl_loss: 5.6633 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5278 - reconstruction_loss: 2.3810 - kl_loss: 5.6723 - lr: 6.2500e-05\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5128 - reconstruction_loss: 2.3774 - kl_loss: 5.6598 - lr: 6.2500e-05\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5141 - reconstruction_loss: 2.3734 - kl_loss: 5.6657 - lr: 6.2500e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5259 - reconstruction_loss: 2.3751 - kl_loss: 5.6641 - lr: 6.2500e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5117 - reconstruction_loss: 2.3722 - kl_loss: 5.6767 - lr: 6.2500e-05\n",
      "Epoch 70/300\n",
      "188/196 [===========================>..] - ETA: 0s - loss: 3.5058 - reconstruction_loss: 2.3820 - kl_loss: 5.6713\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5061 - reconstruction_loss: 2.3730 - kl_loss: 5.6709 - lr: 6.2500e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5172 - reconstruction_loss: 2.3766 - kl_loss: 5.6713 - lr: 3.1250e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5202 - reconstruction_loss: 2.3731 - kl_loss: 5.6678 - lr: 3.1250e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5136 - reconstruction_loss: 2.3759 - kl_loss: 5.6526 - lr: 3.1250e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5248 - reconstruction_loss: 2.3698 - kl_loss: 5.6626 - lr: 3.1250e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5187 - reconstruction_loss: 2.3709 - kl_loss: 5.6613 - lr: 3.1250e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5167 - reconstruction_loss: 2.3689 - kl_loss: 5.6663 - lr: 3.1250e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4998 - reconstruction_loss: 2.3678 - kl_loss: 5.6497 - lr: 3.1250e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5018 - reconstruction_loss: 2.3623 - kl_loss: 5.6583 - lr: 3.1250e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5089 - reconstruction_loss: 2.3626 - kl_loss: 5.6519 - lr: 3.1250e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5040 - reconstruction_loss: 2.3602 - kl_loss: 5.6671 - lr: 3.1250e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5018 - reconstruction_loss: 2.3605 - kl_loss: 5.6552 - lr: 3.1250e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4972 - reconstruction_loss: 2.3571 - kl_loss: 5.6685 - lr: 3.1250e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5045 - reconstruction_loss: 2.3598 - kl_loss: 5.6617 - lr: 3.1250e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5018 - reconstruction_loss: 2.3597 - kl_loss: 5.6648 - lr: 3.1250e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5023 - reconstruction_loss: 2.3543 - kl_loss: 5.6590 - lr: 3.1250e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4802 - reconstruction_loss: 2.3545 - kl_loss: 5.6612 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4979 - reconstruction_loss: 2.3588 - kl_loss: 5.6531 - lr: 3.1250e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5032 - reconstruction_loss: 2.3568 - kl_loss: 5.6620\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5032 - reconstruction_loss: 2.3568 - kl_loss: 5.6620 - lr: 3.1250e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4941 - reconstruction_loss: 2.3524 - kl_loss: 5.6627 - lr: 1.5625e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4926 - reconstruction_loss: 2.3507 - kl_loss: 5.6610 - lr: 1.5625e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4939 - reconstruction_loss: 2.3496 - kl_loss: 5.6569 - lr: 1.5625e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5067 - reconstruction_loss: 2.3505 - kl_loss: 5.6615 - lr: 1.5625e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4973 - reconstruction_loss: 2.3488 - kl_loss: 5.6618 - lr: 1.5625e-05\n",
      "Epoch 94/300\n",
      "189/196 [===========================>..] - ETA: 0s - loss: 3.4862 - reconstruction_loss: 2.3601 - kl_loss: 5.6586\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4863 - reconstruction_loss: 2.3503 - kl_loss: 5.6596 - lr: 1.5625e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4910 - reconstruction_loss: 2.3494 - kl_loss: 5.6621 - lr: 7.8125e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4960 - reconstruction_loss: 2.3493 - kl_loss: 5.6589 - lr: 7.8125e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4842 - reconstruction_loss: 2.3470 - kl_loss: 5.6627 - lr: 7.8125e-06\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4863 - reconstruction_loss: 2.3454 - kl_loss: 5.6596 - lr: 7.8125e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4715 - reconstruction_loss: 2.3467 - kl_loss: 5.6556 - lr: 7.8125e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4961 - reconstruction_loss: 2.3479 - kl_loss: 5.6568 - lr: 7.8125e-06\n",
      "Epoch 101/300\n",
      "189/196 [===========================>..] - ETA: 0s - loss: 3.5011 - reconstruction_loss: 2.3556 - kl_loss: 5.6627\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5005 - reconstruction_loss: 2.3478 - kl_loss: 5.6631 - lr: 7.8125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4944 - reconstruction_loss: 2.3468 - kl_loss: 5.6662 - lr: 3.9063e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4891 - reconstruction_loss: 2.3454 - kl_loss: 5.6648 - lr: 3.9063e-06\n",
      "Epoch 104/300\n",
      "189/196 [===========================>..] - ETA: 0s - loss: 3.4886 - reconstruction_loss: 2.3504 - kl_loss: 5.6646\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4883 - reconstruction_loss: 2.3446 - kl_loss: 5.6646 - lr: 3.9063e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4921 - reconstruction_loss: 2.3443 - kl_loss: 5.6645 - lr: 1.9531e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4886 - reconstruction_loss: 2.3450 - kl_loss: 5.6628 - lr: 1.9531e-06\n",
      "Epoch 107/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4981 - reconstruction_loss: 2.3532 - kl_loss: 5.6613\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4979 - reconstruction_loss: 2.3449 - kl_loss: 5.6616 - lr: 1.9531e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4826 - reconstruction_loss: 2.3450 - kl_loss: 5.6636 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4828 - reconstruction_loss: 2.3425 - kl_loss: 5.6628 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4799 - reconstruction_loss: 2.3464 - kl_loss: 5.6633 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4764 - reconstruction_loss: 2.3447 - kl_loss: 5.6644 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4855 - reconstruction_loss: 2.3422 - kl_loss: 5.6634 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4787 - reconstruction_loss: 2.3437 - kl_loss: 5.6617 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4861 - reconstruction_loss: 2.3455 - kl_loss: 5.6630 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.5085 - reconstruction_loss: 2.3438 - kl_loss: 5.6632 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4989 - reconstruction_loss: 2.3452 - kl_loss: 5.6613 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4865 - reconstruction_loss: 2.3451 - kl_loss: 5.6617 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4867 - reconstruction_loss: 2.3440 - kl_loss: 5.6610 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4890 - reconstruction_loss: 2.3434 - kl_loss: 5.6613 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4823 - reconstruction_loss: 2.3420 - kl_loss: 5.6592 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4791 - reconstruction_loss: 2.3439 - kl_loss: 5.6585 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4919 - reconstruction_loss: 2.3456 - kl_loss: 5.6601 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4918 - reconstruction_loss: 2.3452 - kl_loss: 5.6607 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4903 - reconstruction_loss: 2.3436 - kl_loss: 5.6612 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4816 - reconstruction_loss: 2.3452 - kl_loss: 5.6619 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4780 - reconstruction_loss: 2.3461 - kl_loss: 5.6611 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4960 - reconstruction_loss: 2.3441 - kl_loss: 5.6636 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4825 - reconstruction_loss: 2.3437 - kl_loss: 5.6625 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4814 - reconstruction_loss: 2.3434 - kl_loss: 5.6611 - lr: 1.0000e-06\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 3.4774 - reconstruction_loss: 2.3433 - kl_loss: 5.6606 - lr: 1.0000e-06\n",
      "Epoch 130: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 15:25:01,808]\u001b[0m Trial 71 finished with value: 0.004594004908521924 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 21, 'encoder_units_l2': 832, 'encoder_units_l3': 373, 'decoder_layers': 3, 'decoder_units_l1': 148, 'decoder_units_l2': 60, 'decoder_units_l3': 839, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 13.4623 - reconstruction_loss: 9.2525 - kl_loss: 3.9253 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 7.3342 - reconstruction_loss: 5.9721 - kl_loss: 4.8531 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 6.1185 - reconstruction_loss: 4.8315 - kl_loss: 5.2959 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 5.3808 - reconstruction_loss: 4.2051 - kl_loss: 5.5408 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 5.1319 - reconstruction_loss: 3.9133 - kl_loss: 5.6598 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.8446 - reconstruction_loss: 3.6555 - kl_loss: 5.7658 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.6892 - reconstruction_loss: 3.5098 - kl_loss: 5.7764 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.6064 - reconstruction_loss: 3.4026 - kl_loss: 5.7913 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.5641 - reconstruction_loss: 3.2848 - kl_loss: 5.7640 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.3209 - reconstruction_loss: 3.1357 - kl_loss: 5.8467 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.2766 - reconstruction_loss: 3.0614 - kl_loss: 5.8554 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.2368 - reconstruction_loss: 3.0312 - kl_loss: 5.8442 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.1415 - reconstruction_loss: 2.9613 - kl_loss: 5.8301 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.2047 - reconstruction_loss: 2.9438 - kl_loss: 5.8223 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.0388 - reconstruction_loss: 2.8790 - kl_loss: 5.8028 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.9889 - reconstruction_loss: 2.7985 - kl_loss: 5.8293 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.9944 - reconstruction_loss: 2.7971 - kl_loss: 5.8294 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.9629 - reconstruction_loss: 2.7867 - kl_loss: 5.8176 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.9626 - reconstruction_loss: 2.7916 - kl_loss: 5.8137 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.9060 - reconstruction_loss: 2.7818 - kl_loss: 5.8528 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 4.0266 - reconstruction_loss: 2.7984 - kl_loss: 5.8142\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.0255 - reconstruction_loss: 2.7880 - kl_loss: 5.8132 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.8073 - reconstruction_loss: 2.6328 - kl_loss: 5.8528 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.7826 - reconstruction_loss: 2.6125 - kl_loss: 5.8765 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.7514 - reconstruction_loss: 2.5741 - kl_loss: 5.8626 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.7660 - reconstruction_loss: 2.5823 - kl_loss: 5.8725 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.7149 - reconstruction_loss: 2.5462 - kl_loss: 5.8332 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.7296 - reconstruction_loss: 2.5405 - kl_loss: 5.8431 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.7050 - reconstruction_loss: 2.5282 - kl_loss: 5.8300 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.7155 - reconstruction_loss: 2.5688 - kl_loss: 5.8638 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.8219 - reconstruction_loss: 2.6045 - kl_loss: 5.8554 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.7799 - reconstruction_loss: 2.5848 - kl_loss: 5.8153\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.7797 - reconstruction_loss: 2.5848 - kl_loss: 5.8153 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6899 - reconstruction_loss: 2.5029 - kl_loss: 5.8305 - lr: 2.5000e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6722 - reconstruction_loss: 2.4856 - kl_loss: 5.8402 - lr: 2.5000e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6795 - reconstruction_loss: 2.5069 - kl_loss: 5.8574 - lr: 2.5000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6606 - reconstruction_loss: 2.4999 - kl_loss: 5.8439 - lr: 2.5000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6373 - reconstruction_loss: 2.4677 - kl_loss: 5.8161 - lr: 2.5000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6337 - reconstruction_loss: 2.4629 - kl_loss: 5.8269 - lr: 2.5000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6457 - reconstruction_loss: 2.4617 - kl_loss: 5.8267 - lr: 2.5000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6444 - reconstruction_loss: 2.4831 - kl_loss: 5.8457 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6691 - reconstruction_loss: 2.4829 - kl_loss: 5.8465 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6516 - reconstruction_loss: 2.4530 - kl_loss: 5.8341 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5938 - reconstruction_loss: 2.4525 - kl_loss: 5.8137 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6294 - reconstruction_loss: 2.4597 - kl_loss: 5.8345 - lr: 2.5000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6319 - reconstruction_loss: 2.4501 - kl_loss: 5.8432 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6294 - reconstruction_loss: 2.4529 - kl_loss: 5.7813 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5845 - reconstruction_loss: 2.4244 - kl_loss: 5.7947 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6061 - reconstruction_loss: 2.4240 - kl_loss: 5.7979 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6067 - reconstruction_loss: 2.4458 - kl_loss: 5.8201 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5974 - reconstruction_loss: 2.4351 - kl_loss: 5.7958\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5972 - reconstruction_loss: 2.4255 - kl_loss: 5.7962 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5753 - reconstruction_loss: 2.4001 - kl_loss: 5.7938 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5532 - reconstruction_loss: 2.3844 - kl_loss: 5.8051 - lr: 1.2500e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5748 - reconstruction_loss: 2.4034 - kl_loss: 5.7967 - lr: 1.2500e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5578 - reconstruction_loss: 2.3923 - kl_loss: 5.7970 - lr: 1.2500e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5533 - reconstruction_loss: 2.3916 - kl_loss: 5.8076\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5533 - reconstruction_loss: 2.3916 - kl_loss: 5.8076 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5441 - reconstruction_loss: 2.3742 - kl_loss: 5.8238 - lr: 6.2500e-05\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5381 - reconstruction_loss: 2.3715 - kl_loss: 5.8135 - lr: 6.2500e-05\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5392 - reconstruction_loss: 2.3677 - kl_loss: 5.8235 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5207 - reconstruction_loss: 2.3648 - kl_loss: 5.8086 - lr: 6.2500e-05\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5485 - reconstruction_loss: 2.3668 - kl_loss: 5.8062 - lr: 6.2500e-05\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5391 - reconstruction_loss: 2.3617 - kl_loss: 5.8021 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5193 - reconstruction_loss: 2.3636 - kl_loss: 5.8107 - lr: 6.2500e-05\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5388 - reconstruction_loss: 2.3662 - kl_loss: 5.8092 - lr: 6.2500e-05\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5411 - reconstruction_loss: 2.3708 - kl_loss: 5.8000\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5410 - reconstruction_loss: 2.3708 - kl_loss: 5.8000 - lr: 6.2500e-05\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5161 - reconstruction_loss: 2.3570 - kl_loss: 5.7968 - lr: 3.1250e-05\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5217 - reconstruction_loss: 2.3556 - kl_loss: 5.8121 - lr: 3.1250e-05\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5344 - reconstruction_loss: 2.3629 - kl_loss: 5.8012 - lr: 3.1250e-05\n",
      "Epoch 67/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5337 - reconstruction_loss: 2.3704 - kl_loss: 5.8042\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5336 - reconstruction_loss: 2.3621 - kl_loss: 5.8037 - lr: 3.1250e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5184 - reconstruction_loss: 2.3548 - kl_loss: 5.7938 - lr: 1.5625e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5174 - reconstruction_loss: 2.3489 - kl_loss: 5.8031 - lr: 1.5625e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5228 - reconstruction_loss: 2.3553 - kl_loss: 5.7969 - lr: 1.5625e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5243 - reconstruction_loss: 2.3538 - kl_loss: 5.7977 - lr: 1.5625e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5040 - reconstruction_loss: 2.3462 - kl_loss: 5.8015 - lr: 1.5625e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5162 - reconstruction_loss: 2.3476 - kl_loss: 5.8034 - lr: 1.5625e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5118 - reconstruction_loss: 2.3514 - kl_loss: 5.7931 - lr: 1.5625e-05\n",
      "Epoch 75/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.5159 - reconstruction_loss: 2.3571 - kl_loss: 5.7961\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5158 - reconstruction_loss: 2.3479 - kl_loss: 5.7967 - lr: 1.5625e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5349 - reconstruction_loss: 2.3479 - kl_loss: 5.7996 - lr: 7.8125e-06\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4991 - reconstruction_loss: 2.3461 - kl_loss: 5.8110 - lr: 7.8125e-06\n",
      "Epoch 78/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.5233 - reconstruction_loss: 2.3557 - kl_loss: 5.8023\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5230 - reconstruction_loss: 2.3482 - kl_loss: 5.8022 - lr: 7.8125e-06\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5208 - reconstruction_loss: 2.3476 - kl_loss: 5.8055 - lr: 3.9063e-06\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5251 - reconstruction_loss: 2.3464 - kl_loss: 5.8039 - lr: 3.9063e-06\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5078 - reconstruction_loss: 2.3432 - kl_loss: 5.8055 - lr: 3.9063e-06\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5068 - reconstruction_loss: 2.3442 - kl_loss: 5.8085 - lr: 3.9063e-06\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5151 - reconstruction_loss: 2.3450 - kl_loss: 5.8055 - lr: 3.9063e-06\n",
      "Epoch 84/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5257 - reconstruction_loss: 2.3555 - kl_loss: 5.8051\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5255 - reconstruction_loss: 2.3468 - kl_loss: 5.8052 - lr: 3.9063e-06\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5127 - reconstruction_loss: 2.3447 - kl_loss: 5.8016 - lr: 1.9531e-06\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5099 - reconstruction_loss: 2.3467 - kl_loss: 5.8073 - lr: 1.9531e-06\n",
      "Epoch 87/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5021 - reconstruction_loss: 2.3514 - kl_loss: 5.8104\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5022 - reconstruction_loss: 2.3430 - kl_loss: 5.8103 - lr: 1.9531e-06\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5188 - reconstruction_loss: 2.3402 - kl_loss: 5.8110 - lr: 1.0000e-06\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4988 - reconstruction_loss: 2.3427 - kl_loss: 5.8091 - lr: 1.0000e-06\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5179 - reconstruction_loss: 2.3436 - kl_loss: 5.8099 - lr: 1.0000e-06\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5100 - reconstruction_loss: 2.3443 - kl_loss: 5.8100 - lr: 1.0000e-06\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5157 - reconstruction_loss: 2.3449 - kl_loss: 5.8106 - lr: 1.0000e-06\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4925 - reconstruction_loss: 2.3431 - kl_loss: 5.8105 - lr: 1.0000e-06\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5116 - reconstruction_loss: 2.3414 - kl_loss: 5.8110 - lr: 1.0000e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5094 - reconstruction_loss: 2.3426 - kl_loss: 5.8106 - lr: 1.0000e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5221 - reconstruction_loss: 2.3468 - kl_loss: 5.8097 - lr: 1.0000e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5203 - reconstruction_loss: 2.3434 - kl_loss: 5.8126 - lr: 1.0000e-06\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5183 - reconstruction_loss: 2.3410 - kl_loss: 5.8136 - lr: 1.0000e-06\n",
      "Epoch 98: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 15:27:45,390]\u001b[0m Trial 72 finished with value: 0.004584902656575509 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 29, 'encoder_units_l2': 595, 'encoder_units_l3': 533, 'decoder_layers': 5, 'decoder_units_l1': 122, 'decoder_units_l2': 37, 'decoder_units_l3': 722, 'decoder_units_l4': 37, 'decoder_units_l5': 21, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 7ms/step - loss: 13.0947 - reconstruction_loss: 9.0349 - kl_loss: 3.8347 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 8.0440 - reconstruction_loss: 7.0981 - kl_loss: 4.2563 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 7.0540 - reconstruction_loss: 5.7405 - kl_loss: 4.9235 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.7742 - reconstruction_loss: 4.5420 - kl_loss: 5.2020 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.3115 - reconstruction_loss: 4.1542 - kl_loss: 5.3931 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 5.0357 - reconstruction_loss: 3.8750 - kl_loss: 5.5013 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.8449 - reconstruction_loss: 3.6946 - kl_loss: 5.5486 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 4.7287 - reconstruction_loss: 3.5667 - kl_loss: 5.5587 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 4.6295 - reconstruction_loss: 3.4507 - kl_loss: 5.5989 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.5087 - reconstruction_loss: 3.3428 - kl_loss: 5.6376 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.4778 - reconstruction_loss: 3.2595 - kl_loss: 5.6790 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.3347 - reconstruction_loss: 3.1423 - kl_loss: 5.6905 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.2961 - reconstruction_loss: 3.0992 - kl_loss: 5.7057 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.2158 - reconstruction_loss: 3.0295 - kl_loss: 5.7059 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.1880 - reconstruction_loss: 2.9916 - kl_loss: 5.7103 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.1304 - reconstruction_loss: 2.9480 - kl_loss: 5.7042 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.0899 - reconstruction_loss: 2.9099 - kl_loss: 5.7385 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.0767 - reconstruction_loss: 2.8729 - kl_loss: 5.7588 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.0079 - reconstruction_loss: 2.8538 - kl_loss: 5.7490 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.9280 - reconstruction_loss: 2.7880 - kl_loss: 5.7522 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.9272 - reconstruction_loss: 2.7787 - kl_loss: 5.7095 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9826 - reconstruction_loss: 2.7773 - kl_loss: 5.7375 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9534 - reconstruction_loss: 2.7594 - kl_loss: 5.7293 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.9144 - reconstruction_loss: 2.7705 - kl_loss: 5.7255 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9010 - reconstruction_loss: 2.7064 - kl_loss: 5.7671 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.9172 - reconstruction_loss: 2.7315 - kl_loss: 5.7649 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.8711 - reconstruction_loss: 2.7396 - kl_loss: 5.7594 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.8697 - reconstruction_loss: 2.6879 - kl_loss: 5.7282 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8658 - reconstruction_loss: 2.7039 - kl_loss: 5.7457 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8896 - reconstruction_loss: 2.6831 - kl_loss: 5.7412 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7998 - reconstruction_loss: 2.6414 - kl_loss: 5.7154 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8623 - reconstruction_loss: 2.7155 - kl_loss: 5.7278 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8435 - reconstruction_loss: 2.6873 - kl_loss: 5.7325 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "190/196 [============================>.] - ETA: 0s - loss: 3.8247 - reconstruction_loss: 2.6712 - kl_loss: 5.7475\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8245 - reconstruction_loss: 2.6627 - kl_loss: 5.7461 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7452 - reconstruction_loss: 2.5658 - kl_loss: 5.7217 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6712 - reconstruction_loss: 2.5273 - kl_loss: 5.7334 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6879 - reconstruction_loss: 2.5360 - kl_loss: 5.7542 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6850 - reconstruction_loss: 2.5287 - kl_loss: 5.7354 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6726 - reconstruction_loss: 2.5139 - kl_loss: 5.7426 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6428 - reconstruction_loss: 2.5048 - kl_loss: 5.7357 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6661 - reconstruction_loss: 2.4953 - kl_loss: 5.7152 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.6262 - reconstruction_loss: 2.4896 - kl_loss: 5.6914 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6187 - reconstruction_loss: 2.4801 - kl_loss: 5.7122 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6419 - reconstruction_loss: 2.4887 - kl_loss: 5.7174 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6194 - reconstruction_loss: 2.4726 - kl_loss: 5.7030 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6293 - reconstruction_loss: 2.4859 - kl_loss: 5.7079 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6400 - reconstruction_loss: 2.4788 - kl_loss: 5.7229 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.6196 - reconstruction_loss: 2.4983 - kl_loss: 5.6615\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6197 - reconstruction_loss: 2.4915 - kl_loss: 5.6614 - lr: 5.0000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5916 - reconstruction_loss: 2.4312 - kl_loss: 5.6517 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5545 - reconstruction_loss: 2.4240 - kl_loss: 5.6588 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5649 - reconstruction_loss: 2.4196 - kl_loss: 5.6596 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5544 - reconstruction_loss: 2.4063 - kl_loss: 5.6510 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5436 - reconstruction_loss: 2.4157 - kl_loss: 5.6370 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5576 - reconstruction_loss: 2.4246 - kl_loss: 5.6463 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5459 - reconstruction_loss: 2.3992 - kl_loss: 5.6618 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5468 - reconstruction_loss: 2.3957 - kl_loss: 5.6616 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5377 - reconstruction_loss: 2.4030 - kl_loss: 5.6576 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5406 - reconstruction_loss: 2.4010 - kl_loss: 5.6758 - lr: 2.5000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5243 - reconstruction_loss: 2.3941 - kl_loss: 5.6316 - lr: 2.5000e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5225 - reconstruction_loss: 2.3924 - kl_loss: 5.6474 - lr: 2.5000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5627 - reconstruction_loss: 2.4241 - kl_loss: 5.6518 - lr: 2.5000e-04\n",
      "Epoch 62/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.5439 - reconstruction_loss: 2.4142 - kl_loss: 5.6782\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5439 - reconstruction_loss: 2.4047 - kl_loss: 5.6778 - lr: 2.5000e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5187 - reconstruction_loss: 2.3782 - kl_loss: 5.6533 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5116 - reconstruction_loss: 2.3727 - kl_loss: 5.6670 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5035 - reconstruction_loss: 2.3745 - kl_loss: 5.6659 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5092 - reconstruction_loss: 2.3732 - kl_loss: 5.6469 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.5108 - reconstruction_loss: 2.3617 - kl_loss: 5.6704 - lr: 1.2500e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4915 - reconstruction_loss: 2.3506 - kl_loss: 5.6472 - lr: 1.2500e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4981 - reconstruction_loss: 2.3655 - kl_loss: 5.6341 - lr: 1.2500e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.5029 - reconstruction_loss: 2.3687 - kl_loss: 5.6397 - lr: 1.2500e-04\n",
      "Epoch 71/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.5012 - reconstruction_loss: 2.3714 - kl_loss: 5.6517\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5011 - reconstruction_loss: 2.3620 - kl_loss: 5.6509 - lr: 1.2500e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4941 - reconstruction_loss: 2.3520 - kl_loss: 5.6536 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4803 - reconstruction_loss: 2.3508 - kl_loss: 5.6416 - lr: 6.2500e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4874 - reconstruction_loss: 2.3542 - kl_loss: 5.6233 - lr: 6.2500e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4807 - reconstruction_loss: 2.3512 - kl_loss: 5.6355 - lr: 6.2500e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4814 - reconstruction_loss: 2.3441 - kl_loss: 5.6458 - lr: 6.2500e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4727 - reconstruction_loss: 2.3464 - kl_loss: 5.6345 - lr: 6.2500e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4766 - reconstruction_loss: 2.3434 - kl_loss: 5.6385 - lr: 6.2500e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4730 - reconstruction_loss: 2.3471 - kl_loss: 5.6277 - lr: 6.2500e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4708 - reconstruction_loss: 2.3412 - kl_loss: 5.6396 - lr: 6.2500e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4570 - reconstruction_loss: 2.3371 - kl_loss: 5.6280 - lr: 6.2500e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4730 - reconstruction_loss: 2.3406 - kl_loss: 5.6371 - lr: 6.2500e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4690 - reconstruction_loss: 2.3385 - kl_loss: 5.6388 - lr: 6.2500e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4837 - reconstruction_loss: 2.3345 - kl_loss: 5.6396 - lr: 6.2500e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4679 - reconstruction_loss: 2.3476 - kl_loss: 5.6343 - lr: 6.2500e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4792 - reconstruction_loss: 2.3420 - kl_loss: 5.6330 - lr: 6.2500e-05\n",
      "Epoch 87/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4625 - reconstruction_loss: 2.3445 - kl_loss: 5.6443\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4626 - reconstruction_loss: 2.3360 - kl_loss: 5.6442 - lr: 6.2500e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4741 - reconstruction_loss: 2.3355 - kl_loss: 5.6359 - lr: 3.1250e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4565 - reconstruction_loss: 2.3271 - kl_loss: 5.6375 - lr: 3.1250e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4695 - reconstruction_loss: 2.3283 - kl_loss: 5.6536 - lr: 3.1250e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4643 - reconstruction_loss: 2.3194 - kl_loss: 5.6433 - lr: 3.1250e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4450 - reconstruction_loss: 2.3221 - kl_loss: 5.6418 - lr: 3.1250e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4417 - reconstruction_loss: 2.3247 - kl_loss: 5.6461 - lr: 3.1250e-05\n",
      "Epoch 94/300\n",
      "189/196 [===========================>..] - ETA: 0s - loss: 3.4646 - reconstruction_loss: 2.3369 - kl_loss: 5.6379\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4646 - reconstruction_loss: 2.3290 - kl_loss: 5.6370 - lr: 3.1250e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.4656 - reconstruction_loss: 2.3233 - kl_loss: 5.6481 - lr: 1.5625e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4601 - reconstruction_loss: 2.3230 - kl_loss: 5.6424 - lr: 1.5625e-05\n",
      "Epoch 97/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4615 - reconstruction_loss: 2.3278 - kl_loss: 5.6538\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4614 - reconstruction_loss: 2.3224 - kl_loss: 5.6533 - lr: 1.5625e-05\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4596 - reconstruction_loss: 2.3198 - kl_loss: 5.6464 - lr: 7.8125e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4568 - reconstruction_loss: 2.3213 - kl_loss: 5.6512 - lr: 7.8125e-06\n",
      "Epoch 100/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4612 - reconstruction_loss: 2.3288 - kl_loss: 5.6464\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4611 - reconstruction_loss: 2.3202 - kl_loss: 5.6466 - lr: 7.8125e-06\n",
      "Epoch 101/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 1s 7ms/step - loss: 3.4568 - reconstruction_loss: 2.3199 - kl_loss: 5.6420 - lr: 3.9063e-06\n",
      "Epoch 101: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 15:30:12,188]\u001b[0m Trial 73 finished with value: 0.0045410318748515975 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 39, 'encoder_units_l2': 672, 'encoder_units_l3': 909, 'decoder_layers': 4, 'decoder_units_l1': 107, 'decoder_units_l2': 48, 'decoder_units_l3': 614, 'decoder_units_l4': 56, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 12.4326 - reconstruction_loss: 8.5840 - kl_loss: 4.0385 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 7.2918 - reconstruction_loss: 5.8795 - kl_loss: 4.7670 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.8173 - reconstruction_loss: 4.6121 - kl_loss: 5.1323 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 5.2899 - reconstruction_loss: 4.1611 - kl_loss: 5.3747 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 5.0294 - reconstruction_loss: 3.8572 - kl_loss: 5.5155 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 4.8425 - reconstruction_loss: 3.6743 - kl_loss: 5.6167 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.7126 - reconstruction_loss: 3.5371 - kl_loss: 5.6883 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 4.6191 - reconstruction_loss: 3.4572 - kl_loss: 5.6896 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 4.5135 - reconstruction_loss: 3.3287 - kl_loss: 5.7464 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.4472 - reconstruction_loss: 3.2488 - kl_loss: 5.7846 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.3680 - reconstruction_loss: 3.1654 - kl_loss: 5.7757 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.2761 - reconstruction_loss: 3.0758 - kl_loss: 5.8385 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.2837 - reconstruction_loss: 3.0681 - kl_loss: 5.8328 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 4.1693 - reconstruction_loss: 2.9663 - kl_loss: 5.8545 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.1212 - reconstruction_loss: 2.9089 - kl_loss: 5.9128 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 4.1094 - reconstruction_loss: 2.8716 - kl_loss: 5.8820 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 4.0317 - reconstruction_loss: 2.8017 - kl_loss: 5.8748 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.9101 - reconstruction_loss: 2.7282 - kl_loss: 5.8759 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8244 - reconstruction_loss: 2.6566 - kl_loss: 5.8685 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.9487 - reconstruction_loss: 2.7155 - kl_loss: 5.9360 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.8515 - reconstruction_loss: 2.6226 - kl_loss: 5.8688 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7489 - reconstruction_loss: 2.6026 - kl_loss: 5.8444 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7539 - reconstruction_loss: 2.6047 - kl_loss: 5.8351 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7272 - reconstruction_loss: 2.5740 - kl_loss: 5.8151 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7570 - reconstruction_loss: 2.6053 - kl_loss: 5.8813 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.7734 - reconstruction_loss: 2.5706 - kl_loss: 5.8442 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.6808 - reconstruction_loss: 2.5115 - kl_loss: 5.7802 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6676 - reconstruction_loss: 2.5128 - kl_loss: 5.7740 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.7667 - reconstruction_loss: 2.5544 - kl_loss: 5.8793 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 3.6696 - reconstruction_loss: 2.4980 - kl_loss: 5.7819 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 3.6875 - reconstruction_loss: 2.5005 - kl_loss: 5.7924 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.7495 - reconstruction_loss: 2.5586 - kl_loss: 5.8102 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.7677 - reconstruction_loss: 2.5279 - kl_loss: 5.7715\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7658 - reconstruction_loss: 2.5186 - kl_loss: 5.7719 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6068 - reconstruction_loss: 2.4382 - kl_loss: 5.7314 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5825 - reconstruction_loss: 2.4322 - kl_loss: 5.7226 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5913 - reconstruction_loss: 2.4310 - kl_loss: 5.7437 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5874 - reconstruction_loss: 2.4189 - kl_loss: 5.7268 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5808 - reconstruction_loss: 2.4096 - kl_loss: 5.7300 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5662 - reconstruction_loss: 2.4051 - kl_loss: 5.7292 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5743 - reconstruction_loss: 2.4125 - kl_loss: 5.7172 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5705 - reconstruction_loss: 2.4081 - kl_loss: 5.7123 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5794 - reconstruction_loss: 2.4057 - kl_loss: 5.7190 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5656 - reconstruction_loss: 2.4144 - kl_loss: 5.7302 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5528 - reconstruction_loss: 2.4140 - kl_loss: 5.7346 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.5935 - reconstruction_loss: 2.4355 - kl_loss: 5.7784\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5933 - reconstruction_loss: 2.4264 - kl_loss: 5.7800 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5774 - reconstruction_loss: 2.3975 - kl_loss: 5.8362 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5518 - reconstruction_loss: 2.3819 - kl_loss: 5.7953 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5120 - reconstruction_loss: 2.3682 - kl_loss: 5.7404 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5197 - reconstruction_loss: 2.3743 - kl_loss: 5.7383 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5180 - reconstruction_loss: 2.3686 - kl_loss: 5.7191 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5136 - reconstruction_loss: 2.3734 - kl_loss: 5.7373 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5273 - reconstruction_loss: 2.3769 - kl_loss: 5.7423 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5192 - reconstruction_loss: 2.3599 - kl_loss: 5.7267 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5097 - reconstruction_loss: 2.3631 - kl_loss: 5.7308 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5078 - reconstruction_loss: 2.3529 - kl_loss: 5.7196 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4893 - reconstruction_loss: 2.3437 - kl_loss: 5.7076 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4906 - reconstruction_loss: 2.3556 - kl_loss: 5.7116 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5156 - reconstruction_loss: 2.3637 - kl_loss: 5.7445 - lr: 2.5000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5023 - reconstruction_loss: 2.3425 - kl_loss: 5.7264\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5022 - reconstruction_loss: 2.3425 - kl_loss: 5.7264 - lr: 2.5000e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4821 - reconstruction_loss: 2.3316 - kl_loss: 5.7190 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4744 - reconstruction_loss: 2.3230 - kl_loss: 5.7193 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4617 - reconstruction_loss: 2.3217 - kl_loss: 5.7091 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4632 - reconstruction_loss: 2.3276 - kl_loss: 5.7036 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4737 - reconstruction_loss: 2.3226 - kl_loss: 5.7145 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4831 - reconstruction_loss: 2.3279 - kl_loss: 5.7273\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4828 - reconstruction_loss: 2.3198 - kl_loss: 5.7277 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4694 - reconstruction_loss: 2.3178 - kl_loss: 5.7085 - lr: 6.2500e-05\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4721 - reconstruction_loss: 2.3191 - kl_loss: 5.7054 - lr: 6.2500e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4538 - reconstruction_loss: 2.3104 - kl_loss: 5.7002 - lr: 6.2500e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4703 - reconstruction_loss: 2.3150 - kl_loss: 5.7092 - lr: 6.2500e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4515 - reconstruction_loss: 2.3079 - kl_loss: 5.7134 - lr: 6.2500e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4525 - reconstruction_loss: 2.3059 - kl_loss: 5.7095 - lr: 6.2500e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4679 - reconstruction_loss: 2.3082 - kl_loss: 5.7083 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4557 - reconstruction_loss: 2.3052 - kl_loss: 5.7141 - lr: 6.2500e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4459 - reconstruction_loss: 2.2999 - kl_loss: 5.7295 - lr: 6.2500e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4454 - reconstruction_loss: 2.3010 - kl_loss: 5.7195 - lr: 6.2500e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4427 - reconstruction_loss: 2.2981 - kl_loss: 5.7258 - lr: 6.2500e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4486 - reconstruction_loss: 2.2948 - kl_loss: 5.7290 - lr: 6.2500e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4539 - reconstruction_loss: 2.2967 - kl_loss: 5.7155 - lr: 6.2500e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4421 - reconstruction_loss: 2.2989 - kl_loss: 5.7093 - lr: 6.2500e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4448 - reconstruction_loss: 2.2898 - kl_loss: 5.7189 - lr: 6.2500e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4512 - reconstruction_loss: 2.2933 - kl_loss: 5.7133 - lr: 6.2500e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4510 - reconstruction_loss: 2.2950 - kl_loss: 5.7187 - lr: 6.2500e-05\n",
      "Epoch 83/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4485 - reconstruction_loss: 2.2999 - kl_loss: 5.7156\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4484 - reconstruction_loss: 2.2919 - kl_loss: 5.7158 - lr: 6.2500e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4276 - reconstruction_loss: 2.2855 - kl_loss: 5.7176 - lr: 3.1250e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4447 - reconstruction_loss: 2.2818 - kl_loss: 5.7262 - lr: 3.1250e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4354 - reconstruction_loss: 2.2838 - kl_loss: 5.7217 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4354 - reconstruction_loss: 2.2843 - kl_loss: 5.7132 - lr: 3.1250e-05\n",
      "Epoch 88/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.4287 - reconstruction_loss: 2.2960 - kl_loss: 5.7235\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4289 - reconstruction_loss: 2.2885 - kl_loss: 5.7232 - lr: 3.1250e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4394 - reconstruction_loss: 2.2846 - kl_loss: 5.7271 - lr: 1.5625e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4333 - reconstruction_loss: 2.2803 - kl_loss: 5.7242 - lr: 1.5625e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4183 - reconstruction_loss: 2.2752 - kl_loss: 5.7169 - lr: 1.5625e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4276 - reconstruction_loss: 2.2766 - kl_loss: 5.7174 - lr: 1.5625e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4220 - reconstruction_loss: 2.2756 - kl_loss: 5.7287 - lr: 1.5625e-05\n",
      "Epoch 94/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4308 - reconstruction_loss: 2.2825 - kl_loss: 5.7226\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4306 - reconstruction_loss: 2.2745 - kl_loss: 5.7224 - lr: 1.5625e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4091 - reconstruction_loss: 2.2709 - kl_loss: 5.7214 - lr: 7.8125e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4240 - reconstruction_loss: 2.2726 - kl_loss: 5.7256 - lr: 7.8125e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4373 - reconstruction_loss: 2.2711 - kl_loss: 5.7236 - lr: 7.8125e-06\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4339 - reconstruction_loss: 2.2718 - kl_loss: 5.7148 - lr: 7.8125e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4215 - reconstruction_loss: 2.2720 - kl_loss: 5.7198 - lr: 7.8125e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4248 - reconstruction_loss: 2.2703 - kl_loss: 5.7223 - lr: 7.8125e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4129 - reconstruction_loss: 2.2696 - kl_loss: 5.7165 - lr: 7.8125e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4253 - reconstruction_loss: 2.2706 - kl_loss: 5.7200 - lr: 7.8125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4148 - reconstruction_loss: 2.2686 - kl_loss: 5.7257 - lr: 7.8125e-06\n",
      "Epoch 104/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4221 - reconstruction_loss: 2.2770 - kl_loss: 5.7262\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4220 - reconstruction_loss: 2.2682 - kl_loss: 5.7264 - lr: 7.8125e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4227 - reconstruction_loss: 2.2680 - kl_loss: 5.7197 - lr: 3.9063e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4156 - reconstruction_loss: 2.2686 - kl_loss: 5.7205 - lr: 3.9063e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4165 - reconstruction_loss: 2.2674 - kl_loss: 5.7247 - lr: 3.9063e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4260 - reconstruction_loss: 2.2673 - kl_loss: 5.7217 - lr: 3.9063e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4172 - reconstruction_loss: 2.2676 - kl_loss: 5.7288 - lr: 3.9063e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4203 - reconstruction_loss: 2.2665 - kl_loss: 5.7331 - lr: 3.9063e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4113 - reconstruction_loss: 2.2667 - kl_loss: 5.7326\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4113 - reconstruction_loss: 2.2667 - kl_loss: 5.7326 - lr: 3.9063e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4116 - reconstruction_loss: 2.2669 - kl_loss: 5.7294 - lr: 1.9531e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4191 - reconstruction_loss: 2.2655 - kl_loss: 5.7287 - lr: 1.9531e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4209 - reconstruction_loss: 2.2651 - kl_loss: 5.7296 - lr: 1.9531e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4235 - reconstruction_loss: 2.2649 - kl_loss: 5.7247 - lr: 1.9531e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4234 - reconstruction_loss: 2.2679 - kl_loss: 5.7232 - lr: 1.9531e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4225 - reconstruction_loss: 2.2668 - kl_loss: 5.7176 - lr: 1.9531e-06\n",
      "Epoch 118/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4156 - reconstruction_loss: 2.2736 - kl_loss: 5.7203\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4155 - reconstruction_loss: 2.2657 - kl_loss: 5.7203 - lr: 1.9531e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4233 - reconstruction_loss: 2.2681 - kl_loss: 5.7206 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4215 - reconstruction_loss: 2.2668 - kl_loss: 5.7205 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4233 - reconstruction_loss: 2.2675 - kl_loss: 5.7238 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4221 - reconstruction_loss: 2.2663 - kl_loss: 5.7263 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4164 - reconstruction_loss: 2.2671 - kl_loss: 5.7267 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4204 - reconstruction_loss: 2.2656 - kl_loss: 5.7265 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4225 - reconstruction_loss: 2.2671 - kl_loss: 5.7275 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4283 - reconstruction_loss: 2.2661 - kl_loss: 5.7274 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4253 - reconstruction_loss: 2.2674 - kl_loss: 5.7272 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4306 - reconstruction_loss: 2.2655 - kl_loss: 5.7297 - lr: 1.0000e-06\n",
      "Epoch 128: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 15:33:45,048]\u001b[0m Trial 74 finished with value: 0.004439077536410546 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 34, 'encoder_units_l2': 637, 'encoder_units_l3': 917, 'decoder_layers': 4, 'decoder_units_l1': 137, 'decoder_units_l2': 51, 'decoder_units_l3': 585, 'decoder_units_l4': 57, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 12.7932 - reconstruction_loss: 8.9474 - kl_loss: 3.8769 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 7.9380 - reconstruction_loss: 6.6398 - kl_loss: 4.4323 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 6.1601 - reconstruction_loss: 4.9055 - kl_loss: 4.9490 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 5.4773 - reconstruction_loss: 4.3512 - kl_loss: 5.0961 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 5.2029 - reconstruction_loss: 4.0512 - kl_loss: 5.2529 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.9609 - reconstruction_loss: 3.8587 - kl_loss: 5.3634 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 4.8521 - reconstruction_loss: 3.7410 - kl_loss: 5.4237 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.7879 - reconstruction_loss: 3.6369 - kl_loss: 5.4819 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.6414 - reconstruction_loss: 3.5133 - kl_loss: 5.5112 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.5841 - reconstruction_loss: 3.4347 - kl_loss: 5.5922 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.5091 - reconstruction_loss: 3.3455 - kl_loss: 5.6122 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.4155 - reconstruction_loss: 3.2559 - kl_loss: 5.6579 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.3712 - reconstruction_loss: 3.1914 - kl_loss: 5.6555 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.2655 - reconstruction_loss: 3.0927 - kl_loss: 5.6846 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.1982 - reconstruction_loss: 3.0384 - kl_loss: 5.7059 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.1836 - reconstruction_loss: 3.0122 - kl_loss: 5.7011 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.1245 - reconstruction_loss: 2.9738 - kl_loss: 5.7231 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0774 - reconstruction_loss: 2.9181 - kl_loss: 5.7204 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.0961 - reconstruction_loss: 2.9173 - kl_loss: 5.7168 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9844 - reconstruction_loss: 2.8411 - kl_loss: 5.7228 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.0185 - reconstruction_loss: 2.8597 - kl_loss: 5.7187 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9570 - reconstruction_loss: 2.8138 - kl_loss: 5.7145 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0127 - reconstruction_loss: 2.7977 - kl_loss: 5.7524 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8864 - reconstruction_loss: 2.7421 - kl_loss: 5.7124 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9078 - reconstruction_loss: 2.7507 - kl_loss: 5.7106 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8848 - reconstruction_loss: 2.7206 - kl_loss: 5.7151 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.8279 - reconstruction_loss: 2.6722 - kl_loss: 5.7191 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8334 - reconstruction_loss: 2.6799 - kl_loss: 5.7390 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.8114 - reconstruction_loss: 2.6366 - kl_loss: 5.7495 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8326 - reconstruction_loss: 2.6547 - kl_loss: 5.7420 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8301 - reconstruction_loss: 2.6510 - kl_loss: 5.7598 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7689 - reconstruction_loss: 2.6156 - kl_loss: 5.7333 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.8425 - reconstruction_loss: 2.6572 - kl_loss: 5.7376 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.7882 - reconstruction_loss: 2.6393 - kl_loss: 5.7200 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.7265 - reconstruction_loss: 2.5697 - kl_loss: 5.7126 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.7384 - reconstruction_loss: 2.6033 - kl_loss: 5.7675 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.7920 - reconstruction_loss: 2.6425 - kl_loss: 5.8091 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.8090 - reconstruction_loss: 2.6348 - kl_loss: 5.8289\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.8087 - reconstruction_loss: 2.6256 - kl_loss: 5.8262 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6930 - reconstruction_loss: 2.5275 - kl_loss: 5.7776 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6526 - reconstruction_loss: 2.5016 - kl_loss: 5.7936 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6929 - reconstruction_loss: 2.5103 - kl_loss: 5.7334 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6379 - reconstruction_loss: 2.4932 - kl_loss: 5.7809 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6544 - reconstruction_loss: 2.4753 - kl_loss: 5.7759 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6330 - reconstruction_loss: 2.4720 - kl_loss: 5.7281 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6199 - reconstruction_loss: 2.4836 - kl_loss: 5.7309 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6621 - reconstruction_loss: 2.4963 - kl_loss: 5.7590 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6367 - reconstruction_loss: 2.4623 - kl_loss: 5.7336 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6118 - reconstruction_loss: 2.4668 - kl_loss: 5.7354 - lr: 5.0000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6047 - reconstruction_loss: 2.4422 - kl_loss: 5.7022 - lr: 5.0000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5903 - reconstruction_loss: 2.4596 - kl_loss: 5.7015 - lr: 5.0000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5774 - reconstruction_loss: 2.4418 - kl_loss: 5.7206 - lr: 5.0000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5996 - reconstruction_loss: 2.4509 - kl_loss: 5.6997\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5996 - reconstruction_loss: 2.4509 - kl_loss: 5.6997 - lr: 5.0000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5546 - reconstruction_loss: 2.4070 - kl_loss: 5.6729 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5448 - reconstruction_loss: 2.4016 - kl_loss: 5.6687 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5191 - reconstruction_loss: 2.4023 - kl_loss: 5.6707 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5457 - reconstruction_loss: 2.4116 - kl_loss: 5.6765 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.5371 - reconstruction_loss: 2.4162 - kl_loss: 5.6690\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5372 - reconstruction_loss: 2.4089 - kl_loss: 5.6695 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5446 - reconstruction_loss: 2.4001 - kl_loss: 5.6676 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5312 - reconstruction_loss: 2.3847 - kl_loss: 5.6537 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5200 - reconstruction_loss: 2.3833 - kl_loss: 5.6455 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5219 - reconstruction_loss: 2.3796 - kl_loss: 5.6525 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5250 - reconstruction_loss: 2.3754 - kl_loss: 5.6476 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5115 - reconstruction_loss: 2.3730 - kl_loss: 5.6557 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5176 - reconstruction_loss: 2.3735 - kl_loss: 5.6592 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5145 - reconstruction_loss: 2.3786 - kl_loss: 5.6395 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4931 - reconstruction_loss: 2.3685 - kl_loss: 5.6456 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5024 - reconstruction_loss: 2.3684 - kl_loss: 5.6564 - lr: 1.2500e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5091 - reconstruction_loss: 2.3688 - kl_loss: 5.6517 - lr: 1.2500e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4834 - reconstruction_loss: 2.3611 - kl_loss: 5.6448 - lr: 1.2500e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5112 - reconstruction_loss: 2.3623 - kl_loss: 5.6636 - lr: 1.2500e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5129 - reconstruction_loss: 2.3669 - kl_loss: 5.6533 - lr: 1.2500e-04\n",
      "Epoch 72/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5067 - reconstruction_loss: 2.3702 - kl_loss: 5.6600\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5065 - reconstruction_loss: 2.3613 - kl_loss: 5.6595 - lr: 1.2500e-04\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4770 - reconstruction_loss: 2.3533 - kl_loss: 5.6460 - lr: 6.2500e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4824 - reconstruction_loss: 2.3476 - kl_loss: 5.6448 - lr: 6.2500e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4860 - reconstruction_loss: 2.3564 - kl_loss: 5.6494 - lr: 6.2500e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4833 - reconstruction_loss: 2.3502 - kl_loss: 5.6407 - lr: 6.2500e-05\n",
      "Epoch 77/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4850 - reconstruction_loss: 2.3583 - kl_loss: 5.6390\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4849 - reconstruction_loss: 2.3500 - kl_loss: 5.6391 - lr: 6.2500e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4895 - reconstruction_loss: 2.3466 - kl_loss: 5.6471 - lr: 3.1250e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4773 - reconstruction_loss: 2.3415 - kl_loss: 5.6526 - lr: 3.1250e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4752 - reconstruction_loss: 2.3443 - kl_loss: 5.6355 - lr: 3.1250e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4791 - reconstruction_loss: 2.3482 - kl_loss: 5.6385 - lr: 3.1250e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4651 - reconstruction_loss: 2.3376 - kl_loss: 5.6473 - lr: 3.1250e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4843 - reconstruction_loss: 2.3390 - kl_loss: 5.6304 - lr: 3.1250e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4785 - reconstruction_loss: 2.3420 - kl_loss: 5.6413 - lr: 3.1250e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4688 - reconstruction_loss: 2.3466 - kl_loss: 5.6352 - lr: 3.1250e-05\n",
      "Epoch 86/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.4857 - reconstruction_loss: 2.3514 - kl_loss: 5.6371\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4854 - reconstruction_loss: 2.3410 - kl_loss: 5.6374 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4820 - reconstruction_loss: 2.3372 - kl_loss: 5.6413 - lr: 1.5625e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4778 - reconstruction_loss: 2.3361 - kl_loss: 5.6397 - lr: 1.5625e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4718 - reconstruction_loss: 2.3386 - kl_loss: 5.6278 - lr: 1.5625e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4762 - reconstruction_loss: 2.3362 - kl_loss: 5.6434 - lr: 1.5625e-05\n",
      "Epoch 91/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4701 - reconstruction_loss: 2.3428 - kl_loss: 5.6425\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4701 - reconstruction_loss: 2.3368 - kl_loss: 5.6432 - lr: 1.5625e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4889 - reconstruction_loss: 2.3341 - kl_loss: 5.6462 - lr: 7.8125e-06\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4734 - reconstruction_loss: 2.3331 - kl_loss: 5.6374 - lr: 7.8125e-06\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4736 - reconstruction_loss: 2.3351 - kl_loss: 5.6318 - lr: 7.8125e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4805 - reconstruction_loss: 2.3336 - kl_loss: 5.6353 - lr: 7.8125e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4690 - reconstruction_loss: 2.3330 - kl_loss: 5.6268 - lr: 7.8125e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4655 - reconstruction_loss: 2.3333 - kl_loss: 5.6282 - lr: 7.8125e-06\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4699 - reconstruction_loss: 2.3312 - kl_loss: 5.6310 - lr: 7.8125e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4763 - reconstruction_loss: 2.3321 - kl_loss: 5.6340 - lr: 7.8125e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4771 - reconstruction_loss: 2.3339 - kl_loss: 5.6344 - lr: 7.8125e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4741 - reconstruction_loss: 2.3323 - kl_loss: 5.6388\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4740 - reconstruction_loss: 2.3323 - kl_loss: 5.6388 - lr: 7.8125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4824 - reconstruction_loss: 2.3282 - kl_loss: 5.6402 - lr: 3.9063e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4620 - reconstruction_loss: 2.3301 - kl_loss: 5.6390 - lr: 3.9063e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4720 - reconstruction_loss: 2.3283 - kl_loss: 5.6380 - lr: 3.9063e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4662 - reconstruction_loss: 2.3291 - kl_loss: 5.6352 - lr: 3.9063e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4728 - reconstruction_loss: 2.3301 - kl_loss: 5.6400 - lr: 3.9063e-06\n",
      "Epoch 107/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4582 - reconstruction_loss: 2.3362 - kl_loss: 5.6458\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4583 - reconstruction_loss: 2.3278 - kl_loss: 5.6459 - lr: 3.9063e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4750 - reconstruction_loss: 2.3292 - kl_loss: 5.6418 - lr: 1.9531e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4568 - reconstruction_loss: 2.3246 - kl_loss: 5.6363 - lr: 1.9531e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4676 - reconstruction_loss: 2.3308 - kl_loss: 5.6357 - lr: 1.9531e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4718 - reconstruction_loss: 2.3268 - kl_loss: 5.6388 - lr: 1.9531e-06\n",
      "Epoch 112/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.4634 - reconstruction_loss: 2.3339 - kl_loss: 5.6392\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4633 - reconstruction_loss: 2.3267 - kl_loss: 5.6396 - lr: 1.9531e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4716 - reconstruction_loss: 2.3278 - kl_loss: 5.6385 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4692 - reconstruction_loss: 2.3268 - kl_loss: 5.6394 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4668 - reconstruction_loss: 2.3274 - kl_loss: 5.6395 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4743 - reconstruction_loss: 2.3266 - kl_loss: 5.6379 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4575 - reconstruction_loss: 2.3255 - kl_loss: 5.6376 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4648 - reconstruction_loss: 2.3276 - kl_loss: 5.6375 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4745 - reconstruction_loss: 2.3282 - kl_loss: 5.6394 - lr: 1.0000e-06\n",
      "Epoch 119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 15:39:09,716]\u001b[0m Trial 75 finished with value: 0.004558908581925992 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 33, 'encoder_units_l2': 930, 'encoder_units_l3': 988, 'decoder_layers': 3, 'decoder_units_l1': 194, 'decoder_units_l2': 68, 'decoder_units_l3': 552, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 4s 15ms/step - loss: 12.3071 - reconstruction_loss: 8.5831 - kl_loss: 4.0533 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 6.4405 - reconstruction_loss: 4.8276 - kl_loss: 4.9787 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 5.2222 - reconstruction_loss: 4.0568 - kl_loss: 5.2302 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 5.0634 - reconstruction_loss: 3.9082 - kl_loss: 5.4206 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.6898 - reconstruction_loss: 3.5292 - kl_loss: 5.5759 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 4.5138 - reconstruction_loss: 3.4649 - kl_loss: 5.6380 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.6485 - reconstruction_loss: 3.3340 - kl_loss: 5.6859 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.3460 - reconstruction_loss: 3.1775 - kl_loss: 5.7626 - lr: 0.0100\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.2465 - reconstruction_loss: 3.2960 - kl_loss: 5.7836 - lr: 0.0100\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.3701 - reconstruction_loss: 3.2462 - kl_loss: 5.8397 - lr: 0.0100\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.5064 - reconstruction_loss: 3.0447 - kl_loss: 5.8167 - lr: 0.0100\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.1319 - reconstruction_loss: 2.9438 - kl_loss: 5.8090 - lr: 0.0100\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 4.0900 - reconstruction_loss: 2.8973 - kl_loss: 5.7942 - lr: 0.0100\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.8919 - reconstruction_loss: 2.7250 - kl_loss: 5.7524 - lr: 0.0100\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.2523 - reconstruction_loss: 3.3172 - kl_loss: 5.8199 - lr: 0.0100\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 4.3439 - reconstruction_loss: 3.0471 - kl_loss: 5.7954 - lr: 0.0100\n",
      "Epoch 17/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 4.3701 - reconstruction_loss: 3.1835 - kl_loss: 5.8924\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.3698 - reconstruction_loss: 3.1678 - kl_loss: 5.8926 - lr: 0.0100\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.9039 - reconstruction_loss: 2.7608 - kl_loss: 5.8749 - lr: 0.0050\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.1442 - reconstruction_loss: 2.9328 - kl_loss: 5.9377 - lr: 0.0050\n",
      "Epoch 20/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 4.3443 - reconstruction_loss: 3.0191 - kl_loss: 6.0602\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.3425 - reconstruction_loss: 3.0082 - kl_loss: 6.0624 - lr: 0.0050\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.9128 - reconstruction_loss: 2.6817 - kl_loss: 5.9338 - lr: 0.0025\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.7162 - reconstruction_loss: 2.5352 - kl_loss: 5.7903 - lr: 0.0025\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6774 - reconstruction_loss: 2.5122 - kl_loss: 5.7361 - lr: 0.0025\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6365 - reconstruction_loss: 2.4763 - kl_loss: 5.7247 - lr: 0.0025\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6572 - reconstruction_loss: 2.4993 - kl_loss: 5.7304 - lr: 0.0025\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6308 - reconstruction_loss: 2.5033 - kl_loss: 5.7097 - lr: 0.0025\n",
      "Epoch 27/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.6762 - reconstruction_loss: 2.5588 - kl_loss: 5.7766\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6767 - reconstruction_loss: 2.5499 - kl_loss: 5.7756 - lr: 0.0025\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6407 - reconstruction_loss: 2.4821 - kl_loss: 5.6882 - lr: 0.0012\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5879 - reconstruction_loss: 2.4372 - kl_loss: 5.6654 - lr: 0.0012\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5658 - reconstruction_loss: 2.4347 - kl_loss: 5.6814 - lr: 0.0012\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6339 - reconstruction_loss: 2.4828 - kl_loss: 5.6721 - lr: 0.0012\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5802 - reconstruction_loss: 2.4332 - kl_loss: 5.6621 - lr: 0.0012\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5597 - reconstruction_loss: 2.4273 - kl_loss: 5.6514 - lr: 0.0012\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5448 - reconstruction_loss: 2.4234 - kl_loss: 5.6116 - lr: 0.0012\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5352 - reconstruction_loss: 2.4039 - kl_loss: 5.6079 - lr: 0.0012\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5444 - reconstruction_loss: 2.4242 - kl_loss: 5.6303 - lr: 0.0012\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5743 - reconstruction_loss: 2.4330 - kl_loss: 5.6096 - lr: 0.0012\n",
      "Epoch 38/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.5407 - reconstruction_loss: 2.4206 - kl_loss: 5.6162\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5407 - reconstruction_loss: 2.4121 - kl_loss: 5.6161 - lr: 0.0012\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5563 - reconstruction_loss: 2.4057 - kl_loss: 5.5939 - lr: 6.2500e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5305 - reconstruction_loss: 2.3984 - kl_loss: 5.6054 - lr: 6.2500e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5131 - reconstruction_loss: 2.3882 - kl_loss: 5.5760 - lr: 6.2500e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5184 - reconstruction_loss: 2.3999 - kl_loss: 5.5821 - lr: 6.2500e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5130 - reconstruction_loss: 2.3887 - kl_loss: 5.5937 - lr: 6.2500e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5245 - reconstruction_loss: 2.3857 - kl_loss: 5.5899\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5244 - reconstruction_loss: 2.3857 - kl_loss: 5.5899 - lr: 6.2500e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5229 - reconstruction_loss: 2.3742 - kl_loss: 5.5963 - lr: 3.1250e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4951 - reconstruction_loss: 2.3774 - kl_loss: 5.6070 - lr: 3.1250e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5105 - reconstruction_loss: 2.3789 - kl_loss: 5.5906 - lr: 3.1250e-04\n",
      "Epoch 48/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4969 - reconstruction_loss: 2.3840 - kl_loss: 5.5918\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4968 - reconstruction_loss: 2.3754 - kl_loss: 5.5915 - lr: 3.1250e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4939 - reconstruction_loss: 2.3684 - kl_loss: 5.5944 - lr: 1.5625e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4907 - reconstruction_loss: 2.3645 - kl_loss: 5.5861 - lr: 1.5625e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4891 - reconstruction_loss: 2.3636 - kl_loss: 5.5922 - lr: 1.5625e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4734 - reconstruction_loss: 2.3564 - kl_loss: 5.5828 - lr: 1.5625e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5029 - reconstruction_loss: 2.3616 - kl_loss: 5.5784 - lr: 1.5625e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4788 - reconstruction_loss: 2.3598 - kl_loss: 5.5772 - lr: 1.5625e-04\n",
      "Epoch 55/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4847 - reconstruction_loss: 2.3685 - kl_loss: 5.5746\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4846 - reconstruction_loss: 2.3600 - kl_loss: 5.5747 - lr: 1.5625e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4794 - reconstruction_loss: 2.3558 - kl_loss: 5.5906 - lr: 7.8125e-05\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4780 - reconstruction_loss: 2.3541 - kl_loss: 5.5774 - lr: 7.8125e-05\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4674 - reconstruction_loss: 2.3578 - kl_loss: 5.5673 - lr: 7.8125e-05\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4837 - reconstruction_loss: 2.3538 - kl_loss: 5.5696 - lr: 7.8125e-05\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4677 - reconstruction_loss: 2.3519 - kl_loss: 5.5805 - lr: 7.8125e-05\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4723 - reconstruction_loss: 2.3515 - kl_loss: 5.5725 - lr: 7.8125e-05\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4840 - reconstruction_loss: 2.3524 - kl_loss: 5.5723 - lr: 7.8125e-05\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4765 - reconstruction_loss: 2.3509 - kl_loss: 5.5686 - lr: 7.8125e-05\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4752 - reconstruction_loss: 2.3512 - kl_loss: 5.5657 - lr: 7.8125e-05\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4769 - reconstruction_loss: 2.3518 - kl_loss: 5.5613 - lr: 7.8125e-05\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4670 - reconstruction_loss: 2.3506 - kl_loss: 5.5700 - lr: 7.8125e-05\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4567 - reconstruction_loss: 2.3510 - kl_loss: 5.5689 - lr: 7.8125e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4628 - reconstruction_loss: 2.3467 - kl_loss: 5.5738 - lr: 7.8125e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4694 - reconstruction_loss: 2.3497 - kl_loss: 5.5710 - lr: 7.8125e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4702 - reconstruction_loss: 2.3477 - kl_loss: 5.5651 - lr: 7.8125e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4801 - reconstruction_loss: 2.3511 - kl_loss: 5.5644 - lr: 7.8125e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4714 - reconstruction_loss: 2.3518 - kl_loss: 5.5640 - lr: 7.8125e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4771 - reconstruction_loss: 2.3460 - kl_loss: 5.5718 - lr: 7.8125e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4613 - reconstruction_loss: 2.3456 - kl_loss: 5.5738 - lr: 7.8125e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4621 - reconstruction_loss: 2.3471 - kl_loss: 5.5733 - lr: 7.8125e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4660 - reconstruction_loss: 2.3463 - kl_loss: 5.5672 - lr: 7.8125e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4757 - reconstruction_loss: 2.3476 - kl_loss: 5.5659 - lr: 7.8125e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4595 - reconstruction_loss: 2.3453 - kl_loss: 5.5697 - lr: 7.8125e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4640 - reconstruction_loss: 2.3431 - kl_loss: 5.5764 - lr: 7.8125e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4638 - reconstruction_loss: 2.3489 - kl_loss: 5.5668 - lr: 7.8125e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4750 - reconstruction_loss: 2.3495 - kl_loss: 5.5721 - lr: 7.8125e-05\n",
      "Epoch 82/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4632 - reconstruction_loss: 2.3563 - kl_loss: 5.5669\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4633 - reconstruction_loss: 2.3484 - kl_loss: 5.5676 - lr: 7.8125e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4467 - reconstruction_loss: 2.3498 - kl_loss: 5.5567 - lr: 3.9062e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4613 - reconstruction_loss: 2.3461 - kl_loss: 5.5587 - lr: 3.9062e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4681 - reconstruction_loss: 2.3445 - kl_loss: 5.5619 - lr: 3.9062e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4665 - reconstruction_loss: 2.3418 - kl_loss: 5.5703 - lr: 3.9062e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4745 - reconstruction_loss: 2.3410 - kl_loss: 5.5682 - lr: 3.9062e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4652 - reconstruction_loss: 2.3451 - kl_loss: 5.5655 - lr: 3.9062e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4611 - reconstruction_loss: 2.3444 - kl_loss: 5.5671 - lr: 3.9062e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4706 - reconstruction_loss: 2.3457 - kl_loss: 5.5687\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4705 - reconstruction_loss: 2.3457 - kl_loss: 5.5687 - lr: 3.9062e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4672 - reconstruction_loss: 2.3428 - kl_loss: 5.5743 - lr: 1.9531e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4552 - reconstruction_loss: 2.3426 - kl_loss: 5.5683 - lr: 1.9531e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4663 - reconstruction_loss: 2.3412 - kl_loss: 5.5701\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4663 - reconstruction_loss: 2.3412 - kl_loss: 5.5701 - lr: 1.9531e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4669 - reconstruction_loss: 2.3396 - kl_loss: 5.5693 - lr: 9.7656e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4585 - reconstruction_loss: 2.3390 - kl_loss: 5.5684 - lr: 9.7656e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4577 - reconstruction_loss: 2.3407 - kl_loss: 5.5674 - lr: 9.7656e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4552 - reconstruction_loss: 2.3419 - kl_loss: 5.5681 - lr: 9.7656e-06\n",
      "Epoch 98/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4658 - reconstruction_loss: 2.3498 - kl_loss: 5.5665\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4656 - reconstruction_loss: 2.3414 - kl_loss: 5.5662 - lr: 9.7656e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4591 - reconstruction_loss: 2.3381 - kl_loss: 5.5675 - lr: 4.8828e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4585 - reconstruction_loss: 2.3395 - kl_loss: 5.5663 - lr: 4.8828e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4706 - reconstruction_loss: 2.3411 - kl_loss: 5.5647 - lr: 4.8828e-06\n",
      "Epoch 102/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4668 - reconstruction_loss: 2.3509 - kl_loss: 5.5658\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4667 - reconstruction_loss: 2.3428 - kl_loss: 5.5662 - lr: 4.8828e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4740 - reconstruction_loss: 2.3387 - kl_loss: 5.5656 - lr: 2.4414e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4696 - reconstruction_loss: 2.3405 - kl_loss: 5.5655 - lr: 2.4414e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4513 - reconstruction_loss: 2.3397 - kl_loss: 5.5651\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4514 - reconstruction_loss: 2.3397 - kl_loss: 5.5651 - lr: 2.4414e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4572 - reconstruction_loss: 2.3381 - kl_loss: 5.5651 - lr: 1.2207e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4717 - reconstruction_loss: 2.3379 - kl_loss: 5.5648 - lr: 1.2207e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4672 - reconstruction_loss: 2.3380 - kl_loss: 5.5640 - lr: 1.2207e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4750 - reconstruction_loss: 2.3389 - kl_loss: 5.5645 - lr: 1.2207e-06\n",
      "Epoch 110/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4595 - reconstruction_loss: 2.3479 - kl_loss: 5.5643\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4595 - reconstruction_loss: 2.3392 - kl_loss: 5.5643 - lr: 1.2207e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4603 - reconstruction_loss: 2.3368 - kl_loss: 5.5639 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4567 - reconstruction_loss: 2.3362 - kl_loss: 5.5639 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4675 - reconstruction_loss: 2.3379 - kl_loss: 5.5638 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4560 - reconstruction_loss: 2.3378 - kl_loss: 5.5635 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4588 - reconstruction_loss: 2.3382 - kl_loss: 5.5626 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4493 - reconstruction_loss: 2.3365 - kl_loss: 5.5632 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4561 - reconstruction_loss: 2.3395 - kl_loss: 5.5626 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4507 - reconstruction_loss: 2.3367 - kl_loss: 5.5629 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4516 - reconstruction_loss: 2.3373 - kl_loss: 5.5627 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4548 - reconstruction_loss: 2.3398 - kl_loss: 5.5630 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4600 - reconstruction_loss: 2.3397 - kl_loss: 5.5626 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4667 - reconstruction_loss: 2.3387 - kl_loss: 5.5625 - lr: 1.0000e-06\n",
      "Epoch 122: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 15:45:09,046]\u001b[0m Trial 76 finished with value: 0.004581982511885879 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 18, 'encoder_units_l2': 537, 'encoder_units_l3': 456, 'decoder_layers': 4, 'decoder_units_l1': 134, 'decoder_units_l2': 56, 'decoder_units_l3': 755, 'decoder_units_l4': 24, 'beta': 0.2, 'lr': 0.01, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 4s 17ms/step - loss: 14.6816 - reconstruction_loss: 10.0752 - kl_loss: 3.4796 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 7.1187 - reconstruction_loss: 5.5386 - kl_loss: 4.8670 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 5.6476 - reconstruction_loss: 4.4509 - kl_loss: 5.3215 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 5.2735 - reconstruction_loss: 4.0849 - kl_loss: 5.5096 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 4.9888 - reconstruction_loss: 3.8279 - kl_loss: 5.5852 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 4.8970 - reconstruction_loss: 3.7154 - kl_loss: 5.6363 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.7910 - reconstruction_loss: 3.5672 - kl_loss: 5.6381 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.6224 - reconstruction_loss: 3.4341 - kl_loss: 5.6742 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.5316 - reconstruction_loss: 3.3484 - kl_loss: 5.6904 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.4481 - reconstruction_loss: 3.2566 - kl_loss: 5.7156 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 4.4565 - reconstruction_loss: 3.2040 - kl_loss: 5.6956 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 4.2826 - reconstruction_loss: 3.1062 - kl_loss: 5.7538 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.2121 - reconstruction_loss: 3.0482 - kl_loss: 5.7321 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 4.1944 - reconstruction_loss: 3.0487 - kl_loss: 5.7434 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 4.1098 - reconstruction_loss: 2.9512 - kl_loss: 5.7842 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.1485 - reconstruction_loss: 2.9470 - kl_loss: 5.7793 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.0319 - reconstruction_loss: 2.9143 - kl_loss: 5.7850 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 4.1432 - reconstruction_loss: 2.8647 - kl_loss: 5.7370 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.9272 - reconstruction_loss: 2.7845 - kl_loss: 5.7936 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.9179 - reconstruction_loss: 2.7547 - kl_loss: 5.8068 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.9469 - reconstruction_loss: 2.7495 - kl_loss: 5.8091 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.8915 - reconstruction_loss: 2.7552 - kl_loss: 5.8108 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.8433 - reconstruction_loss: 2.6986 - kl_loss: 5.7960 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.9171 - reconstruction_loss: 2.7118 - kl_loss: 5.7671 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.8480 - reconstruction_loss: 2.7077 - kl_loss: 5.7727 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.8664 - reconstruction_loss: 2.7040 - kl_loss: 5.7540 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.8593 - reconstruction_loss: 2.7007 - kl_loss: 5.7456 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.8116 - reconstruction_loss: 2.7137 - kl_loss: 5.7819 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.8442 - reconstruction_loss: 2.6424 - kl_loss: 5.7744 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.7545 - reconstruction_loss: 2.5981 - kl_loss: 5.7677 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.8750 - reconstruction_loss: 2.6758 - kl_loss: 5.7672 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.7958 - reconstruction_loss: 2.6710 - kl_loss: 5.7440 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.8149 - reconstruction_loss: 2.6549 - kl_loss: 5.7476\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.8146 - reconstruction_loss: 2.6443 - kl_loss: 5.7474 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.7528 - reconstruction_loss: 2.5816 - kl_loss: 5.7336 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.7252 - reconstruction_loss: 2.5767 - kl_loss: 5.7433 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.7019 - reconstruction_loss: 2.5492 - kl_loss: 5.7670 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.7035 - reconstruction_loss: 2.5566 - kl_loss: 5.7904 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6658 - reconstruction_loss: 2.5053 - kl_loss: 5.7623 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6594 - reconstruction_loss: 2.4971 - kl_loss: 5.7394 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.6305 - reconstruction_loss: 2.4793 - kl_loss: 5.7288 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 3.6289 - reconstruction_loss: 2.4782 - kl_loss: 5.7523 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 3.6551 - reconstruction_loss: 2.5043 - kl_loss: 5.7450 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.6671 - reconstruction_loss: 2.4987 - kl_loss: 5.7637\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.6668 - reconstruction_loss: 2.4907 - kl_loss: 5.7642 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.6133 - reconstruction_loss: 2.4637 - kl_loss: 5.7439 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.6132 - reconstruction_loss: 2.4615 - kl_loss: 5.7340 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6037 - reconstruction_loss: 2.4529 - kl_loss: 5.7459 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.5898 - reconstruction_loss: 2.4394 - kl_loss: 5.7280 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.6195 - reconstruction_loss: 2.4466 - kl_loss: 5.7394 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5964 - reconstruction_loss: 2.4434 - kl_loss: 5.7303 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.6210 - reconstruction_loss: 2.4601 - kl_loss: 5.7230\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.6206 - reconstruction_loss: 2.4513 - kl_loss: 5.7229 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.5853 - reconstruction_loss: 2.4150 - kl_loss: 5.7417 - lr: 1.2500e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5841 - reconstruction_loss: 2.4168 - kl_loss: 5.7491 - lr: 1.2500e-04\n",
      "Epoch 53/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5644 - reconstruction_loss: 2.4168 - kl_loss: 5.7321 - lr: 1.2500e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5639 - reconstruction_loss: 2.4179 - kl_loss: 5.7260 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5571 - reconstruction_loss: 2.4129 - kl_loss: 5.7297 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5535 - reconstruction_loss: 2.4186 - kl_loss: 5.7196 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5754 - reconstruction_loss: 2.4356 - kl_loss: 5.6917 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5810 - reconstruction_loss: 2.4268 - kl_loss: 5.7275\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5808 - reconstruction_loss: 2.4178 - kl_loss: 5.7270 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5638 - reconstruction_loss: 2.4074 - kl_loss: 5.7328 - lr: 6.2500e-05\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5546 - reconstruction_loss: 2.4043 - kl_loss: 5.7282 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5496 - reconstruction_loss: 2.3990 - kl_loss: 5.7463 - lr: 6.2500e-05\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5519 - reconstruction_loss: 2.3951 - kl_loss: 5.7380 - lr: 6.2500e-05\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5564 - reconstruction_loss: 2.3921 - kl_loss: 5.7431 - lr: 6.2500e-05\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5642 - reconstruction_loss: 2.3877 - kl_loss: 5.7523 - lr: 6.2500e-05\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5507 - reconstruction_loss: 2.3898 - kl_loss: 5.7590 - lr: 6.2500e-05\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5359 - reconstruction_loss: 2.3925 - kl_loss: 5.7429 - lr: 6.2500e-05\n",
      "Epoch 67/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5511 - reconstruction_loss: 2.4033 - kl_loss: 5.7614\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5511 - reconstruction_loss: 2.3951 - kl_loss: 5.7618 - lr: 6.2500e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5248 - reconstruction_loss: 2.3833 - kl_loss: 5.7405 - lr: 3.1250e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5408 - reconstruction_loss: 2.3793 - kl_loss: 5.7519 - lr: 3.1250e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5418 - reconstruction_loss: 2.3791 - kl_loss: 5.7437 - lr: 3.1250e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5326 - reconstruction_loss: 2.3778 - kl_loss: 5.7375 - lr: 3.1250e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5325 - reconstruction_loss: 2.3751 - kl_loss: 5.7431 - lr: 3.1250e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5203 - reconstruction_loss: 2.3736 - kl_loss: 5.7535 - lr: 3.1250e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5506 - reconstruction_loss: 2.3797 - kl_loss: 5.7349 - lr: 3.1250e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5081 - reconstruction_loss: 2.3718 - kl_loss: 5.7296 - lr: 3.1250e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5407 - reconstruction_loss: 2.3730 - kl_loss: 5.7500 - lr: 3.1250e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5228 - reconstruction_loss: 2.3739 - kl_loss: 5.7408 - lr: 3.1250e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5252 - reconstruction_loss: 2.3705 - kl_loss: 5.7581\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 4s 20ms/step - loss: 3.5252 - reconstruction_loss: 2.3705 - kl_loss: 5.7581 - lr: 3.1250e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 4s 20ms/step - loss: 3.5238 - reconstruction_loss: 2.3632 - kl_loss: 5.7496 - lr: 1.5625e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5242 - reconstruction_loss: 2.3667 - kl_loss: 5.7481 - lr: 1.5625e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.5233 - reconstruction_loss: 2.3626 - kl_loss: 5.7481 - lr: 1.5625e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.5197 - reconstruction_loss: 2.3683 - kl_loss: 5.7296 - lr: 1.5625e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5119 - reconstruction_loss: 2.3641 - kl_loss: 5.7445 - lr: 1.5625e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5310 - reconstruction_loss: 2.3658 - kl_loss: 5.7588\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5309 - reconstruction_loss: 2.3658 - kl_loss: 5.7588 - lr: 1.5625e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5342 - reconstruction_loss: 2.3637 - kl_loss: 5.7522 - lr: 7.8125e-06\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5193 - reconstruction_loss: 2.3617 - kl_loss: 5.7533 - lr: 7.8125e-06\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5179 - reconstruction_loss: 2.3601 - kl_loss: 5.7555 - lr: 7.8125e-06\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5142 - reconstruction_loss: 2.3594 - kl_loss: 5.7496 - lr: 7.8125e-06\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5161 - reconstruction_loss: 2.3596 - kl_loss: 5.7530 - lr: 7.8125e-06\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5162 - reconstruction_loss: 2.3580 - kl_loss: 5.7520 - lr: 7.8125e-06\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5143 - reconstruction_loss: 2.3572 - kl_loss: 5.7511 - lr: 7.8125e-06\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5058 - reconstruction_loss: 2.3558 - kl_loss: 5.7570 - lr: 7.8125e-06\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5259 - reconstruction_loss: 2.3557 - kl_loss: 5.7550 - lr: 7.8125e-06\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.5179 - reconstruction_loss: 2.3549 - kl_loss: 5.7509 - lr: 7.8125e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5216 - reconstruction_loss: 2.3584 - kl_loss: 5.7554 - lr: 7.8125e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5189 - reconstruction_loss: 2.3578 - kl_loss: 5.7522 - lr: 7.8125e-06\n",
      "Epoch 97/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5208 - reconstruction_loss: 2.3633 - kl_loss: 5.7528\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5206 - reconstruction_loss: 2.3552 - kl_loss: 5.7525 - lr: 7.8125e-06\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5245 - reconstruction_loss: 2.3530 - kl_loss: 5.7471 - lr: 3.9063e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5144 - reconstruction_loss: 2.3558 - kl_loss: 5.7484 - lr: 3.9063e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5187 - reconstruction_loss: 2.3566 - kl_loss: 5.7524 - lr: 3.9063e-06\n",
      "Epoch 101/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5065 - reconstruction_loss: 2.3638 - kl_loss: 5.7624\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5066 - reconstruction_loss: 2.3553 - kl_loss: 5.7620 - lr: 3.9063e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.5215 - reconstruction_loss: 2.3508 - kl_loss: 5.7601 - lr: 1.9531e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5119 - reconstruction_loss: 2.3476 - kl_loss: 5.7624 - lr: 1.9531e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5029 - reconstruction_loss: 2.3534 - kl_loss: 5.7634 - lr: 1.9531e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5065 - reconstruction_loss: 2.3522 - kl_loss: 5.7616 - lr: 1.9531e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5039 - reconstruction_loss: 2.3511 - kl_loss: 5.7611\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5039 - reconstruction_loss: 2.3511 - kl_loss: 5.7611 - lr: 1.9531e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5021 - reconstruction_loss: 2.3503 - kl_loss: 5.7610 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5102 - reconstruction_loss: 2.3510 - kl_loss: 5.7599 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4981 - reconstruction_loss: 2.3502 - kl_loss: 5.7573 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5128 - reconstruction_loss: 2.3554 - kl_loss: 5.7572 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5118 - reconstruction_loss: 2.3548 - kl_loss: 5.7596 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4969 - reconstruction_loss: 2.3552 - kl_loss: 5.7607 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5183 - reconstruction_loss: 2.3541 - kl_loss: 5.7575 - lr: 1.0000e-06\n",
      "Epoch 113: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 15:51:34,437]\u001b[0m Trial 77 finished with value: 0.004600895794950409 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 21, 'encoder_units_l2': 795, 'encoder_units_l3': 707, 'decoder_layers': 5, 'decoder_units_l1': 155, 'decoder_units_l2': 78, 'decoder_units_l3': 871, 'decoder_units_l4': 37, 'decoder_units_l5': 942, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 12ms/step - loss: 13.6088 - reconstruction_loss: 9.1741 - kl_loss: 3.7909 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6683 - reconstruction_loss: 6.4189 - kl_loss: 4.5001 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 6.3358 - reconstruction_loss: 5.0918 - kl_loss: 4.9667 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 5.6386 - reconstruction_loss: 4.5281 - kl_loss: 5.1352 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 5.3383 - reconstruction_loss: 4.1893 - kl_loss: 5.2906 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 5.1125 - reconstruction_loss: 3.9556 - kl_loss: 5.4173 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.9529 - reconstruction_loss: 3.7868 - kl_loss: 5.5250 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.8071 - reconstruction_loss: 3.6556 - kl_loss: 5.5582 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.6742 - reconstruction_loss: 3.5342 - kl_loss: 5.5990 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.6146 - reconstruction_loss: 3.4483 - kl_loss: 5.6480 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.5551 - reconstruction_loss: 3.3962 - kl_loss: 5.6532 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.4964 - reconstruction_loss: 3.3109 - kl_loss: 5.6799 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.4195 - reconstruction_loss: 3.2570 - kl_loss: 5.6800 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.3554 - reconstruction_loss: 3.1618 - kl_loss: 5.6945 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.2970 - reconstruction_loss: 3.1066 - kl_loss: 5.7149 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.2034 - reconstruction_loss: 3.0173 - kl_loss: 5.7368 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.1265 - reconstruction_loss: 2.9804 - kl_loss: 5.7386 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.1470 - reconstruction_loss: 2.9647 - kl_loss: 5.7242 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0398 - reconstruction_loss: 2.8716 - kl_loss: 5.7374 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0160 - reconstruction_loss: 2.8327 - kl_loss: 5.7321 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0309 - reconstruction_loss: 2.8401 - kl_loss: 5.7305 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.9223 - reconstruction_loss: 2.7609 - kl_loss: 5.7282 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.9060 - reconstruction_loss: 2.7475 - kl_loss: 5.7231 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.9089 - reconstruction_loss: 2.7439 - kl_loss: 5.7219 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8570 - reconstruction_loss: 2.6892 - kl_loss: 5.7249 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8347 - reconstruction_loss: 2.6778 - kl_loss: 5.7224 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8186 - reconstruction_loss: 2.6730 - kl_loss: 5.7170 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8076 - reconstruction_loss: 2.6402 - kl_loss: 5.7081 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7868 - reconstruction_loss: 2.6335 - kl_loss: 5.6990 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7682 - reconstruction_loss: 2.5968 - kl_loss: 5.7179 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7435 - reconstruction_loss: 2.5827 - kl_loss: 5.7160 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7345 - reconstruction_loss: 2.6033 - kl_loss: 5.7233 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7330 - reconstruction_loss: 2.5790 - kl_loss: 5.7188 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7346 - reconstruction_loss: 2.5833 - kl_loss: 5.7085 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7142 - reconstruction_loss: 2.5584 - kl_loss: 5.6723 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6793 - reconstruction_loss: 2.5399 - kl_loss: 5.6846 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6873 - reconstruction_loss: 2.5648 - kl_loss: 5.6919 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7266 - reconstruction_loss: 2.5906 - kl_loss: 5.6936 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.7446 - reconstruction_loss: 2.5863 - kl_loss: 5.7133\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7445 - reconstruction_loss: 2.5863 - kl_loss: 5.7133 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6940 - reconstruction_loss: 2.5268 - kl_loss: 5.7221 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6644 - reconstruction_loss: 2.4854 - kl_loss: 5.7108 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6076 - reconstruction_loss: 2.4679 - kl_loss: 5.6956 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6272 - reconstruction_loss: 2.4744 - kl_loss: 5.6772 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6008 - reconstruction_loss: 2.4641 - kl_loss: 5.6773 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5913 - reconstruction_loss: 2.4527 - kl_loss: 5.6808 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6094 - reconstruction_loss: 2.4640 - kl_loss: 5.6792 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6867 - reconstruction_loss: 2.5525 - kl_loss: 5.7458 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.7114 - reconstruction_loss: 2.5047 - kl_loss: 5.7111\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7106 - reconstruction_loss: 2.4959 - kl_loss: 5.7110 - lr: 5.0000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5932 - reconstruction_loss: 2.4447 - kl_loss: 5.6747 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5655 - reconstruction_loss: 2.4185 - kl_loss: 5.6793 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5607 - reconstruction_loss: 2.4143 - kl_loss: 5.6768 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5815 - reconstruction_loss: 2.4296 - kl_loss: 5.6980 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5563 - reconstruction_loss: 2.4110 - kl_loss: 5.6753 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5484 - reconstruction_loss: 2.4019 - kl_loss: 5.6621 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5661 - reconstruction_loss: 2.4184 - kl_loss: 5.6715 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5558 - reconstruction_loss: 2.4048 - kl_loss: 5.6784 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5425 - reconstruction_loss: 2.3949 - kl_loss: 5.6732 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5221 - reconstruction_loss: 2.4065 - kl_loss: 5.6498 - lr: 2.5000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5424 - reconstruction_loss: 2.4036 - kl_loss: 5.6348 - lr: 2.5000e-04\n",
      "Epoch 60/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.5475 - reconstruction_loss: 2.4164 - kl_loss: 5.6601\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5474 - reconstruction_loss: 2.4082 - kl_loss: 5.6614 - lr: 2.5000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5356 - reconstruction_loss: 2.3800 - kl_loss: 5.6606 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5251 - reconstruction_loss: 2.3719 - kl_loss: 5.6552 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5202 - reconstruction_loss: 2.3775 - kl_loss: 5.6629 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5158 - reconstruction_loss: 2.3691 - kl_loss: 5.6619 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5183 - reconstruction_loss: 2.3623 - kl_loss: 5.6555 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4989 - reconstruction_loss: 2.3553 - kl_loss: 5.6573 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5108 - reconstruction_loss: 2.3650 - kl_loss: 5.6642 - lr: 1.2500e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4949 - reconstruction_loss: 2.3528 - kl_loss: 5.6756 - lr: 1.2500e-04\n",
      "Epoch 69/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.5002 - reconstruction_loss: 2.3667 - kl_loss: 5.6789\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5001 - reconstruction_loss: 2.3583 - kl_loss: 5.6784 - lr: 1.2500e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4757 - reconstruction_loss: 2.3417 - kl_loss: 5.6709 - lr: 6.2500e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4866 - reconstruction_loss: 2.3433 - kl_loss: 5.6646 - lr: 6.2500e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4781 - reconstruction_loss: 2.3386 - kl_loss: 5.6652 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4901 - reconstruction_loss: 2.3403 - kl_loss: 5.6760 - lr: 6.2500e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4917 - reconstruction_loss: 2.3481 - kl_loss: 5.6718 - lr: 6.2500e-05\n",
      "Epoch 75/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4798 - reconstruction_loss: 2.3529 - kl_loss: 5.6799\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4798 - reconstruction_loss: 2.3450 - kl_loss: 5.6798 - lr: 6.2500e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4887 - reconstruction_loss: 2.3387 - kl_loss: 5.6789 - lr: 3.1250e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4769 - reconstruction_loss: 2.3329 - kl_loss: 5.6752 - lr: 3.1250e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4769 - reconstruction_loss: 2.3308 - kl_loss: 5.6799 - lr: 3.1250e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4700 - reconstruction_loss: 2.3300 - kl_loss: 5.6738 - lr: 3.1250e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4709 - reconstruction_loss: 2.3316 - kl_loss: 5.6730 - lr: 3.1250e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4659 - reconstruction_loss: 2.3321 - kl_loss: 5.6732 - lr: 3.1250e-05\n",
      "Epoch 82/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4848 - reconstruction_loss: 2.3406 - kl_loss: 5.6825\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4847 - reconstruction_loss: 2.3327 - kl_loss: 5.6821 - lr: 3.1250e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4761 - reconstruction_loss: 2.3268 - kl_loss: 5.6705 - lr: 1.5625e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4749 - reconstruction_loss: 2.3270 - kl_loss: 5.6726 - lr: 1.5625e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4691 - reconstruction_loss: 2.3265 - kl_loss: 5.6817 - lr: 1.5625e-05\n",
      "Epoch 86/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4793 - reconstruction_loss: 2.3318 - kl_loss: 5.6891\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4790 - reconstruction_loss: 2.3247 - kl_loss: 5.6889 - lr: 1.5625e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4734 - reconstruction_loss: 2.3218 - kl_loss: 5.6820 - lr: 7.8125e-06\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4603 - reconstruction_loss: 2.3214 - kl_loss: 5.6741 - lr: 7.8125e-06\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4680 - reconstruction_loss: 2.3207 - kl_loss: 5.6814 - lr: 7.8125e-06\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4577 - reconstruction_loss: 2.3221 - kl_loss: 5.6738 - lr: 7.8125e-06\n",
      "Epoch 91/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4814 - reconstruction_loss: 2.3308 - kl_loss: 5.6783\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4810 - reconstruction_loss: 2.3222 - kl_loss: 5.6775 - lr: 7.8125e-06\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4674 - reconstruction_loss: 2.3185 - kl_loss: 5.6794 - lr: 3.9063e-06\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4619 - reconstruction_loss: 2.3197 - kl_loss: 5.6775 - lr: 3.9063e-06\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4779 - reconstruction_loss: 2.3188 - kl_loss: 5.6819 - lr: 3.9063e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4531 - reconstruction_loss: 2.3168 - kl_loss: 5.6762 - lr: 3.9063e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4670 - reconstruction_loss: 2.3207 - kl_loss: 5.6750 - lr: 3.9063e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4590 - reconstruction_loss: 2.3188 - kl_loss: 5.6746 - lr: 3.9063e-06\n",
      "Epoch 98/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4682 - reconstruction_loss: 2.3304 - kl_loss: 5.6752\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4680 - reconstruction_loss: 2.3223 - kl_loss: 5.6748 - lr: 3.9063e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4550 - reconstruction_loss: 2.3155 - kl_loss: 5.6788 - lr: 1.9531e-06\n",
      "Epoch 100/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4585 - reconstruction_loss: 2.3185 - kl_loss: 5.6796 - lr: 1.9531e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4547 - reconstruction_loss: 2.3164 - kl_loss: 5.6770 - lr: 1.9531e-06\n",
      "Epoch 102/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4695 - reconstruction_loss: 2.3284 - kl_loss: 5.6771\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4693 - reconstruction_loss: 2.3203 - kl_loss: 5.6774 - lr: 1.9531e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4584 - reconstruction_loss: 2.3181 - kl_loss: 5.6795 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4607 - reconstruction_loss: 2.3167 - kl_loss: 5.6796 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4769 - reconstruction_loss: 2.3178 - kl_loss: 5.6806 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4723 - reconstruction_loss: 2.3171 - kl_loss: 5.6823 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4579 - reconstruction_loss: 2.3164 - kl_loss: 5.6826 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4560 - reconstruction_loss: 2.3151 - kl_loss: 5.6822 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4590 - reconstruction_loss: 2.3173 - kl_loss: 5.6825 - lr: 1.0000e-06\n",
      "Epoch 109: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 15:55:48,528]\u001b[0m Trial 78 finished with value: 0.004539019373942856 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 29, 'encoder_units_l2': 1023, 'encoder_units_l3': 549, 'decoder_layers': 3, 'decoder_units_l1': 227, 'decoder_units_l2': 27, 'decoder_units_l3': 1010, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 12ms/step - loss: 13.2156 - reconstruction_loss: 9.1536 - kl_loss: 3.7520 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7627 - reconstruction_loss: 6.4219 - kl_loss: 4.4779 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 5.9386 - reconstruction_loss: 4.6685 - kl_loss: 5.0594 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 5.2921 - reconstruction_loss: 4.1207 - kl_loss: 5.2925 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 5.0273 - reconstruction_loss: 3.8579 - kl_loss: 5.4491 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.7857 - reconstruction_loss: 3.6340 - kl_loss: 5.5417 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.6293 - reconstruction_loss: 3.5155 - kl_loss: 5.5576 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.5774 - reconstruction_loss: 3.3864 - kl_loss: 5.5758 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.4479 - reconstruction_loss: 3.2772 - kl_loss: 5.5939 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.3545 - reconstruction_loss: 3.2083 - kl_loss: 5.5818 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.2671 - reconstruction_loss: 3.1045 - kl_loss: 5.6071 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.1706 - reconstruction_loss: 2.9972 - kl_loss: 5.6815 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.0515 - reconstruction_loss: 2.9031 - kl_loss: 5.6779 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.0200 - reconstruction_loss: 2.8439 - kl_loss: 5.6893 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9770 - reconstruction_loss: 2.8384 - kl_loss: 5.7085 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9477 - reconstruction_loss: 2.7828 - kl_loss: 5.7199 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9492 - reconstruction_loss: 2.7733 - kl_loss: 5.7009 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9591 - reconstruction_loss: 2.7530 - kl_loss: 5.7019 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8612 - reconstruction_loss: 2.6757 - kl_loss: 5.6991 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8109 - reconstruction_loss: 2.6713 - kl_loss: 5.7024 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7803 - reconstruction_loss: 2.6507 - kl_loss: 5.7082 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7808 - reconstruction_loss: 2.6150 - kl_loss: 5.7243 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7888 - reconstruction_loss: 2.6522 - kl_loss: 5.7242 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8107 - reconstruction_loss: 2.6342 - kl_loss: 5.7087 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7440 - reconstruction_loss: 2.6127 - kl_loss: 5.6658 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7536 - reconstruction_loss: 2.6336 - kl_loss: 5.6840 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7737 - reconstruction_loss: 2.6653 - kl_loss: 5.7356 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.9351 - reconstruction_loss: 2.7068 - kl_loss: 5.7597\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9338 - reconstruction_loss: 2.6971 - kl_loss: 5.7595 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7153 - reconstruction_loss: 2.5328 - kl_loss: 5.7151 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6741 - reconstruction_loss: 2.5062 - kl_loss: 5.7017 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6645 - reconstruction_loss: 2.5144 - kl_loss: 5.6989 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6764 - reconstruction_loss: 2.5039 - kl_loss: 5.7194 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6377 - reconstruction_loss: 2.4860 - kl_loss: 5.6774 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6115 - reconstruction_loss: 2.4753 - kl_loss: 5.6792 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6270 - reconstruction_loss: 2.4723 - kl_loss: 5.6551 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6064 - reconstruction_loss: 2.4617 - kl_loss: 5.6688 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6174 - reconstruction_loss: 2.4955 - kl_loss: 5.6505 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6885 - reconstruction_loss: 2.5153 - kl_loss: 5.7175 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.6794 - reconstruction_loss: 2.4998 - kl_loss: 5.7294\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6792 - reconstruction_loss: 2.4998 - kl_loss: 5.7294 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6059 - reconstruction_loss: 2.4481 - kl_loss: 5.7050 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5737 - reconstruction_loss: 2.4323 - kl_loss: 5.7130 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5880 - reconstruction_loss: 2.4256 - kl_loss: 5.6974 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5796 - reconstruction_loss: 2.4138 - kl_loss: 5.6915 - lr: 2.5000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5379 - reconstruction_loss: 2.4060 - kl_loss: 5.6797 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5540 - reconstruction_loss: 2.4115 - kl_loss: 5.6704 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5448 - reconstruction_loss: 2.4116 - kl_loss: 5.6581 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5300 - reconstruction_loss: 2.3998 - kl_loss: 5.6800 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5632 - reconstruction_loss: 2.4153 - kl_loss: 5.6776 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5415 - reconstruction_loss: 2.4074 - kl_loss: 5.6821 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5440 - reconstruction_loss: 2.4001 - kl_loss: 5.6777 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5497 - reconstruction_loss: 2.4033 - kl_loss: 5.6762 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5491 - reconstruction_loss: 2.3969 - kl_loss: 5.6727 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5122 - reconstruction_loss: 2.3867 - kl_loss: 5.6594 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5388 - reconstruction_loss: 2.3980 - kl_loss: 5.6807 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5467 - reconstruction_loss: 2.4027 - kl_loss: 5.6769 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5587 - reconstruction_loss: 2.4211 - kl_loss: 5.7162\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5587 - reconstruction_loss: 2.4211 - kl_loss: 5.7162 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5686 - reconstruction_loss: 2.4015 - kl_loss: 5.6905 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5413 - reconstruction_loss: 2.3912 - kl_loss: 5.6739 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5243 - reconstruction_loss: 2.3977 - kl_loss: 5.6660\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5243 - reconstruction_loss: 2.3892 - kl_loss: 5.6669 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4995 - reconstruction_loss: 2.3677 - kl_loss: 5.6521 - lr: 6.2500e-05\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5080 - reconstruction_loss: 2.3675 - kl_loss: 5.6476 - lr: 6.2500e-05\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4986 - reconstruction_loss: 2.3565 - kl_loss: 5.6464 - lr: 6.2500e-05\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4876 - reconstruction_loss: 2.3500 - kl_loss: 5.6564 - lr: 6.2500e-05\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4673 - reconstruction_loss: 2.3458 - kl_loss: 5.6503 - lr: 6.2500e-05\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4943 - reconstruction_loss: 2.3557 - kl_loss: 5.6514 - lr: 6.2500e-05\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5025 - reconstruction_loss: 2.3584 - kl_loss: 5.6590 - lr: 6.2500e-05\n",
      "Epoch 67/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4887 - reconstruction_loss: 2.3623 - kl_loss: 5.6564\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4887 - reconstruction_loss: 2.3543 - kl_loss: 5.6566 - lr: 6.2500e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4891 - reconstruction_loss: 2.3442 - kl_loss: 5.6573 - lr: 3.1250e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4885 - reconstruction_loss: 2.3425 - kl_loss: 5.6512 - lr: 3.1250e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4887 - reconstruction_loss: 2.3424 - kl_loss: 5.6494 - lr: 3.1250e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4719 - reconstruction_loss: 2.3388 - kl_loss: 5.6506 - lr: 3.1250e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4740 - reconstruction_loss: 2.3427 - kl_loss: 5.6371 - lr: 3.1250e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4662 - reconstruction_loss: 2.3406 - kl_loss: 5.6458 - lr: 3.1250e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4888 - reconstruction_loss: 2.3386 - kl_loss: 5.6568\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4887 - reconstruction_loss: 2.3386 - kl_loss: 5.6568 - lr: 3.1250e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4884 - reconstruction_loss: 2.3341 - kl_loss: 5.6477 - lr: 1.5625e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4589 - reconstruction_loss: 2.3348 - kl_loss: 5.6308 - lr: 1.5625e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4797 - reconstruction_loss: 2.3345 - kl_loss: 5.6426 - lr: 1.5625e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4723 - reconstruction_loss: 2.3366 - kl_loss: 5.6448 - lr: 1.5625e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4714 - reconstruction_loss: 2.3345 - kl_loss: 5.6523\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4714 - reconstruction_loss: 2.3345 - kl_loss: 5.6523 - lr: 1.5625e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4705 - reconstruction_loss: 2.3362 - kl_loss: 5.6511 - lr: 7.8125e-06\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4810 - reconstruction_loss: 2.3334 - kl_loss: 5.6440 - lr: 7.8125e-06\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4594 - reconstruction_loss: 2.3313 - kl_loss: 5.6470 - lr: 7.8125e-06\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4689 - reconstruction_loss: 2.3299 - kl_loss: 5.6454 - lr: 7.8125e-06\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4595 - reconstruction_loss: 2.3303 - kl_loss: 5.6442 - lr: 7.8125e-06\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4643 - reconstruction_loss: 2.3285 - kl_loss: 5.6452 - lr: 7.8125e-06\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4770 - reconstruction_loss: 2.3303 - kl_loss: 5.6395 - lr: 7.8125e-06\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4691 - reconstruction_loss: 2.3292 - kl_loss: 5.6454 - lr: 7.8125e-06\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4682 - reconstruction_loss: 2.3267 - kl_loss: 5.6484 - lr: 7.8125e-06\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4767 - reconstruction_loss: 2.3261 - kl_loss: 5.6474 - lr: 7.8125e-06\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4612 - reconstruction_loss: 2.3269 - kl_loss: 5.6528 - lr: 7.8125e-06\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4567 - reconstruction_loss: 2.3252 - kl_loss: 5.6477 - lr: 7.8125e-06\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4668 - reconstruction_loss: 2.3253 - kl_loss: 5.6446 - lr: 7.8125e-06\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4561 - reconstruction_loss: 2.3248 - kl_loss: 5.6439 - lr: 7.8125e-06\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4557 - reconstruction_loss: 2.3274 - kl_loss: 5.6388 - lr: 7.8125e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4604 - reconstruction_loss: 2.3283 - kl_loss: 5.6460 - lr: 7.8125e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4711 - reconstruction_loss: 2.3298 - kl_loss: 5.6518\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4711 - reconstruction_loss: 2.3298 - kl_loss: 5.6518 - lr: 7.8125e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4510 - reconstruction_loss: 2.3255 - kl_loss: 5.6504 - lr: 3.9063e-06\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4584 - reconstruction_loss: 2.3240 - kl_loss: 5.6518 - lr: 3.9063e-06\n",
      "Epoch 99/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4692 - reconstruction_loss: 2.3332 - kl_loss: 5.6492\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4690 - reconstruction_loss: 2.3252 - kl_loss: 5.6484 - lr: 3.9063e-06\n",
      "Epoch 100/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4732 - reconstruction_loss: 2.3238 - kl_loss: 5.6502 - lr: 1.9531e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4574 - reconstruction_loss: 2.3235 - kl_loss: 5.6479 - lr: 1.9531e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4679 - reconstruction_loss: 2.3218 - kl_loss: 5.6480 - lr: 1.9531e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4659 - reconstruction_loss: 2.3250 - kl_loss: 5.6487 - lr: 1.9531e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4579 - reconstruction_loss: 2.3217 - kl_loss: 5.6483 - lr: 1.9531e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4593 - reconstruction_loss: 2.3218 - kl_loss: 5.6482\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4593 - reconstruction_loss: 2.3218 - kl_loss: 5.6482 - lr: 1.9531e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4593 - reconstruction_loss: 2.3223 - kl_loss: 5.6477 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4574 - reconstruction_loss: 2.3220 - kl_loss: 5.6465 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4521 - reconstruction_loss: 2.3208 - kl_loss: 5.6468 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4645 - reconstruction_loss: 2.3189 - kl_loss: 5.6461 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4588 - reconstruction_loss: 2.3236 - kl_loss: 5.6467 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4601 - reconstruction_loss: 2.3226 - kl_loss: 5.6467 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4612 - reconstruction_loss: 2.3214 - kl_loss: 5.6465 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4573 - reconstruction_loss: 2.3235 - kl_loss: 5.6452 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4740 - reconstruction_loss: 2.3238 - kl_loss: 5.6460 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4641 - reconstruction_loss: 2.3220 - kl_loss: 5.6473 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4497 - reconstruction_loss: 2.3197 - kl_loss: 5.6445 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4518 - reconstruction_loss: 2.3238 - kl_loss: 5.6450 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4474 - reconstruction_loss: 2.3240 - kl_loss: 5.6457 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4585 - reconstruction_loss: 2.3208 - kl_loss: 5.6455 - lr: 1.0000e-06\n",
      "Epoch 119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 16:00:11,387]\u001b[0m Trial 79 finished with value: 0.004549595797675333 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 41, 'encoder_units_l2': 599, 'encoder_units_l3': 845, 'decoder_layers': 4, 'decoder_units_l1': 90, 'decoder_units_l2': 42, 'decoder_units_l3': 570, 'decoder_units_l4': 18, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 61 with value: 0.004372707378647208.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 11ms/step - loss: 13.9721 - reconstruction_loss: 9.5779 - kl_loss: 3.7551 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6394 - reconstruction_loss: 6.1663 - kl_loss: 4.6415 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 5.9429 - reconstruction_loss: 4.7196 - kl_loss: 5.1625 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 5.4451 - reconstruction_loss: 4.2836 - kl_loss: 5.2878 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 5.1271 - reconstruction_loss: 3.9745 - kl_loss: 5.4417 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.9547 - reconstruction_loss: 3.8031 - kl_loss: 5.5417 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.7870 - reconstruction_loss: 3.6453 - kl_loss: 5.6089 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.6777 - reconstruction_loss: 3.5299 - kl_loss: 5.6679 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.6964 - reconstruction_loss: 3.5045 - kl_loss: 5.6615 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.5999 - reconstruction_loss: 3.4164 - kl_loss: 5.6672 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.5165 - reconstruction_loss: 3.3537 - kl_loss: 5.6992 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.4320 - reconstruction_loss: 3.3006 - kl_loss: 5.7240 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.4843 - reconstruction_loss: 3.2474 - kl_loss: 5.7225 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.3930 - reconstruction_loss: 3.1898 - kl_loss: 5.7281 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.3619 - reconstruction_loss: 3.1766 - kl_loss: 5.7115 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.3099 - reconstruction_loss: 3.1081 - kl_loss: 5.7426 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.2565 - reconstruction_loss: 3.0638 - kl_loss: 5.7562 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.2036 - reconstruction_loss: 3.0397 - kl_loss: 5.7607 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.1136 - reconstruction_loss: 2.9688 - kl_loss: 5.7626 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.1324 - reconstruction_loss: 2.9634 - kl_loss: 5.7730 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.1381 - reconstruction_loss: 2.9839 - kl_loss: 5.8188 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0267 - reconstruction_loss: 2.8616 - kl_loss: 5.8320 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0395 - reconstruction_loss: 2.8446 - kl_loss: 5.8042 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0349 - reconstruction_loss: 2.8341 - kl_loss: 5.8185 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0221 - reconstruction_loss: 2.8167 - kl_loss: 5.8136 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9333 - reconstruction_loss: 2.7780 - kl_loss: 5.8153 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9305 - reconstruction_loss: 2.7634 - kl_loss: 5.8095 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9297 - reconstruction_loss: 2.7634 - kl_loss: 5.8066 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8989 - reconstruction_loss: 2.7233 - kl_loss: 5.8259 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8936 - reconstruction_loss: 2.7220 - kl_loss: 5.8411 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8614 - reconstruction_loss: 2.6900 - kl_loss: 5.8456 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8719 - reconstruction_loss: 2.6888 - kl_loss: 5.8152 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8811 - reconstruction_loss: 2.6762 - kl_loss: 5.8226 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9335 - reconstruction_loss: 2.7046 - kl_loss: 5.8128 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8089 - reconstruction_loss: 2.6205 - kl_loss: 5.8209 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7730 - reconstruction_loss: 2.6099 - kl_loss: 5.8382 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7695 - reconstruction_loss: 2.5920 - kl_loss: 5.8051 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7546 - reconstruction_loss: 2.5823 - kl_loss: 5.8306 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8557 - reconstruction_loss: 2.6317 - kl_loss: 5.8214 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7261 - reconstruction_loss: 2.5815 - kl_loss: 5.8263 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7280 - reconstruction_loss: 2.5673 - kl_loss: 5.8345 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7657 - reconstruction_loss: 2.6002 - kl_loss: 5.8400 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7294 - reconstruction_loss: 2.5585 - kl_loss: 5.8360 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7343 - reconstruction_loss: 2.5815 - kl_loss: 5.8389 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7121 - reconstruction_loss: 2.5203 - kl_loss: 5.8167 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6958 - reconstruction_loss: 2.5019 - kl_loss: 5.8211 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6459 - reconstruction_loss: 2.4753 - kl_loss: 5.8263 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6898 - reconstruction_loss: 2.5041 - kl_loss: 5.8228 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7478 - reconstruction_loss: 2.5021 - kl_loss: 5.8257 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.6600 - reconstruction_loss: 2.4871 - kl_loss: 5.8178\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6599 - reconstruction_loss: 2.4871 - kl_loss: 5.8178 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6532 - reconstruction_loss: 2.4328 - kl_loss: 5.8146 - lr: 5.0000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5681 - reconstruction_loss: 2.3999 - kl_loss: 5.8237 - lr: 5.0000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5733 - reconstruction_loss: 2.3897 - kl_loss: 5.8492 - lr: 5.0000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5675 - reconstruction_loss: 2.3830 - kl_loss: 5.8381 - lr: 5.0000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5602 - reconstruction_loss: 2.3799 - kl_loss: 5.8338 - lr: 5.0000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5485 - reconstruction_loss: 2.3822 - kl_loss: 5.8493 - lr: 5.0000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5816 - reconstruction_loss: 2.3956 - kl_loss: 5.8661 - lr: 5.0000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5402 - reconstruction_loss: 2.3791 - kl_loss: 5.8248 - lr: 5.0000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5374 - reconstruction_loss: 2.3598 - kl_loss: 5.8339 - lr: 5.0000e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5432 - reconstruction_loss: 2.3667 - kl_loss: 5.8280 - lr: 5.0000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5470 - reconstruction_loss: 2.3715 - kl_loss: 5.8251 - lr: 5.0000e-04\n",
      "Epoch 62/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.5595 - reconstruction_loss: 2.3777 - kl_loss: 5.8232\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5590 - reconstruction_loss: 2.3681 - kl_loss: 5.8233 - lr: 5.0000e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4829 - reconstruction_loss: 2.3138 - kl_loss: 5.8093 - lr: 2.5000e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4868 - reconstruction_loss: 2.3092 - kl_loss: 5.8245 - lr: 2.5000e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4995 - reconstruction_loss: 2.3204 - kl_loss: 5.8225 - lr: 2.5000e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4916 - reconstruction_loss: 2.3120 - kl_loss: 5.8308 - lr: 2.5000e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4915 - reconstruction_loss: 2.3090 - kl_loss: 5.8469\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4914 - reconstruction_loss: 2.3090 - kl_loss: 5.8469 - lr: 2.5000e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4617 - reconstruction_loss: 2.2848 - kl_loss: 5.8341 - lr: 1.2500e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4568 - reconstruction_loss: 2.2809 - kl_loss: 5.8535 - lr: 1.2500e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4632 - reconstruction_loss: 2.2814 - kl_loss: 5.8458 - lr: 1.2500e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4422 - reconstruction_loss: 2.2726 - kl_loss: 5.8441 - lr: 1.2500e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4497 - reconstruction_loss: 2.2692 - kl_loss: 5.8636 - lr: 1.2500e-04\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4648 - reconstruction_loss: 2.2761 - kl_loss: 5.8615 - lr: 1.2500e-04\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4439 - reconstruction_loss: 2.2689 - kl_loss: 5.8604 - lr: 1.2500e-04\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4455 - reconstruction_loss: 2.2626 - kl_loss: 5.8612 - lr: 1.2500e-04\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4327 - reconstruction_loss: 2.2577 - kl_loss: 5.8646 - lr: 1.2500e-04\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4388 - reconstruction_loss: 2.2564 - kl_loss: 5.8662 - lr: 1.2500e-04\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4614 - reconstruction_loss: 2.2656 - kl_loss: 5.8535 - lr: 1.2500e-04\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4625 - reconstruction_loss: 2.2699 - kl_loss: 5.8606 - lr: 1.2500e-04\n",
      "Epoch 80/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4435 - reconstruction_loss: 2.2691 - kl_loss: 5.8720\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4434 - reconstruction_loss: 2.2612 - kl_loss: 5.8724 - lr: 1.2500e-04\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4116 - reconstruction_loss: 2.2473 - kl_loss: 5.8647 - lr: 6.2500e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4190 - reconstruction_loss: 2.2455 - kl_loss: 5.8530 - lr: 6.2500e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4209 - reconstruction_loss: 2.2456 - kl_loss: 5.8632 - lr: 6.2500e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4170 - reconstruction_loss: 2.2434 - kl_loss: 5.8610 - lr: 6.2500e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4175 - reconstruction_loss: 2.2405 - kl_loss: 5.8658 - lr: 6.2500e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4238 - reconstruction_loss: 2.2417 - kl_loss: 5.8650 - lr: 6.2500e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4210 - reconstruction_loss: 2.2371 - kl_loss: 5.8550 - lr: 6.2500e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4182 - reconstruction_loss: 2.2373 - kl_loss: 5.8559 - lr: 6.2500e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4254 - reconstruction_loss: 2.2424 - kl_loss: 5.8632 - lr: 6.2500e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4101 - reconstruction_loss: 2.2414 - kl_loss: 5.8473\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4101 - reconstruction_loss: 2.2414 - kl_loss: 5.8473 - lr: 6.2500e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4165 - reconstruction_loss: 2.2305 - kl_loss: 5.8643 - lr: 3.1250e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4058 - reconstruction_loss: 2.2316 - kl_loss: 5.8602 - lr: 3.1250e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4153 - reconstruction_loss: 2.2304 - kl_loss: 5.8698 - lr: 3.1250e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4105 - reconstruction_loss: 2.2305 - kl_loss: 5.8580 - lr: 3.1250e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4177 - reconstruction_loss: 2.2308 - kl_loss: 5.8661 - lr: 3.1250e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4230 - reconstruction_loss: 2.2305 - kl_loss: 5.8711 - lr: 3.1250e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4036 - reconstruction_loss: 2.2249 - kl_loss: 5.8609 - lr: 3.1250e-05\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4066 - reconstruction_loss: 2.2234 - kl_loss: 5.8829 - lr: 3.1250e-05\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4066 - reconstruction_loss: 2.2265 - kl_loss: 5.8645 - lr: 3.1250e-05\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4141 - reconstruction_loss: 2.2237 - kl_loss: 5.8696\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4140 - reconstruction_loss: 2.2237 - kl_loss: 5.8696 - lr: 3.1250e-05\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4119 - reconstruction_loss: 2.2209 - kl_loss: 5.8669 - lr: 1.5625e-05\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4079 - reconstruction_loss: 2.2178 - kl_loss: 5.8640 - lr: 1.5625e-05\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4047 - reconstruction_loss: 2.2206 - kl_loss: 5.8731 - lr: 1.5625e-05\n",
      "Epoch 104/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3996 - reconstruction_loss: 2.2172 - kl_loss: 5.8725 - lr: 1.5625e-05\n",
      "Epoch 105/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.3954 - reconstruction_loss: 2.2269 - kl_loss: 5.8686\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.3954 - reconstruction_loss: 2.2180 - kl_loss: 5.8682 - lr: 1.5625e-05\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4002 - reconstruction_loss: 2.2149 - kl_loss: 5.8728 - lr: 7.8125e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4033 - reconstruction_loss: 2.2173 - kl_loss: 5.8671 - lr: 7.8125e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.3937 - reconstruction_loss: 2.2150 - kl_loss: 5.8710 - lr: 7.8125e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.3933 - reconstruction_loss: 2.2174 - kl_loss: 5.8622 - lr: 7.8125e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4143 - reconstruction_loss: 2.2139 - kl_loss: 5.8728 - lr: 7.8125e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3974 - reconstruction_loss: 2.2135 - kl_loss: 5.8704 - lr: 7.8125e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4094 - reconstruction_loss: 2.2154 - kl_loss: 5.8715 - lr: 7.8125e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3976 - reconstruction_loss: 2.2158 - kl_loss: 5.8727 - lr: 7.8125e-06\n",
      "Epoch 114/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4002 - reconstruction_loss: 2.2238 - kl_loss: 5.8730\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4001 - reconstruction_loss: 2.2163 - kl_loss: 5.8732 - lr: 7.8125e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3962 - reconstruction_loss: 2.2138 - kl_loss: 5.8772 - lr: 3.9063e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4024 - reconstruction_loss: 2.2163 - kl_loss: 5.8764 - lr: 3.9063e-06\n",
      "Epoch 117/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.3977 - reconstruction_loss: 2.2212 - kl_loss: 5.8782\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3976 - reconstruction_loss: 2.2141 - kl_loss: 5.8779 - lr: 3.9063e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3958 - reconstruction_loss: 2.2129 - kl_loss: 5.8825 - lr: 1.9531e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3938 - reconstruction_loss: 2.2131 - kl_loss: 5.8794 - lr: 1.9531e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.3920 - reconstruction_loss: 2.2125 - kl_loss: 5.8808\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3920 - reconstruction_loss: 2.2125 - kl_loss: 5.8808 - lr: 1.9531e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.3996 - reconstruction_loss: 2.2129 - kl_loss: 5.8811 - lr: 1.0000e-06\n",
      "Epoch 121: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 16:04:36,907]\u001b[0m Trial 80 finished with value: 0.004329346224041648 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 23, 'encoder_units_l2': 847, 'encoder_units_l3': 125, 'decoder_layers': 4, 'decoder_units_l1': 183, 'decoder_units_l2': 48, 'decoder_units_l3': 691, 'decoder_units_l4': 46, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 80 with value: 0.004329346224041648.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 12ms/step - loss: 13.5722 - reconstruction_loss: 9.3161 - kl_loss: 3.7696 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4209 - reconstruction_loss: 5.9063 - kl_loss: 4.6738 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 5.7943 - reconstruction_loss: 4.6200 - kl_loss: 5.1759 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 5.3223 - reconstruction_loss: 4.1450 - kl_loss: 5.4092 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 5.0854 - reconstruction_loss: 3.9074 - kl_loss: 5.5140 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.9021 - reconstruction_loss: 3.7513 - kl_loss: 5.5328 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.7499 - reconstruction_loss: 3.5759 - kl_loss: 5.5715 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.6103 - reconstruction_loss: 3.4500 - kl_loss: 5.6226 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.5225 - reconstruction_loss: 3.3673 - kl_loss: 5.6257 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.3992 - reconstruction_loss: 3.2507 - kl_loss: 5.6470 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.3218 - reconstruction_loss: 3.1624 - kl_loss: 5.6632 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.2774 - reconstruction_loss: 3.1324 - kl_loss: 5.6689 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.2324 - reconstruction_loss: 3.0700 - kl_loss: 5.6791 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.1516 - reconstruction_loss: 3.0002 - kl_loss: 5.6662 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.0991 - reconstruction_loss: 2.9631 - kl_loss: 5.6777 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.0789 - reconstruction_loss: 2.9385 - kl_loss: 5.6740 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.0602 - reconstruction_loss: 2.9246 - kl_loss: 5.7075 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0448 - reconstruction_loss: 2.8762 - kl_loss: 5.6930 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9925 - reconstruction_loss: 2.8186 - kl_loss: 5.6928 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9507 - reconstruction_loss: 2.7911 - kl_loss: 5.6964 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9529 - reconstruction_loss: 2.7879 - kl_loss: 5.7005 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8950 - reconstruction_loss: 2.7645 - kl_loss: 5.6903 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9426 - reconstruction_loss: 2.8122 - kl_loss: 5.7022 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9242 - reconstruction_loss: 2.7501 - kl_loss: 5.7082 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8613 - reconstruction_loss: 2.7165 - kl_loss: 5.7144 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8795 - reconstruction_loss: 2.7054 - kl_loss: 5.6800 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8559 - reconstruction_loss: 2.7118 - kl_loss: 5.7061 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8251 - reconstruction_loss: 2.6828 - kl_loss: 5.6574 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7869 - reconstruction_loss: 2.6426 - kl_loss: 5.6898 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9087 - reconstruction_loss: 2.6925 - kl_loss: 5.7191 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7921 - reconstruction_loss: 2.6275 - kl_loss: 5.7069 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8269 - reconstruction_loss: 2.6377 - kl_loss: 5.7065 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7373 - reconstruction_loss: 2.6247 - kl_loss: 5.6730 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7386 - reconstruction_loss: 2.6163 - kl_loss: 5.7279 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7072 - reconstruction_loss: 2.5673 - kl_loss: 5.6915 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7698 - reconstruction_loss: 2.6053 - kl_loss: 5.7167 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7114 - reconstruction_loss: 2.5610 - kl_loss: 5.6992 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7255 - reconstruction_loss: 2.5454 - kl_loss: 5.6979 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6908 - reconstruction_loss: 2.5646 - kl_loss: 5.7125 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7199 - reconstruction_loss: 2.5799 - kl_loss: 5.6959 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.8938 - reconstruction_loss: 2.6507 - kl_loss: 5.7686\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8933 - reconstruction_loss: 2.6507 - kl_loss: 5.7686 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6676 - reconstruction_loss: 2.4900 - kl_loss: 5.7018 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6282 - reconstruction_loss: 2.4662 - kl_loss: 5.6972 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5858 - reconstruction_loss: 2.4530 - kl_loss: 5.6677 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5725 - reconstruction_loss: 2.4330 - kl_loss: 5.6639 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5875 - reconstruction_loss: 2.4420 - kl_loss: 5.6682 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5863 - reconstruction_loss: 2.4516 - kl_loss: 5.6573 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5729 - reconstruction_loss: 2.4560 - kl_loss: 5.6804\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5730 - reconstruction_loss: 2.4473 - kl_loss: 5.6803 - lr: 5.0000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5735 - reconstruction_loss: 2.4206 - kl_loss: 5.6788 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5468 - reconstruction_loss: 2.4112 - kl_loss: 5.6410 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5373 - reconstruction_loss: 2.4062 - kl_loss: 5.6441 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5386 - reconstruction_loss: 2.3967 - kl_loss: 5.6430 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5276 - reconstruction_loss: 2.3961 - kl_loss: 5.6415 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5298 - reconstruction_loss: 2.3913 - kl_loss: 5.6285 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5181 - reconstruction_loss: 2.3773 - kl_loss: 5.6362 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5230 - reconstruction_loss: 2.3854 - kl_loss: 5.6384 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5149 - reconstruction_loss: 2.3935 - kl_loss: 5.6519 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.5245 - reconstruction_loss: 2.3973 - kl_loss: 5.6611\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5245 - reconstruction_loss: 2.3881 - kl_loss: 5.6606 - lr: 2.5000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5068 - reconstruction_loss: 2.3671 - kl_loss: 5.6463 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4999 - reconstruction_loss: 2.3622 - kl_loss: 5.6456 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5072 - reconstruction_loss: 2.3615 - kl_loss: 5.6482 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4941 - reconstruction_loss: 2.3542 - kl_loss: 5.6419 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4903 - reconstruction_loss: 2.3532 - kl_loss: 5.6406 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4864 - reconstruction_loss: 2.3561 - kl_loss: 5.6423 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4912 - reconstruction_loss: 2.3450 - kl_loss: 5.6342 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4902 - reconstruction_loss: 2.3624 - kl_loss: 5.6210 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4777 - reconstruction_loss: 2.3518 - kl_loss: 5.6455 - lr: 1.2500e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4882 - reconstruction_loss: 2.3548 - kl_loss: 5.6410\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4882 - reconstruction_loss: 2.3548 - kl_loss: 5.6410 - lr: 1.2500e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4914 - reconstruction_loss: 2.3494 - kl_loss: 5.6245 - lr: 6.2500e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4701 - reconstruction_loss: 2.3451 - kl_loss: 5.6245 - lr: 6.2500e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4779 - reconstruction_loss: 2.3448 - kl_loss: 5.6366 - lr: 6.2500e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4798 - reconstruction_loss: 2.3414 - kl_loss: 5.6341 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4734 - reconstruction_loss: 2.3370 - kl_loss: 5.6417 - lr: 6.2500e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4817 - reconstruction_loss: 2.3368 - kl_loss: 5.6258 - lr: 6.2500e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4686 - reconstruction_loss: 2.3346 - kl_loss: 5.6349 - lr: 6.2500e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4618 - reconstruction_loss: 2.3305 - kl_loss: 5.6565 - lr: 6.2500e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4685 - reconstruction_loss: 2.3322 - kl_loss: 5.6350 - lr: 6.2500e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4716 - reconstruction_loss: 2.3394 - kl_loss: 5.6418 - lr: 6.2500e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4636 - reconstruction_loss: 2.3275 - kl_loss: 5.6346 - lr: 6.2500e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4749 - reconstruction_loss: 2.3430 - kl_loss: 5.6374 - lr: 6.2500e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4599 - reconstruction_loss: 2.3370 - kl_loss: 5.6299 - lr: 6.2500e-05\n",
      "Epoch 82/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4667 - reconstruction_loss: 2.3415 - kl_loss: 5.6373\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4667 - reconstruction_loss: 2.3331 - kl_loss: 5.6374 - lr: 6.2500e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4646 - reconstruction_loss: 2.3256 - kl_loss: 5.6383 - lr: 3.1250e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4653 - reconstruction_loss: 2.3205 - kl_loss: 5.6342 - lr: 3.1250e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4548 - reconstruction_loss: 2.3173 - kl_loss: 5.6364 - lr: 3.1250e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4552 - reconstruction_loss: 2.3187 - kl_loss: 5.6348 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4600 - reconstruction_loss: 2.3208 - kl_loss: 5.6302 - lr: 3.1250e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4489 - reconstruction_loss: 2.3232 - kl_loss: 5.6365\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4489 - reconstruction_loss: 2.3232 - kl_loss: 5.6365 - lr: 3.1250e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4544 - reconstruction_loss: 2.3196 - kl_loss: 5.6407 - lr: 1.5625e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4604 - reconstruction_loss: 2.3184 - kl_loss: 5.6344 - lr: 1.5625e-05\n",
      "Epoch 91/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.4480 - reconstruction_loss: 2.3263 - kl_loss: 5.6308\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4481 - reconstruction_loss: 2.3186 - kl_loss: 5.6314 - lr: 1.5625e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4414 - reconstruction_loss: 2.3162 - kl_loss: 5.6367 - lr: 7.8125e-06\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4475 - reconstruction_loss: 2.3159 - kl_loss: 5.6357 - lr: 7.8125e-06\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4470 - reconstruction_loss: 2.3139 - kl_loss: 5.6355 - lr: 7.8125e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4480 - reconstruction_loss: 2.3100 - kl_loss: 5.6420 - lr: 7.8125e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4476 - reconstruction_loss: 2.3152 - kl_loss: 5.6323 - lr: 7.8125e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4477 - reconstruction_loss: 2.3139 - kl_loss: 5.6424 - lr: 7.8125e-06\n",
      "Epoch 98/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.4420 - reconstruction_loss: 2.3188 - kl_loss: 5.6431\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4421 - reconstruction_loss: 2.3103 - kl_loss: 5.6426 - lr: 7.8125e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4648 - reconstruction_loss: 2.3129 - kl_loss: 5.6370 - lr: 3.9063e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4572 - reconstruction_loss: 2.3104 - kl_loss: 5.6374 - lr: 3.9063e-06\n",
      "Epoch 101/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4336 - reconstruction_loss: 2.3113 - kl_loss: 5.6312 - lr: 3.9063e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4485 - reconstruction_loss: 2.3096 - kl_loss: 5.6368 - lr: 3.9063e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4485 - reconstruction_loss: 2.3122 - kl_loss: 5.6393 - lr: 3.9063e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4542 - reconstruction_loss: 2.3079 - kl_loss: 5.6444 - lr: 3.9063e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4432 - reconstruction_loss: 2.3067 - kl_loss: 5.6404 - lr: 3.9063e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4477 - reconstruction_loss: 2.3104 - kl_loss: 5.6385 - lr: 3.9063e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4395 - reconstruction_loss: 2.3078 - kl_loss: 5.6413 - lr: 3.9063e-06\n",
      "Epoch 108/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4521 - reconstruction_loss: 2.3190 - kl_loss: 5.6484\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4519 - reconstruction_loss: 2.3109 - kl_loss: 5.6486 - lr: 3.9063e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4622 - reconstruction_loss: 2.3084 - kl_loss: 5.6537 - lr: 1.9531e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4462 - reconstruction_loss: 2.3071 - kl_loss: 5.6494 - lr: 1.9531e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4320 - reconstruction_loss: 2.3038 - kl_loss: 5.6484 - lr: 1.9531e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4376 - reconstruction_loss: 2.3066 - kl_loss: 5.6453 - lr: 1.9531e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4396 - reconstruction_loss: 2.3066 - kl_loss: 5.6443 - lr: 1.9531e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4401 - reconstruction_loss: 2.3078 - kl_loss: 5.6417\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4401 - reconstruction_loss: 2.3078 - kl_loss: 5.6417 - lr: 1.9531e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4456 - reconstruction_loss: 2.3106 - kl_loss: 5.6407 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4435 - reconstruction_loss: 2.3078 - kl_loss: 5.6422 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4418 - reconstruction_loss: 2.3072 - kl_loss: 5.6419 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4572 - reconstruction_loss: 2.3089 - kl_loss: 5.6438 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4454 - reconstruction_loss: 2.3067 - kl_loss: 5.6449 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4413 - reconstruction_loss: 2.3015 - kl_loss: 5.6440 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4479 - reconstruction_loss: 2.3060 - kl_loss: 5.6427 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4487 - reconstruction_loss: 2.3093 - kl_loss: 5.6425 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4508 - reconstruction_loss: 2.3058 - kl_loss: 5.6438 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4439 - reconstruction_loss: 2.3054 - kl_loss: 5.6420 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4473 - reconstruction_loss: 2.3094 - kl_loss: 5.6419 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4434 - reconstruction_loss: 2.3051 - kl_loss: 5.6428 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4498 - reconstruction_loss: 2.3085 - kl_loss: 5.6419 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4422 - reconstruction_loss: 2.3081 - kl_loss: 5.6422 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4391 - reconstruction_loss: 2.3040 - kl_loss: 5.6435 - lr: 1.0000e-06\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4321 - reconstruction_loss: 2.3057 - kl_loss: 5.6432 - lr: 1.0000e-06\n",
      "Epoch 130: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 16:09:15,793]\u001b[0m Trial 81 finished with value: 0.004516855848722535 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 23, 'encoder_units_l2': 832, 'encoder_units_l3': 114, 'decoder_layers': 4, 'decoder_units_l1': 180, 'decoder_units_l2': 48, 'decoder_units_l3': 711, 'decoder_units_l4': 46, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 80 with value: 0.004329346224041648.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 12.8986 - reconstruction_loss: 8.7517 - kl_loss: 3.9921 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 6.9805 - reconstruction_loss: 5.5198 - kl_loss: 4.9455 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 5.7327 - reconstruction_loss: 4.5175 - kl_loss: 5.2442 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 5.2453 - reconstruction_loss: 4.0788 - kl_loss: 5.5328 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.9853 - reconstruction_loss: 3.7720 - kl_loss: 5.6573 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.8657 - reconstruction_loss: 3.6460 - kl_loss: 5.6937 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.7244 - reconstruction_loss: 3.5094 - kl_loss: 5.7078 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.4932 - reconstruction_loss: 3.3288 - kl_loss: 5.7574 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.4291 - reconstruction_loss: 3.2552 - kl_loss: 5.7694 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.3993 - reconstruction_loss: 3.1619 - kl_loss: 5.7984 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.2279 - reconstruction_loss: 3.0142 - kl_loss: 5.8114 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.1658 - reconstruction_loss: 2.9694 - kl_loss: 5.7736 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.0641 - reconstruction_loss: 2.8681 - kl_loss: 5.8180 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.0320 - reconstruction_loss: 2.8304 - kl_loss: 5.7886 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.9597 - reconstruction_loss: 2.7810 - kl_loss: 5.8107 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.8994 - reconstruction_loss: 2.7266 - kl_loss: 5.7884 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.8590 - reconstruction_loss: 2.7424 - kl_loss: 5.7945 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.0071 - reconstruction_loss: 2.7820 - kl_loss: 5.7587 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.8295 - reconstruction_loss: 2.6665 - kl_loss: 5.7678 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.8297 - reconstruction_loss: 2.7045 - kl_loss: 5.7903 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.9369 - reconstruction_loss: 2.7028 - kl_loss: 5.8020 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.8411 - reconstruction_loss: 2.6635 - kl_loss: 5.7617 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7840 - reconstruction_loss: 2.6642 - kl_loss: 5.7606 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.9399 - reconstruction_loss: 2.7095 - kl_loss: 5.7635 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7878 - reconstruction_loss: 2.6228 - kl_loss: 5.7386 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7453 - reconstruction_loss: 2.5775 - kl_loss: 5.7312 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7473 - reconstruction_loss: 2.6028 - kl_loss: 5.7780 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7394 - reconstruction_loss: 2.5886 - kl_loss: 5.7205 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7036 - reconstruction_loss: 2.5630 - kl_loss: 5.7379 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7460 - reconstruction_loss: 2.5547 - kl_loss: 5.7521 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6500 - reconstruction_loss: 2.5080 - kl_loss: 5.7223 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7053 - reconstruction_loss: 2.5504 - kl_loss: 5.7017 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.7672 - reconstruction_loss: 2.5849 - kl_loss: 5.7416 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.7187 - reconstruction_loss: 2.5386 - kl_loss: 5.7458\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7186 - reconstruction_loss: 2.5386 - kl_loss: 5.7458 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6087 - reconstruction_loss: 2.4377 - kl_loss: 5.7112 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5805 - reconstruction_loss: 2.4295 - kl_loss: 5.6996 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5578 - reconstruction_loss: 2.4073 - kl_loss: 5.6833 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5533 - reconstruction_loss: 2.4108 - kl_loss: 5.6738 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5543 - reconstruction_loss: 2.4052 - kl_loss: 5.6762 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5436 - reconstruction_loss: 2.4038 - kl_loss: 5.6564 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5539 - reconstruction_loss: 2.4190 - kl_loss: 5.6691 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5699 - reconstruction_loss: 2.4171 - kl_loss: 5.6825 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5536 - reconstruction_loss: 2.4210 - kl_loss: 5.6712\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5535 - reconstruction_loss: 2.4133 - kl_loss: 5.6705 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5216 - reconstruction_loss: 2.3781 - kl_loss: 5.6478 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5036 - reconstruction_loss: 2.3676 - kl_loss: 5.6624 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5454 - reconstruction_loss: 2.3793 - kl_loss: 5.6541 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4962 - reconstruction_loss: 2.3598 - kl_loss: 5.6419 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5096 - reconstruction_loss: 2.3603 - kl_loss: 5.6290 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4959 - reconstruction_loss: 2.3571 - kl_loss: 5.6306 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.4763 - reconstruction_loss: 2.3514 - kl_loss: 5.6346 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4751 - reconstruction_loss: 2.3601 - kl_loss: 5.6577 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5030 - reconstruction_loss: 2.3649 - kl_loss: 5.6458 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4985 - reconstruction_loss: 2.3685 - kl_loss: 5.6665\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4985 - reconstruction_loss: 2.3685 - kl_loss: 5.6665 - lr: 2.5000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5029 - reconstruction_loss: 2.3621 - kl_loss: 5.6883 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4893 - reconstruction_loss: 2.3430 - kl_loss: 5.6694 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4964 - reconstruction_loss: 2.3599 - kl_loss: 5.6645 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4858 - reconstruction_loss: 2.3506 - kl_loss: 5.6569 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4747 - reconstruction_loss: 2.3442 - kl_loss: 5.6395 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5056 - reconstruction_loss: 2.3505 - kl_loss: 5.6608 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4894 - reconstruction_loss: 2.3405 - kl_loss: 5.6415 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4806 - reconstruction_loss: 2.3413 - kl_loss: 5.6660 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4723 - reconstruction_loss: 2.3306 - kl_loss: 5.6495 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4586 - reconstruction_loss: 2.3251 - kl_loss: 5.6346 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4600 - reconstruction_loss: 2.3242 - kl_loss: 5.6385 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4625 - reconstruction_loss: 2.3244 - kl_loss: 5.6191 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4607 - reconstruction_loss: 2.3250 - kl_loss: 5.6300 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4537 - reconstruction_loss: 2.3159 - kl_loss: 5.6115 - lr: 1.2500e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4438 - reconstruction_loss: 2.3201 - kl_loss: 5.6161 - lr: 1.2500e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4479 - reconstruction_loss: 2.3207 - kl_loss: 5.6160 - lr: 1.2500e-04\n",
      "Epoch 70/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4505 - reconstruction_loss: 2.3364 - kl_loss: 5.6192\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4506 - reconstruction_loss: 2.3262 - kl_loss: 5.6199 - lr: 1.2500e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4524 - reconstruction_loss: 2.3166 - kl_loss: 5.6151 - lr: 6.2500e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4512 - reconstruction_loss: 2.3123 - kl_loss: 5.6304 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4458 - reconstruction_loss: 2.3147 - kl_loss: 5.6110 - lr: 6.2500e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4376 - reconstruction_loss: 2.3092 - kl_loss: 5.6149 - lr: 6.2500e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4389 - reconstruction_loss: 2.3052 - kl_loss: 5.6248 - lr: 6.2500e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4272 - reconstruction_loss: 2.3035 - kl_loss: 5.6066 - lr: 6.2500e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4362 - reconstruction_loss: 2.3018 - kl_loss: 5.6215 - lr: 6.2500e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4266 - reconstruction_loss: 2.2956 - kl_loss: 5.6026 - lr: 6.2500e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4339 - reconstruction_loss: 2.3025 - kl_loss: 5.6206 - lr: 6.2500e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4272 - reconstruction_loss: 2.3005 - kl_loss: 5.6075 - lr: 6.2500e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4109 - reconstruction_loss: 2.2914 - kl_loss: 5.6144 - lr: 6.2500e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4153 - reconstruction_loss: 2.2946 - kl_loss: 5.6118 - lr: 6.2500e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4214 - reconstruction_loss: 2.2918 - kl_loss: 5.6106 - lr: 6.2500e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4177 - reconstruction_loss: 2.2897 - kl_loss: 5.6077 - lr: 6.2500e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4317 - reconstruction_loss: 2.2880 - kl_loss: 5.6202 - lr: 6.2500e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4155 - reconstruction_loss: 2.2940 - kl_loss: 5.6060 - lr: 6.2500e-05\n",
      "Epoch 87/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4207 - reconstruction_loss: 2.3023 - kl_loss: 5.6135\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4207 - reconstruction_loss: 2.2940 - kl_loss: 5.6143 - lr: 6.2500e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4093 - reconstruction_loss: 2.2804 - kl_loss: 5.6245 - lr: 3.1250e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4098 - reconstruction_loss: 2.2794 - kl_loss: 5.6110 - lr: 3.1250e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4041 - reconstruction_loss: 2.2838 - kl_loss: 5.6154 - lr: 3.1250e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4187 - reconstruction_loss: 2.2813 - kl_loss: 5.6122 - lr: 3.1250e-05\n",
      "Epoch 92/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4126 - reconstruction_loss: 2.2887 - kl_loss: 5.6186\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4125 - reconstruction_loss: 2.2822 - kl_loss: 5.6179 - lr: 3.1250e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4097 - reconstruction_loss: 2.2768 - kl_loss: 5.6099 - lr: 1.5625e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4149 - reconstruction_loss: 2.2752 - kl_loss: 5.6118 - lr: 1.5625e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4119 - reconstruction_loss: 2.2766 - kl_loss: 5.6197 - lr: 1.5625e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4058 - reconstruction_loss: 2.2751 - kl_loss: 5.6158 - lr: 1.5625e-05\n",
      "Epoch 97/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4046 - reconstruction_loss: 2.2827 - kl_loss: 5.6155\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4046 - reconstruction_loss: 2.2752 - kl_loss: 5.6156 - lr: 1.5625e-05\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3945 - reconstruction_loss: 2.2729 - kl_loss: 5.6130 - lr: 7.8125e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4157 - reconstruction_loss: 2.2731 - kl_loss: 5.6195 - lr: 7.8125e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4093 - reconstruction_loss: 2.2696 - kl_loss: 5.6217 - lr: 7.8125e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3988 - reconstruction_loss: 2.2687 - kl_loss: 5.6192 - lr: 7.8125e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3920 - reconstruction_loss: 2.2686 - kl_loss: 5.6197 - lr: 7.8125e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3918 - reconstruction_loss: 2.2709 - kl_loss: 5.6147 - lr: 7.8125e-06\n",
      "Epoch 104/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4120 - reconstruction_loss: 2.2802 - kl_loss: 5.6080\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4117 - reconstruction_loss: 2.2707 - kl_loss: 5.6086 - lr: 7.8125e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3965 - reconstruction_loss: 2.2698 - kl_loss: 5.6140 - lr: 3.9063e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4041 - reconstruction_loss: 2.2664 - kl_loss: 5.6151 - lr: 3.9063e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4008 - reconstruction_loss: 2.2672 - kl_loss: 5.6167 - lr: 3.9063e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4131 - reconstruction_loss: 2.2675 - kl_loss: 5.6161 - lr: 3.9063e-06\n",
      "Epoch 109/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4195 - reconstruction_loss: 2.2758 - kl_loss: 5.6179\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4190 - reconstruction_loss: 2.2668 - kl_loss: 5.6181 - lr: 3.9063e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3947 - reconstruction_loss: 2.2654 - kl_loss: 5.6202 - lr: 1.9531e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3943 - reconstruction_loss: 2.2629 - kl_loss: 5.6183 - lr: 1.9531e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3999 - reconstruction_loss: 2.2648 - kl_loss: 5.6094 - lr: 1.9531e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3945 - reconstruction_loss: 2.2695 - kl_loss: 5.6114 - lr: 1.9531e-06\n",
      "Epoch 114/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4028 - reconstruction_loss: 2.2743 - kl_loss: 5.6164\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4026 - reconstruction_loss: 2.2650 - kl_loss: 5.6162 - lr: 1.9531e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3846 - reconstruction_loss: 2.2634 - kl_loss: 5.6204 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4057 - reconstruction_loss: 2.2635 - kl_loss: 5.6224 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3957 - reconstruction_loss: 2.2669 - kl_loss: 5.6218 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3958 - reconstruction_loss: 2.2652 - kl_loss: 5.6208 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3996 - reconstruction_loss: 2.2649 - kl_loss: 5.6220 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3895 - reconstruction_loss: 2.2638 - kl_loss: 5.6205 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4093 - reconstruction_loss: 2.2650 - kl_loss: 5.6202 - lr: 1.0000e-06\n",
      "Epoch 121: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 16:12:43,900]\u001b[0m Trial 82 finished with value: 0.0044391484151938545 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 34, 'encoder_units_l2': 660, 'encoder_units_l3': 140, 'decoder_layers': 4, 'decoder_units_l1': 181, 'decoder_units_l2': 49, 'decoder_units_l3': 691, 'decoder_units_l4': 45, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 80 with value: 0.004329346224041648.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 6s 26ms/step - loss: 13.3957 - reconstruction_loss: 9.1685 - kl_loss: 3.8484 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 7.3501 - reconstruction_loss: 5.7675 - kl_loss: 4.7967 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 5.6438 - reconstruction_loss: 4.4607 - kl_loss: 5.2729 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 5.2189 - reconstruction_loss: 4.0501 - kl_loss: 5.4405 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 5.0286 - reconstruction_loss: 3.8278 - kl_loss: 5.5578 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 4.8266 - reconstruction_loss: 3.6512 - kl_loss: 5.6242 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 4.7007 - reconstruction_loss: 3.5199 - kl_loss: 5.7294 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 4.5651 - reconstruction_loss: 3.3957 - kl_loss: 5.7417 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 4.5528 - reconstruction_loss: 3.3303 - kl_loss: 5.7986 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 4.4050 - reconstruction_loss: 3.2204 - kl_loss: 5.7801 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 4.3441 - reconstruction_loss: 3.1539 - kl_loss: 5.7957 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 4.3407 - reconstruction_loss: 3.1686 - kl_loss: 5.7684 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 4.2212 - reconstruction_loss: 3.0259 - kl_loss: 5.7878 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 4.1614 - reconstruction_loss: 2.9653 - kl_loss: 5.8177 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 4.1241 - reconstruction_loss: 2.9642 - kl_loss: 5.8077 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 4.0848 - reconstruction_loss: 2.9152 - kl_loss: 5.8222 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 4.0677 - reconstruction_loss: 2.9022 - kl_loss: 5.7679 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 4.0151 - reconstruction_loss: 2.8584 - kl_loss: 5.8069 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 4.0598 - reconstruction_loss: 2.8641 - kl_loss: 5.8244 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.9510 - reconstruction_loss: 2.7887 - kl_loss: 5.8101 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.9951 - reconstruction_loss: 2.7783 - kl_loss: 5.8479 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.8933 - reconstruction_loss: 2.7351 - kl_loss: 5.8056 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.8847 - reconstruction_loss: 2.7423 - kl_loss: 5.7809 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.9086 - reconstruction_loss: 2.7177 - kl_loss: 5.7595 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.8878 - reconstruction_loss: 2.7058 - kl_loss: 5.7728 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.8641 - reconstruction_loss: 2.6923 - kl_loss: 5.7632 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.9260 - reconstruction_loss: 2.7257 - kl_loss: 5.7669 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.8141 - reconstruction_loss: 2.6521 - kl_loss: 5.7660 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.8266 - reconstruction_loss: 2.6435 - kl_loss: 5.7573 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.7897 - reconstruction_loss: 2.6302 - kl_loss: 5.7842 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.8417 - reconstruction_loss: 2.6706 - kl_loss: 5.7719 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.7617 - reconstruction_loss: 2.6067 - kl_loss: 5.7485 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.7878 - reconstruction_loss: 2.6328 - kl_loss: 5.7764 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.7474 - reconstruction_loss: 2.5979 - kl_loss: 5.7465 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.7733 - reconstruction_loss: 2.6355 - kl_loss: 5.7323 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.7477 - reconstruction_loss: 2.6223 - kl_loss: 5.7694 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.7251 - reconstruction_loss: 2.5548 - kl_loss: 5.7432 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.7291 - reconstruction_loss: 2.5912 - kl_loss: 5.7420 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.7654 - reconstruction_loss: 2.6117 - kl_loss: 5.7634 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.7645 - reconstruction_loss: 2.5891 - kl_loss: 5.7545\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.7640 - reconstruction_loss: 2.5791 - kl_loss: 5.7528 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.6553 - reconstruction_loss: 2.4967 - kl_loss: 5.7539 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.6480 - reconstruction_loss: 2.4985 - kl_loss: 5.7581 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.6628 - reconstruction_loss: 2.5040 - kl_loss: 5.7663 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.6678 - reconstruction_loss: 2.5296 - kl_loss: 5.7512\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.6679 - reconstruction_loss: 2.5210 - kl_loss: 5.7509 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.6165 - reconstruction_loss: 2.4662 - kl_loss: 5.6838 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.6031 - reconstruction_loss: 2.4478 - kl_loss: 5.6974 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5836 - reconstruction_loss: 2.4420 - kl_loss: 5.6879 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.6135 - reconstruction_loss: 2.4470 - kl_loss: 5.6898 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.5869 - reconstruction_loss: 2.4579 - kl_loss: 5.7210 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5765 - reconstruction_loss: 2.4326 - kl_loss: 5.7210 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.6310 - reconstruction_loss: 2.4626 - kl_loss: 5.7866 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5904 - reconstruction_loss: 2.4315 - kl_loss: 5.7369 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.6130 - reconstruction_loss: 2.4602 - kl_loss: 5.7346\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.6129 - reconstruction_loss: 2.4514 - kl_loss: 5.7347 - lr: 2.5000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5753 - reconstruction_loss: 2.4225 - kl_loss: 5.6982 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5566 - reconstruction_loss: 2.4185 - kl_loss: 5.7052 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.5671 - reconstruction_loss: 2.4081 - kl_loss: 5.7032 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5438 - reconstruction_loss: 2.4019 - kl_loss: 5.6814 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5468 - reconstruction_loss: 2.4040 - kl_loss: 5.6756 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5610 - reconstruction_loss: 2.4040 - kl_loss: 5.6892 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5442 - reconstruction_loss: 2.3988 - kl_loss: 5.6921 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5623 - reconstruction_loss: 2.3993 - kl_loss: 5.7000 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5493 - reconstruction_loss: 2.3999 - kl_loss: 5.7022 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5418 - reconstruction_loss: 2.3963 - kl_loss: 5.6971 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5537 - reconstruction_loss: 2.4027 - kl_loss: 5.6951 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5488 - reconstruction_loss: 2.4021 - kl_loss: 5.6860 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5521 - reconstruction_loss: 2.3947 - kl_loss: 5.6987 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5190 - reconstruction_loss: 2.3978 - kl_loss: 5.6847 - lr: 1.2500e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.5511 - reconstruction_loss: 2.4056 - kl_loss: 5.6698 - lr: 1.2500e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.5212 - reconstruction_loss: 2.3928 - kl_loss: 5.6919 - lr: 1.2500e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.5312 - reconstruction_loss: 2.3855 - kl_loss: 5.6934 - lr: 1.2500e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.5241 - reconstruction_loss: 2.3861 - kl_loss: 5.6880 - lr: 1.2500e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5445 - reconstruction_loss: 2.3891 - kl_loss: 5.6837 - lr: 1.2500e-04\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5230 - reconstruction_loss: 2.3821 - kl_loss: 5.7208 - lr: 1.2500e-04\n",
      "Epoch 74/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5295 - reconstruction_loss: 2.3925 - kl_loss: 5.7138\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5294 - reconstruction_loss: 2.3841 - kl_loss: 5.7135 - lr: 1.2500e-04\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5388 - reconstruction_loss: 2.3853 - kl_loss: 5.7243 - lr: 6.2500e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.5463 - reconstruction_loss: 2.3718 - kl_loss: 5.7231 - lr: 6.2500e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.5193 - reconstruction_loss: 2.3702 - kl_loss: 5.7063 - lr: 6.2500e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5225 - reconstruction_loss: 2.3697 - kl_loss: 5.7239 - lr: 6.2500e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.5296 - reconstruction_loss: 2.3701 - kl_loss: 5.7270 - lr: 6.2500e-05\n",
      "Epoch 80/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5035 - reconstruction_loss: 2.3686 - kl_loss: 5.7670\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.5036 - reconstruction_loss: 2.3611 - kl_loss: 5.7665 - lr: 6.2500e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5241 - reconstruction_loss: 2.3632 - kl_loss: 5.7280 - lr: 3.1250e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5232 - reconstruction_loss: 2.3526 - kl_loss: 5.7448 - lr: 3.1250e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 5s 28ms/step - loss: 3.5073 - reconstruction_loss: 2.3516 - kl_loss: 5.7227 - lr: 3.1250e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.5062 - reconstruction_loss: 2.3518 - kl_loss: 5.7290 - lr: 3.1250e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5164 - reconstruction_loss: 2.3526 - kl_loss: 5.7324 - lr: 3.1250e-05\n",
      "Epoch 86/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.5032 - reconstruction_loss: 2.3617 - kl_loss: 5.7342\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5032 - reconstruction_loss: 2.3533 - kl_loss: 5.7346 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.5059 - reconstruction_loss: 2.3486 - kl_loss: 5.7401 - lr: 1.5625e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.5103 - reconstruction_loss: 2.3495 - kl_loss: 5.7301 - lr: 1.5625e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5069 - reconstruction_loss: 2.3459 - kl_loss: 5.7252 - lr: 1.5625e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4975 - reconstruction_loss: 2.3441 - kl_loss: 5.7300 - lr: 1.5625e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4931 - reconstruction_loss: 2.3428 - kl_loss: 5.7355 - lr: 1.5625e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5015 - reconstruction_loss: 2.3410 - kl_loss: 5.7365 - lr: 1.5625e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4926 - reconstruction_loss: 2.3413 - kl_loss: 5.7386 - lr: 1.5625e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4903 - reconstruction_loss: 2.3400 - kl_loss: 5.7317 - lr: 1.5625e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4905 - reconstruction_loss: 2.3399 - kl_loss: 5.7215 - lr: 1.5625e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4851 - reconstruction_loss: 2.3442 - kl_loss: 5.7233 - lr: 1.5625e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4914 - reconstruction_loss: 2.3440 - kl_loss: 5.7273 - lr: 1.5625e-05\n",
      "Epoch 98/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4946 - reconstruction_loss: 2.3469 - kl_loss: 5.7289\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.4945 - reconstruction_loss: 2.3388 - kl_loss: 5.7293 - lr: 1.5625e-05\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4918 - reconstruction_loss: 2.3347 - kl_loss: 5.7314 - lr: 7.8125e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4809 - reconstruction_loss: 2.3349 - kl_loss: 5.7252 - lr: 7.8125e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5000 - reconstruction_loss: 2.3368 - kl_loss: 5.7230 - lr: 7.8125e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4782 - reconstruction_loss: 2.3353 - kl_loss: 5.7241 - lr: 7.8125e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4878 - reconstruction_loss: 2.3338 - kl_loss: 5.7262 - lr: 7.8125e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4900 - reconstruction_loss: 2.3320 - kl_loss: 5.7224 - lr: 7.8125e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4936 - reconstruction_loss: 2.3333 - kl_loss: 5.7172 - lr: 7.8125e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4794 - reconstruction_loss: 2.3334 - kl_loss: 5.7247 - lr: 7.8125e-06\n",
      "Epoch 107/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4974 - reconstruction_loss: 2.3436 - kl_loss: 5.7238\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4972 - reconstruction_loss: 2.3349 - kl_loss: 5.7237 - lr: 7.8125e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4933 - reconstruction_loss: 2.3334 - kl_loss: 5.7267 - lr: 3.9063e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4803 - reconstruction_loss: 2.3305 - kl_loss: 5.7255 - lr: 3.9063e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4769 - reconstruction_loss: 2.3297 - kl_loss: 5.7275 - lr: 3.9063e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4900 - reconstruction_loss: 2.3318 - kl_loss: 5.7278 - lr: 3.9063e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4906 - reconstruction_loss: 2.3288 - kl_loss: 5.7265 - lr: 3.9063e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4960 - reconstruction_loss: 2.3305 - kl_loss: 5.7278 - lr: 3.9063e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4803 - reconstruction_loss: 2.3322 - kl_loss: 5.7200 - lr: 3.9063e-06\n",
      "Epoch 115/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4742 - reconstruction_loss: 2.3390 - kl_loss: 5.7241\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4742 - reconstruction_loss: 2.3310 - kl_loss: 5.7244 - lr: 3.9063e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4770 - reconstruction_loss: 2.3305 - kl_loss: 5.7276 - lr: 1.9531e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4920 - reconstruction_loss: 2.3297 - kl_loss: 5.7307 - lr: 1.9531e-06\n",
      "Epoch 118/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4890 - reconstruction_loss: 2.3373 - kl_loss: 5.7304\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4888 - reconstruction_loss: 2.3294 - kl_loss: 5.7312 - lr: 1.9531e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4755 - reconstruction_loss: 2.3261 - kl_loss: 5.7323 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4746 - reconstruction_loss: 2.3290 - kl_loss: 5.7300 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4794 - reconstruction_loss: 2.3289 - kl_loss: 5.7309 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4837 - reconstruction_loss: 2.3272 - kl_loss: 5.7295 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4877 - reconstruction_loss: 2.3297 - kl_loss: 5.7272 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4828 - reconstruction_loss: 2.3273 - kl_loss: 5.7289 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4823 - reconstruction_loss: 2.3272 - kl_loss: 5.7308 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4834 - reconstruction_loss: 2.3250 - kl_loss: 5.7286 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4850 - reconstruction_loss: 2.3270 - kl_loss: 5.7270 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4821 - reconstruction_loss: 2.3313 - kl_loss: 5.7280 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4866 - reconstruction_loss: 2.3286 - kl_loss: 5.7305 - lr: 1.0000e-06\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.4791 - reconstruction_loss: 2.3279 - kl_loss: 5.7296 - lr: 1.0000e-06\n",
      "Epoch 131/300\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.4808 - reconstruction_loss: 2.3263 - kl_loss: 5.7288 - lr: 1.0000e-06\n",
      "Epoch 132/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4682 - reconstruction_loss: 2.3274 - kl_loss: 5.7274 - lr: 1.0000e-06\n",
      "Epoch 133/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4712 - reconstruction_loss: 2.3293 - kl_loss: 5.7296 - lr: 1.0000e-06\n",
      "Epoch 134/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4762 - reconstruction_loss: 2.3255 - kl_loss: 5.7286 - lr: 1.0000e-06\n",
      "Epoch 135/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4746 - reconstruction_loss: 2.3264 - kl_loss: 5.7271 - lr: 1.0000e-06\n",
      "Epoch 136/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4800 - reconstruction_loss: 2.3281 - kl_loss: 5.7272 - lr: 1.0000e-06\n",
      "Epoch 136: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 16:24:15,048]\u001b[0m Trial 83 finished with value: 0.00456037401818068 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 48, 'encoder_units_l2': 657, 'encoder_units_l3': 150, 'decoder_layers': 5, 'decoder_units_l1': 226, 'decoder_units_l2': 37, 'decoder_units_l3': 662, 'decoder_units_l4': 51, 'decoder_units_l5': 16, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 80 with value: 0.004329346224041648.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 4s 17ms/step - loss: 13.0672 - reconstruction_loss: 8.9206 - kl_loss: 3.9178 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 6.9902 - reconstruction_loss: 5.5602 - kl_loss: 4.8677 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 5.8261 - reconstruction_loss: 4.6293 - kl_loss: 5.2128 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 5.3719 - reconstruction_loss: 4.1862 - kl_loss: 5.4446 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 5.1154 - reconstruction_loss: 3.9225 - kl_loss: 5.5303 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.9351 - reconstruction_loss: 3.7316 - kl_loss: 5.6140 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.7988 - reconstruction_loss: 3.6142 - kl_loss: 5.6926 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.6851 - reconstruction_loss: 3.5068 - kl_loss: 5.7153 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.5958 - reconstruction_loss: 3.4084 - kl_loss: 5.7455 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.5233 - reconstruction_loss: 3.3372 - kl_loss: 5.7771 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.4070 - reconstruction_loss: 3.2377 - kl_loss: 5.8190 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.3670 - reconstruction_loss: 3.1577 - kl_loss: 5.8520 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.2827 - reconstruction_loss: 3.1033 - kl_loss: 5.8496 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.2302 - reconstruction_loss: 3.0655 - kl_loss: 5.8576 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.1792 - reconstruction_loss: 2.9697 - kl_loss: 5.8765 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.1267 - reconstruction_loss: 2.9308 - kl_loss: 5.8835 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.1201 - reconstruction_loss: 2.9402 - kl_loss: 5.8774 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.0679 - reconstruction_loss: 2.8536 - kl_loss: 5.8658 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.0205 - reconstruction_loss: 2.8116 - kl_loss: 5.8624 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.9774 - reconstruction_loss: 2.8098 - kl_loss: 5.8678 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.9279 - reconstruction_loss: 2.7384 - kl_loss: 5.8861 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.9092 - reconstruction_loss: 2.7550 - kl_loss: 5.8417 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.9153 - reconstruction_loss: 2.7079 - kl_loss: 5.8451 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.9343 - reconstruction_loss: 2.7250 - kl_loss: 5.8621 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.8368 - reconstruction_loss: 2.6808 - kl_loss: 5.8006 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.8687 - reconstruction_loss: 2.6592 - kl_loss: 5.8276 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.7974 - reconstruction_loss: 2.6112 - kl_loss: 5.8120 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.8191 - reconstruction_loss: 2.6452 - kl_loss: 5.8062 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.8118 - reconstruction_loss: 2.6198 - kl_loss: 5.8168 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.7515 - reconstruction_loss: 2.6031 - kl_loss: 5.7969 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.7727 - reconstruction_loss: 2.6197 - kl_loss: 5.7875 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.7400 - reconstruction_loss: 2.5818 - kl_loss: 5.7877 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.7282 - reconstruction_loss: 2.5675 - kl_loss: 5.7784 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.7702 - reconstruction_loss: 2.5994 - kl_loss: 5.7667 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.7877 - reconstruction_loss: 2.6256 - kl_loss: 5.7940 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.7665 - reconstruction_loss: 2.6417 - kl_loss: 5.8013\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.7667 - reconstruction_loss: 2.6417 - kl_loss: 5.8013 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.7667 - reconstruction_loss: 2.5515 - kl_loss: 5.7819 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6430 - reconstruction_loss: 2.4779 - kl_loss: 5.7892 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6509 - reconstruction_loss: 2.4942 - kl_loss: 5.8039 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6302 - reconstruction_loss: 2.4600 - kl_loss: 5.7894 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6594 - reconstruction_loss: 2.5037 - kl_loss: 5.7885 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6728 - reconstruction_loss: 2.4779 - kl_loss: 5.7936 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6045 - reconstruction_loss: 2.4402 - kl_loss: 5.7664 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5911 - reconstruction_loss: 2.4219 - kl_loss: 5.7798 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5795 - reconstruction_loss: 2.4216 - kl_loss: 5.7592 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5615 - reconstruction_loss: 2.4154 - kl_loss: 5.7567 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5724 - reconstruction_loss: 2.4167 - kl_loss: 5.7480 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5672 - reconstruction_loss: 2.4114 - kl_loss: 5.7433 - lr: 5.0000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5788 - reconstruction_loss: 2.4136 - kl_loss: 5.7675 - lr: 5.0000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5684 - reconstruction_loss: 2.3995 - kl_loss: 5.7602 - lr: 5.0000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6246 - reconstruction_loss: 2.4402 - kl_loss: 5.7376 - lr: 5.0000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5563 - reconstruction_loss: 2.4011 - kl_loss: 5.7232 - lr: 5.0000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5680 - reconstruction_loss: 2.4023 - kl_loss: 5.7330 - lr: 5.0000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5690 - reconstruction_loss: 2.4124 - kl_loss: 5.7255 - lr: 5.0000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5415 - reconstruction_loss: 2.3901 - kl_loss: 5.7536 - lr: 5.0000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6181 - reconstruction_loss: 2.4331 - kl_loss: 5.7920 - lr: 5.0000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5561 - reconstruction_loss: 2.3967 - kl_loss: 5.7638 - lr: 5.0000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5406 - reconstruction_loss: 2.3792 - kl_loss: 5.7726 - lr: 5.0000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5496 - reconstruction_loss: 2.4028 - kl_loss: 5.8091 - lr: 5.0000e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6245 - reconstruction_loss: 2.4211 - kl_loss: 5.7694 - lr: 5.0000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5336 - reconstruction_loss: 2.3669 - kl_loss: 5.7577 - lr: 5.0000e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5440 - reconstruction_loss: 2.3651 - kl_loss: 5.7831 - lr: 5.0000e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5002 - reconstruction_loss: 2.3346 - kl_loss: 5.7489 - lr: 5.0000e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5089 - reconstruction_loss: 2.3352 - kl_loss: 5.7597 - lr: 5.0000e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5120 - reconstruction_loss: 2.3591 - kl_loss: 5.7708 - lr: 5.0000e-04\n",
      "Epoch 66/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.5034 - reconstruction_loss: 2.3430 - kl_loss: 5.7784\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5033 - reconstruction_loss: 2.3355 - kl_loss: 5.7785 - lr: 5.0000e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4665 - reconstruction_loss: 2.2983 - kl_loss: 5.7747 - lr: 2.5000e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4696 - reconstruction_loss: 2.2913 - kl_loss: 5.7738 - lr: 2.5000e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4383 - reconstruction_loss: 2.2868 - kl_loss: 5.7785 - lr: 2.5000e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4461 - reconstruction_loss: 2.2853 - kl_loss: 5.7771 - lr: 2.5000e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4509 - reconstruction_loss: 2.2959 - kl_loss: 5.7842 - lr: 2.5000e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4711 - reconstruction_loss: 2.3009 - kl_loss: 5.7739 - lr: 2.5000e-04\n",
      "Epoch 73/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4357 - reconstruction_loss: 2.2946 - kl_loss: 5.7861\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4359 - reconstruction_loss: 2.2852 - kl_loss: 5.7876 - lr: 2.5000e-04\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4332 - reconstruction_loss: 2.2636 - kl_loss: 5.7909 - lr: 1.2500e-04\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4212 - reconstruction_loss: 2.2571 - kl_loss: 5.7802 - lr: 1.2500e-04\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4307 - reconstruction_loss: 2.2579 - kl_loss: 5.7824 - lr: 1.2500e-04\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4461 - reconstruction_loss: 2.2660 - kl_loss: 5.7871 - lr: 1.2500e-04\n",
      "Epoch 78/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4145 - reconstruction_loss: 2.2667 - kl_loss: 5.7814\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4146 - reconstruction_loss: 2.2583 - kl_loss: 5.7806 - lr: 1.2500e-04\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4208 - reconstruction_loss: 2.2542 - kl_loss: 5.7887 - lr: 6.2500e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4143 - reconstruction_loss: 2.2499 - kl_loss: 5.7770 - lr: 6.2500e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4061 - reconstruction_loss: 2.2434 - kl_loss: 5.8014 - lr: 6.2500e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4055 - reconstruction_loss: 2.2460 - kl_loss: 5.8010 - lr: 6.2500e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4191 - reconstruction_loss: 2.2484 - kl_loss: 5.7901 - lr: 6.2500e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4012 - reconstruction_loss: 2.2447 - kl_loss: 5.7812 - lr: 6.2500e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4058 - reconstruction_loss: 2.2409 - kl_loss: 5.7977 - lr: 6.2500e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4017 - reconstruction_loss: 2.2384 - kl_loss: 5.7983 - lr: 6.2500e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4004 - reconstruction_loss: 2.2399 - kl_loss: 5.8109 - lr: 6.2500e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4226 - reconstruction_loss: 2.2471 - kl_loss: 5.7844 - lr: 6.2500e-05\n",
      "Epoch 89/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4143 - reconstruction_loss: 2.2592 - kl_loss: 5.7816\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4143 - reconstruction_loss: 2.2509 - kl_loss: 5.7817 - lr: 6.2500e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4077 - reconstruction_loss: 2.2403 - kl_loss: 5.7904 - lr: 3.1250e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4176 - reconstruction_loss: 2.2376 - kl_loss: 5.7979 - lr: 3.1250e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3956 - reconstruction_loss: 2.2331 - kl_loss: 5.8000 - lr: 3.1250e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3806 - reconstruction_loss: 2.2318 - kl_loss: 5.7893 - lr: 3.1250e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4037 - reconstruction_loss: 2.2317 - kl_loss: 5.8013 - lr: 3.1250e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3924 - reconstruction_loss: 2.2321 - kl_loss: 5.7899 - lr: 3.1250e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3779 - reconstruction_loss: 2.2278 - kl_loss: 5.8018 - lr: 3.1250e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4020 - reconstruction_loss: 2.2325 - kl_loss: 5.8106 - lr: 3.1250e-05\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3967 - reconstruction_loss: 2.2279 - kl_loss: 5.7964 - lr: 3.1250e-05\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.3878 - reconstruction_loss: 2.2275 - kl_loss: 5.7902 - lr: 3.1250e-05\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3871 - reconstruction_loss: 2.2250 - kl_loss: 5.7931 - lr: 3.1250e-05\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3823 - reconstruction_loss: 2.2248 - kl_loss: 5.7979 - lr: 3.1250e-05\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3986 - reconstruction_loss: 2.2267 - kl_loss: 5.7992 - lr: 3.1250e-05\n",
      "Epoch 103/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.3908 - reconstruction_loss: 2.2317 - kl_loss: 5.8054\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.3908 - reconstruction_loss: 2.2233 - kl_loss: 5.8048 - lr: 3.1250e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3911 - reconstruction_loss: 2.2191 - kl_loss: 5.8008 - lr: 1.5625e-05\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.3971 - reconstruction_loss: 2.2155 - kl_loss: 5.8095 - lr: 1.5625e-05\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.3947 - reconstruction_loss: 2.2158 - kl_loss: 5.7975 - lr: 1.5625e-05\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3858 - reconstruction_loss: 2.2176 - kl_loss: 5.8038 - lr: 1.5625e-05\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3884 - reconstruction_loss: 2.2193 - kl_loss: 5.8061 - lr: 1.5625e-05\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3772 - reconstruction_loss: 2.2123 - kl_loss: 5.8132 - lr: 1.5625e-05\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3753 - reconstruction_loss: 2.2118 - kl_loss: 5.8125 - lr: 1.5625e-05\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3898 - reconstruction_loss: 2.2175 - kl_loss: 5.7986 - lr: 1.5625e-05\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3926 - reconstruction_loss: 2.2150 - kl_loss: 5.8024 - lr: 1.5625e-05\n",
      "Epoch 113/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.3965 - reconstruction_loss: 2.2227 - kl_loss: 5.8023\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3962 - reconstruction_loss: 2.2157 - kl_loss: 5.8030 - lr: 1.5625e-05\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3890 - reconstruction_loss: 2.2126 - kl_loss: 5.8073 - lr: 7.8125e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.3867 - reconstruction_loss: 2.2088 - kl_loss: 5.8158 - lr: 7.8125e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3855 - reconstruction_loss: 2.2103 - kl_loss: 5.8139 - lr: 7.8125e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3863 - reconstruction_loss: 2.2087 - kl_loss: 5.8195 - lr: 7.8125e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.3628 - reconstruction_loss: 2.2113 - kl_loss: 5.8014 - lr: 7.8125e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3786 - reconstruction_loss: 2.2082 - kl_loss: 5.8017 - lr: 7.8125e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.3860 - reconstruction_loss: 2.2094 - kl_loss: 5.8045 - lr: 7.8125e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.3783 - reconstruction_loss: 2.2109 - kl_loss: 5.8075 - lr: 7.8125e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.3707 - reconstruction_loss: 2.2081 - kl_loss: 5.8091\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3707 - reconstruction_loss: 2.2081 - kl_loss: 5.8091 - lr: 7.8125e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3779 - reconstruction_loss: 2.2089 - kl_loss: 5.8123 - lr: 3.9063e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3789 - reconstruction_loss: 2.2057 - kl_loss: 5.8208 - lr: 3.9063e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.3841 - reconstruction_loss: 2.2060 - kl_loss: 5.8171\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3840 - reconstruction_loss: 2.2060 - kl_loss: 5.8171 - lr: 3.9063e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3758 - reconstruction_loss: 2.2068 - kl_loss: 5.8137 - lr: 1.9531e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3802 - reconstruction_loss: 2.2044 - kl_loss: 5.8134 - lr: 1.9531e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3796 - reconstruction_loss: 2.2057 - kl_loss: 5.8106 - lr: 1.9531e-06\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.3746 - reconstruction_loss: 2.2075 - kl_loss: 5.8063 - lr: 1.9531e-06\n",
      "Epoch 130/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.3848 - reconstruction_loss: 2.2158 - kl_loss: 5.8104\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3846 - reconstruction_loss: 2.2082 - kl_loss: 5.8109 - lr: 1.9531e-06\n",
      "Epoch 131/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3818 - reconstruction_loss: 2.2072 - kl_loss: 5.8111 - lr: 1.0000e-06\n",
      "Epoch 132/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3833 - reconstruction_loss: 2.2042 - kl_loss: 5.8114 - lr: 1.0000e-06\n",
      "Epoch 133/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3900 - reconstruction_loss: 2.2067 - kl_loss: 5.8126 - lr: 1.0000e-06\n",
      "Epoch 134/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3813 - reconstruction_loss: 2.2047 - kl_loss: 5.8122 - lr: 1.0000e-06\n",
      "Epoch 135/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3662 - reconstruction_loss: 2.2073 - kl_loss: 5.8116 - lr: 1.0000e-06\n",
      "Epoch 136/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3747 - reconstruction_loss: 2.2052 - kl_loss: 5.8085 - lr: 1.0000e-06\n",
      "Epoch 137/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3848 - reconstruction_loss: 2.2090 - kl_loss: 5.8085 - lr: 1.0000e-06\n",
      "Epoch 138/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.3782 - reconstruction_loss: 2.2054 - kl_loss: 5.8101 - lr: 1.0000e-06\n",
      "Epoch 139/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3758 - reconstruction_loss: 2.2063 - kl_loss: 5.8098 - lr: 1.0000e-06\n",
      "Epoch 140/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3668 - reconstruction_loss: 2.2063 - kl_loss: 5.8092 - lr: 1.0000e-06\n",
      "Epoch 141/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3771 - reconstruction_loss: 2.2074 - kl_loss: 5.8101 - lr: 1.0000e-06\n",
      "Epoch 142/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.3737 - reconstruction_loss: 2.2052 - kl_loss: 5.8100 - lr: 1.0000e-06\n",
      "Epoch 142: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 16:31:34,934]\u001b[0m Trial 84 finished with value: 0.004318432477628052 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 31, 'encoder_units_l2': 452, 'encoder_units_l3': 88, 'decoder_layers': 4, 'decoder_units_l1': 127, 'decoder_units_l2': 22, 'decoder_units_l3': 448, 'decoder_units_l4': 71, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 13.1983 - reconstruction_loss: 9.1124 - kl_loss: 3.9049 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.4450 - reconstruction_loss: 6.0176 - kl_loss: 4.7952 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.8561 - reconstruction_loss: 4.6715 - kl_loss: 5.2274 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.3861 - reconstruction_loss: 4.2102 - kl_loss: 5.4309 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.1352 - reconstruction_loss: 3.9522 - kl_loss: 5.5302 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.9505 - reconstruction_loss: 3.7455 - kl_loss: 5.6893 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.7572 - reconstruction_loss: 3.6069 - kl_loss: 5.7415 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.6348 - reconstruction_loss: 3.4338 - kl_loss: 5.7753 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.5195 - reconstruction_loss: 3.3230 - kl_loss: 5.8167 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.4687 - reconstruction_loss: 3.2782 - kl_loss: 5.7506 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.3776 - reconstruction_loss: 3.1807 - kl_loss: 5.7940 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.2427 - reconstruction_loss: 3.0623 - kl_loss: 5.8045 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.2030 - reconstruction_loss: 3.0136 - kl_loss: 5.8008 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.1581 - reconstruction_loss: 2.9618 - kl_loss: 5.8001 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0689 - reconstruction_loss: 2.8943 - kl_loss: 5.8062 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.0054 - reconstruction_loss: 2.8364 - kl_loss: 5.8133 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.9796 - reconstruction_loss: 2.7966 - kl_loss: 5.8048 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.9391 - reconstruction_loss: 2.8041 - kl_loss: 5.8436 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0586 - reconstruction_loss: 2.8347 - kl_loss: 5.7959 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8642 - reconstruction_loss: 2.7072 - kl_loss: 5.8054 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8844 - reconstruction_loss: 2.7054 - kl_loss: 5.8134 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8981 - reconstruction_loss: 2.6843 - kl_loss: 5.7998 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8426 - reconstruction_loss: 2.6594 - kl_loss: 5.7598 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8055 - reconstruction_loss: 2.6759 - kl_loss: 5.7647 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8029 - reconstruction_loss: 2.6641 - kl_loss: 5.7548 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.8124 - reconstruction_loss: 2.6729 - kl_loss: 5.7570\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8126 - reconstruction_loss: 2.6636 - kl_loss: 5.7605 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7780 - reconstruction_loss: 2.5830 - kl_loss: 5.8064 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7055 - reconstruction_loss: 2.5408 - kl_loss: 5.7871 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7038 - reconstruction_loss: 2.5516 - kl_loss: 5.7759 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7005 - reconstruction_loss: 2.5391 - kl_loss: 5.7564 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7260 - reconstruction_loss: 2.5383 - kl_loss: 5.7955 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7036 - reconstruction_loss: 2.5307 - kl_loss: 5.7692 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6978 - reconstruction_loss: 2.5352 - kl_loss: 5.7470 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6731 - reconstruction_loss: 2.5284 - kl_loss: 5.7618 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6713 - reconstruction_loss: 2.5181 - kl_loss: 5.7679 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6622 - reconstruction_loss: 2.4972 - kl_loss: 5.7382 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6567 - reconstruction_loss: 2.5057 - kl_loss: 5.7377 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.6941 - reconstruction_loss: 2.5359 - kl_loss: 5.7474 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.6570 - reconstruction_loss: 2.5036 - kl_loss: 5.7629\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6569 - reconstruction_loss: 2.4947 - kl_loss: 5.7631 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.6341 - reconstruction_loss: 2.4685 - kl_loss: 5.7355 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5978 - reconstruction_loss: 2.4619 - kl_loss: 5.7262 - lr: 2.5000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6175 - reconstruction_loss: 2.4682 - kl_loss: 5.7490 - lr: 2.5000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6241 - reconstruction_loss: 2.4608 - kl_loss: 5.7473 - lr: 2.5000e-04\n",
      "Epoch 44/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.6180 - reconstruction_loss: 2.4655 - kl_loss: 5.7580\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6179 - reconstruction_loss: 2.4573 - kl_loss: 5.7580 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5932 - reconstruction_loss: 2.4400 - kl_loss: 5.7464 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5951 - reconstruction_loss: 2.4364 - kl_loss: 5.7489 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5744 - reconstruction_loss: 2.4290 - kl_loss: 5.7439 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5807 - reconstruction_loss: 2.4277 - kl_loss: 5.7479 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5552 - reconstruction_loss: 2.4192 - kl_loss: 5.7452 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5849 - reconstruction_loss: 2.4169 - kl_loss: 5.7573 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5862 - reconstruction_loss: 2.4203 - kl_loss: 5.7564 - lr: 1.2500e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5722 - reconstruction_loss: 2.4144 - kl_loss: 5.7507 - lr: 1.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5670 - reconstruction_loss: 2.4186 - kl_loss: 5.7414 - lr: 1.2500e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5565 - reconstruction_loss: 2.4080 - kl_loss: 5.7392 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5744 - reconstruction_loss: 2.4140 - kl_loss: 5.7397 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5622 - reconstruction_loss: 2.4153 - kl_loss: 5.7336 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5639 - reconstruction_loss: 2.4068 - kl_loss: 5.7318 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5639 - reconstruction_loss: 2.4109 - kl_loss: 5.7347 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5544 - reconstruction_loss: 2.4047 - kl_loss: 5.7356 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5724 - reconstruction_loss: 2.4058 - kl_loss: 5.7265 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5586 - reconstruction_loss: 2.4013 - kl_loss: 5.7327 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5433 - reconstruction_loss: 2.3945 - kl_loss: 5.7385 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5457 - reconstruction_loss: 2.4059 - kl_loss: 5.7242 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5559 - reconstruction_loss: 2.4056 - kl_loss: 5.7178 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5356 - reconstruction_loss: 2.4100 - kl_loss: 5.7247\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5357 - reconstruction_loss: 2.4016 - kl_loss: 5.7249 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5458 - reconstruction_loss: 2.3907 - kl_loss: 5.7343 - lr: 6.2500e-05\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5443 - reconstruction_loss: 2.3845 - kl_loss: 5.7334 - lr: 6.2500e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5364 - reconstruction_loss: 2.3814 - kl_loss: 5.7260 - lr: 6.2500e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5409 - reconstruction_loss: 2.3766 - kl_loss: 5.7336 - lr: 6.2500e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5265 - reconstruction_loss: 2.3841 - kl_loss: 5.7181 - lr: 6.2500e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5362 - reconstruction_loss: 2.3763 - kl_loss: 5.7370 - lr: 6.2500e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5269 - reconstruction_loss: 2.3726 - kl_loss: 5.7348 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5299 - reconstruction_loss: 2.3734 - kl_loss: 5.7330 - lr: 6.2500e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5164 - reconstruction_loss: 2.3738 - kl_loss: 5.7401 - lr: 6.2500e-05\n",
      "Epoch 75/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.5365 - reconstruction_loss: 2.3843 - kl_loss: 5.7303\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5363 - reconstruction_loss: 2.3752 - kl_loss: 5.7304 - lr: 6.2500e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5167 - reconstruction_loss: 2.3655 - kl_loss: 5.7388 - lr: 3.1250e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5194 - reconstruction_loss: 2.3648 - kl_loss: 5.7498 - lr: 3.1250e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5198 - reconstruction_loss: 2.3648 - kl_loss: 5.7223 - lr: 3.1250e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5244 - reconstruction_loss: 2.3655 - kl_loss: 5.7365 - lr: 3.1250e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5148 - reconstruction_loss: 2.3606 - kl_loss: 5.7339 - lr: 3.1250e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5244 - reconstruction_loss: 2.3589 - kl_loss: 5.7436 - lr: 3.1250e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5230 - reconstruction_loss: 2.3661 - kl_loss: 5.7299 - lr: 3.1250e-05\n",
      "Epoch 83/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5174 - reconstruction_loss: 2.3723 - kl_loss: 5.7416\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5174 - reconstruction_loss: 2.3638 - kl_loss: 5.7413 - lr: 3.1250e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5324 - reconstruction_loss: 2.3500 - kl_loss: 5.7612 - lr: 1.5625e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5163 - reconstruction_loss: 2.3549 - kl_loss: 5.7309 - lr: 1.5625e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5208 - reconstruction_loss: 2.3545 - kl_loss: 5.7368 - lr: 1.5625e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5153 - reconstruction_loss: 2.3552 - kl_loss: 5.7334 - lr: 1.5625e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5112 - reconstruction_loss: 2.3525 - kl_loss: 5.7259 - lr: 1.5625e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5043 - reconstruction_loss: 2.3521 - kl_loss: 5.7516 - lr: 1.5625e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5226 - reconstruction_loss: 2.3532 - kl_loss: 5.7445 - lr: 1.5625e-05\n",
      "Epoch 91/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5029 - reconstruction_loss: 2.3610 - kl_loss: 5.7479\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5030 - reconstruction_loss: 2.3512 - kl_loss: 5.7476 - lr: 1.5625e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5118 - reconstruction_loss: 2.3502 - kl_loss: 5.7394 - lr: 7.8125e-06\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5089 - reconstruction_loss: 2.3504 - kl_loss: 5.7394 - lr: 7.8125e-06\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5011 - reconstruction_loss: 2.3493 - kl_loss: 5.7402 - lr: 7.8125e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5128 - reconstruction_loss: 2.3478 - kl_loss: 5.7471 - lr: 7.8125e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5023 - reconstruction_loss: 2.3486 - kl_loss: 5.7401 - lr: 7.8125e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4876 - reconstruction_loss: 2.3477 - kl_loss: 5.7365 - lr: 7.8125e-06\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4992 - reconstruction_loss: 2.3492 - kl_loss: 5.7385 - lr: 7.8125e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5072 - reconstruction_loss: 2.3455 - kl_loss: 5.7391 - lr: 7.8125e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5030 - reconstruction_loss: 2.3482 - kl_loss: 5.7430 - lr: 7.8125e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5088 - reconstruction_loss: 2.3480 - kl_loss: 5.7452 - lr: 7.8125e-06\n",
      "Epoch 102/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5019 - reconstruction_loss: 2.3568 - kl_loss: 5.7381\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5018 - reconstruction_loss: 2.3484 - kl_loss: 5.7382 - lr: 7.8125e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5175 - reconstruction_loss: 2.3466 - kl_loss: 5.7461 - lr: 3.9063e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5011 - reconstruction_loss: 2.3447 - kl_loss: 5.7461 - lr: 3.9063e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5022 - reconstruction_loss: 2.3443 - kl_loss: 5.7427 - lr: 3.9063e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5021 - reconstruction_loss: 2.3470 - kl_loss: 5.7415 - lr: 3.9063e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4985 - reconstruction_loss: 2.3426 - kl_loss: 5.7430 - lr: 3.9063e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5116 - reconstruction_loss: 2.3437 - kl_loss: 5.7407 - lr: 3.9063e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4990 - reconstruction_loss: 2.3470 - kl_loss: 5.7349 - lr: 3.9063e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5113 - reconstruction_loss: 2.3469 - kl_loss: 5.7360\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5112 - reconstruction_loss: 2.3469 - kl_loss: 5.7360 - lr: 3.9063e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4925 - reconstruction_loss: 2.3470 - kl_loss: 5.7366 - lr: 1.9531e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5032 - reconstruction_loss: 2.3449 - kl_loss: 5.7408 - lr: 1.9531e-06\n",
      "Epoch 113/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4976 - reconstruction_loss: 2.3524 - kl_loss: 5.7397\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4975 - reconstruction_loss: 2.3444 - kl_loss: 5.7396 - lr: 1.9531e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5066 - reconstruction_loss: 2.3448 - kl_loss: 5.7428 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4926 - reconstruction_loss: 2.3437 - kl_loss: 5.7421 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5020 - reconstruction_loss: 2.3450 - kl_loss: 5.7418 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5157 - reconstruction_loss: 2.3445 - kl_loss: 5.7410 - lr: 1.0000e-06\n",
      "Epoch 117: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 16:36:31,387]\u001b[0m Trial 85 finished with value: 0.004593129490338422 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 34, 'encoder_units_l2': 445, 'encoder_units_l3': 73, 'decoder_layers': 4, 'decoder_units_l1': 134, 'decoder_units_l2': 22, 'decoder_units_l3': 432, 'decoder_units_l4': 63, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 20.5457 - reconstruction_loss: 17.3495 - kl_loss: 0.5908 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 12.9372 - reconstruction_loss: 9.7741 - kl_loss: 2.0765 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 10.7114 - reconstruction_loss: 8.1227 - kl_loss: 2.5524 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 10.6357 - reconstruction_loss: 8.0598 - kl_loss: 2.5544 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 10.5999 - reconstruction_loss: 8.0164 - kl_loss: 2.5611 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 10.5648 - reconstruction_loss: 7.9757 - kl_loss: 2.5623 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 10.5700 - reconstruction_loss: 7.9175 - kl_loss: 2.5700 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 10.4322 - reconstruction_loss: 7.7932 - kl_loss: 2.5831 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 10.2365 - reconstruction_loss: 7.4206 - kl_loss: 2.6681 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 9.7241 - reconstruction_loss: 6.7514 - kl_loss: 2.8448 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 9.3429 - reconstruction_loss: 6.3338 - kl_loss: 2.9464 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 9.1303 - reconstruction_loss: 6.0843 - kl_loss: 3.0022 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 9.0378 - reconstruction_loss: 5.9295 - kl_loss: 3.0437 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.8893 - reconstruction_loss: 5.7925 - kl_loss: 3.0641 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 8.8459 - reconstruction_loss: 5.7171 - kl_loss: 3.1011 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 8.7529 - reconstruction_loss: 5.6263 - kl_loss: 3.1096 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 8.7370 - reconstruction_loss: 5.5741 - kl_loss: 3.1225 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 8.6764 - reconstruction_loss: 5.5217 - kl_loss: 3.1219 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.6204 - reconstruction_loss: 5.4661 - kl_loss: 3.1335 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.5866 - reconstruction_loss: 5.4237 - kl_loss: 3.1437 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 8.5573 - reconstruction_loss: 5.3867 - kl_loss: 3.1506 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.5261 - reconstruction_loss: 5.3489 - kl_loss: 3.1465 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 8.5453 - reconstruction_loss: 5.3342 - kl_loss: 3.1541 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.4753 - reconstruction_loss: 5.3015 - kl_loss: 3.1544 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.4659 - reconstruction_loss: 5.2777 - kl_loss: 3.1584 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.4413 - reconstruction_loss: 5.2486 - kl_loss: 3.1698 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.4013 - reconstruction_loss: 5.2235 - kl_loss: 3.1680 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.3720 - reconstruction_loss: 5.1990 - kl_loss: 3.1669 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.3802 - reconstruction_loss: 5.1729 - kl_loss: 3.1682 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.3460 - reconstruction_loss: 5.1471 - kl_loss: 3.1770 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 8.3165 - reconstruction_loss: 5.1317 - kl_loss: 3.1801 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.3435 - reconstruction_loss: 5.1102 - kl_loss: 3.1921 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.3209 - reconstruction_loss: 5.0930 - kl_loss: 3.1839 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.2634 - reconstruction_loss: 5.0708 - kl_loss: 3.1917 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.2904 - reconstruction_loss: 5.0616 - kl_loss: 3.2062 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.2417 - reconstruction_loss: 5.0338 - kl_loss: 3.2005 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.2687 - reconstruction_loss: 5.0216 - kl_loss: 3.2116 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.2210 - reconstruction_loss: 4.9933 - kl_loss: 3.2143 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.2377 - reconstruction_loss: 4.9876 - kl_loss: 3.2219 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.2413 - reconstruction_loss: 4.9663 - kl_loss: 3.2352 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.1970 - reconstruction_loss: 4.9520 - kl_loss: 3.2330 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.2238 - reconstruction_loss: 4.9401 - kl_loss: 3.2360 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.1781 - reconstruction_loss: 4.9134 - kl_loss: 3.2442 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.1734 - reconstruction_loss: 4.9044 - kl_loss: 3.2520 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.1740 - reconstruction_loss: 4.8935 - kl_loss: 3.2527 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.1161 - reconstruction_loss: 4.8692 - kl_loss: 3.2487 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.1316 - reconstruction_loss: 4.8513 - kl_loss: 3.2560 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.1247 - reconstruction_loss: 4.8473 - kl_loss: 3.2624 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.1378 - reconstruction_loss: 4.8293 - kl_loss: 3.2744 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.1024 - reconstruction_loss: 4.8206 - kl_loss: 3.2714 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 8.0869 - reconstruction_loss: 4.7945 - kl_loss: 3.2752 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.0959 - reconstruction_loss: 4.7801 - kl_loss: 3.2860 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.0718 - reconstruction_loss: 4.7790 - kl_loss: 3.2824 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.0646 - reconstruction_loss: 4.7589 - kl_loss: 3.2958 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.0653 - reconstruction_loss: 4.7387 - kl_loss: 3.3015 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 8.0457 - reconstruction_loss: 4.7298 - kl_loss: 3.2992 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 8.0435 - reconstruction_loss: 4.7145 - kl_loss: 3.3115 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.0466 - reconstruction_loss: 4.7001 - kl_loss: 3.3181 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.9971 - reconstruction_loss: 4.6801 - kl_loss: 3.3236 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.0304 - reconstruction_loss: 4.6737 - kl_loss: 3.3276 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.0224 - reconstruction_loss: 4.6600 - kl_loss: 3.3359 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 8.0008 - reconstruction_loss: 4.6569 - kl_loss: 3.3295 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.9655 - reconstruction_loss: 4.6261 - kl_loss: 3.3403 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.9799 - reconstruction_loss: 4.6228 - kl_loss: 3.3442 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.9702 - reconstruction_loss: 4.6133 - kl_loss: 3.3456 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.9583 - reconstruction_loss: 4.5914 - kl_loss: 3.3579 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.9669 - reconstruction_loss: 4.5760 - kl_loss: 3.3633 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.9571 - reconstruction_loss: 4.5662 - kl_loss: 3.3637 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.9544 - reconstruction_loss: 4.5631 - kl_loss: 3.3733 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.9064 - reconstruction_loss: 4.5455 - kl_loss: 3.3659 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.9251 - reconstruction_loss: 4.5293 - kl_loss: 3.3697 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.8985 - reconstruction_loss: 4.5166 - kl_loss: 3.3807 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.9194 - reconstruction_loss: 4.5079 - kl_loss: 3.3800 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.8912 - reconstruction_loss: 4.4966 - kl_loss: 3.3846 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.9189 - reconstruction_loss: 4.4862 - kl_loss: 3.3930 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.8964 - reconstruction_loss: 4.4786 - kl_loss: 3.3981 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.9018 - reconstruction_loss: 4.4707 - kl_loss: 3.3959 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.8707 - reconstruction_loss: 4.4630 - kl_loss: 3.3994 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.8781 - reconstruction_loss: 4.4408 - kl_loss: 3.4108 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 7.8541 - reconstruction_loss: 4.4315 - kl_loss: 3.4101 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.8523 - reconstruction_loss: 4.4286 - kl_loss: 3.4111 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.8393 - reconstruction_loss: 4.4158 - kl_loss: 3.4147 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.8473 - reconstruction_loss: 4.3993 - kl_loss: 3.4235 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.8256 - reconstruction_loss: 4.3894 - kl_loss: 3.4232 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.8347 - reconstruction_loss: 4.3810 - kl_loss: 3.4268 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.8309 - reconstruction_loss: 4.3690 - kl_loss: 3.4418 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.8285 - reconstruction_loss: 4.3578 - kl_loss: 3.4340 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.8230 - reconstruction_loss: 4.3427 - kl_loss: 3.4487 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.8129 - reconstruction_loss: 4.3452 - kl_loss: 3.4407 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.7775 - reconstruction_loss: 4.3390 - kl_loss: 3.4448 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 7.7905 - reconstruction_loss: 4.3189 - kl_loss: 3.4527 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 7.7874 - reconstruction_loss: 4.3153 - kl_loss: 3.4541 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.7747 - reconstruction_loss: 4.3146 - kl_loss: 3.4529 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.7717 - reconstruction_loss: 4.2950 - kl_loss: 3.4659 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.7664 - reconstruction_loss: 4.2857 - kl_loss: 3.4719 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.7420 - reconstruction_loss: 4.2744 - kl_loss: 3.4722 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.7788 - reconstruction_loss: 4.2795 - kl_loss: 3.4738 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.7347 - reconstruction_loss: 4.2555 - kl_loss: 3.4758 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.7367 - reconstruction_loss: 4.2489 - kl_loss: 3.4848 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.7072 - reconstruction_loss: 4.2372 - kl_loss: 3.4840 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.7326 - reconstruction_loss: 4.2336 - kl_loss: 3.4815 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.7253 - reconstruction_loss: 4.2265 - kl_loss: 3.4968 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.7379 - reconstruction_loss: 4.2068 - kl_loss: 3.5029 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.7252 - reconstruction_loss: 4.1981 - kl_loss: 3.5046 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.7405 - reconstruction_loss: 4.1941 - kl_loss: 3.5061 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.7143 - reconstruction_loss: 4.1879 - kl_loss: 3.5083 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.7162 - reconstruction_loss: 4.1841 - kl_loss: 3.5092 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6909 - reconstruction_loss: 4.1799 - kl_loss: 3.5096 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6899 - reconstruction_loss: 4.1632 - kl_loss: 3.5116 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.6920 - reconstruction_loss: 4.1588 - kl_loss: 3.5252 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.7105 - reconstruction_loss: 4.1492 - kl_loss: 3.5140 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.6694 - reconstruction_loss: 4.1384 - kl_loss: 3.5300 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.6704 - reconstruction_loss: 4.1367 - kl_loss: 3.5270 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.6669 - reconstruction_loss: 4.1325 - kl_loss: 3.5220 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6296 - reconstruction_loss: 4.1116 - kl_loss: 3.5263 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.6592 - reconstruction_loss: 4.1101 - kl_loss: 3.5306 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.6491 - reconstruction_loss: 4.1061 - kl_loss: 3.5410 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6584 - reconstruction_loss: 4.0902 - kl_loss: 3.5444 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6559 - reconstruction_loss: 4.0938 - kl_loss: 3.5430 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6376 - reconstruction_loss: 4.0758 - kl_loss: 3.5529 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6205 - reconstruction_loss: 4.0693 - kl_loss: 3.5524 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.6446 - reconstruction_loss: 4.0624 - kl_loss: 3.5630 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.6280 - reconstruction_loss: 4.0482 - kl_loss: 3.5593 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6433 - reconstruction_loss: 4.0492 - kl_loss: 3.5626 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6317 - reconstruction_loss: 4.0375 - kl_loss: 3.5685 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.6186 - reconstruction_loss: 4.0319 - kl_loss: 3.5736 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.6123 - reconstruction_loss: 4.0139 - kl_loss: 3.5731 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6301 - reconstruction_loss: 4.0269 - kl_loss: 3.5696 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.5866 - reconstruction_loss: 4.0067 - kl_loss: 3.5775 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.5985 - reconstruction_loss: 3.9993 - kl_loss: 3.5836 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6015 - reconstruction_loss: 3.9914 - kl_loss: 3.5941 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5969 - reconstruction_loss: 3.9894 - kl_loss: 3.5885 - lr: 1.0000e-04\n",
      "Epoch 133/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.5799 - reconstruction_loss: 3.9819 - kl_loss: 3.5879 - lr: 1.0000e-04\n",
      "Epoch 134/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.6057 - reconstruction_loss: 3.9858 - kl_loss: 3.5911 - lr: 1.0000e-04\n",
      "Epoch 135/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5567 - reconstruction_loss: 3.9625 - kl_loss: 3.5949 - lr: 1.0000e-04\n",
      "Epoch 136/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5908 - reconstruction_loss: 3.9669 - kl_loss: 3.5946 - lr: 1.0000e-04\n",
      "Epoch 137/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5702 - reconstruction_loss: 3.9531 - kl_loss: 3.6069 - lr: 1.0000e-04\n",
      "Epoch 138/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 7.5422 - reconstruction_loss: 3.9637 - kl_loss: 3.6131\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.5429 - reconstruction_loss: 3.9486 - kl_loss: 3.6125 - lr: 1.0000e-04\n",
      "Epoch 139/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5604 - reconstruction_loss: 3.9220 - kl_loss: 3.6086 - lr: 5.0000e-05\n",
      "Epoch 140/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.5201 - reconstruction_loss: 3.9178 - kl_loss: 3.6087 - lr: 5.0000e-05\n",
      "Epoch 141/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.5359 - reconstruction_loss: 3.9185 - kl_loss: 3.6086 - lr: 5.0000e-05\n",
      "Epoch 142/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.5556 - reconstruction_loss: 3.9134 - kl_loss: 3.6144 - lr: 5.0000e-05\n",
      "Epoch 143/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5253 - reconstruction_loss: 3.9085 - kl_loss: 3.6037 - lr: 5.0000e-05\n",
      "Epoch 144/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5303 - reconstruction_loss: 3.9043 - kl_loss: 3.6113 - lr: 5.0000e-05\n",
      "Epoch 145/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5366 - reconstruction_loss: 3.9033 - kl_loss: 3.6080 - lr: 5.0000e-05\n",
      "Epoch 146/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5239 - reconstruction_loss: 3.8997 - kl_loss: 3.6149 - lr: 5.0000e-05\n",
      "Epoch 147/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.5050 - reconstruction_loss: 3.8934 - kl_loss: 3.6110 - lr: 5.0000e-05\n",
      "Epoch 148/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.5351 - reconstruction_loss: 3.8927 - kl_loss: 3.6225 - lr: 5.0000e-05\n",
      "Epoch 149/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.5261 - reconstruction_loss: 3.8859 - kl_loss: 3.6260 - lr: 5.0000e-05\n",
      "Epoch 150/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4961 - reconstruction_loss: 3.8790 - kl_loss: 3.6135 - lr: 5.0000e-05\n",
      "Epoch 151/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.5243 - reconstruction_loss: 3.8797 - kl_loss: 3.6226 - lr: 5.0000e-05\n",
      "Epoch 152/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.5045 - reconstruction_loss: 3.8695 - kl_loss: 3.6262 - lr: 5.0000e-05\n",
      "Epoch 153/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 7.5273 - reconstruction_loss: 3.8859 - kl_loss: 3.6325\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5270 - reconstruction_loss: 3.8728 - kl_loss: 3.6325 - lr: 5.0000e-05\n",
      "Epoch 154/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4949 - reconstruction_loss: 3.8652 - kl_loss: 3.6270 - lr: 2.5000e-05\n",
      "Epoch 155/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.5012 - reconstruction_loss: 3.8541 - kl_loss: 3.6300 - lr: 2.5000e-05\n",
      "Epoch 156/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4991 - reconstruction_loss: 3.8537 - kl_loss: 3.6302 - lr: 2.5000e-05\n",
      "Epoch 157/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5011 - reconstruction_loss: 3.8545 - kl_loss: 3.6305 - lr: 2.5000e-05\n",
      "Epoch 158/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4853 - reconstruction_loss: 3.8526 - kl_loss: 3.6279 - lr: 2.5000e-05\n",
      "Epoch 159/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5067 - reconstruction_loss: 3.8509 - kl_loss: 3.6387 - lr: 2.5000e-05\n",
      "Epoch 160/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4925 - reconstruction_loss: 3.8513 - kl_loss: 3.6290 - lr: 2.5000e-05\n",
      "Epoch 161/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5014 - reconstruction_loss: 3.8471 - kl_loss: 3.6341 - lr: 2.5000e-05\n",
      "Epoch 162/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.5052 - reconstruction_loss: 3.8338 - kl_loss: 3.6392 - lr: 2.5000e-05\n",
      "Epoch 163/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4935 - reconstruction_loss: 3.8496 - kl_loss: 3.6399 - lr: 2.5000e-05\n",
      "Epoch 164/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4969 - reconstruction_loss: 3.8367 - kl_loss: 3.6401 - lr: 2.5000e-05\n",
      "Epoch 165/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 7.4953 - reconstruction_loss: 3.8440 - kl_loss: 3.6438\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4951 - reconstruction_loss: 3.8302 - kl_loss: 3.6440 - lr: 2.5000e-05\n",
      "Epoch 166/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4799 - reconstruction_loss: 3.8349 - kl_loss: 3.6368 - lr: 1.2500e-05\n",
      "Epoch 167/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4917 - reconstruction_loss: 3.8262 - kl_loss: 3.6423 - lr: 1.2500e-05\n",
      "Epoch 168/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4854 - reconstruction_loss: 3.8277 - kl_loss: 3.6416 - lr: 1.2500e-05\n",
      "Epoch 169/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4560 - reconstruction_loss: 3.8270 - kl_loss: 3.6368 - lr: 1.2500e-05\n",
      "Epoch 170/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4768 - reconstruction_loss: 3.8289 - kl_loss: 3.6403 - lr: 1.2500e-05\n",
      "Epoch 171/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4768 - reconstruction_loss: 3.8267 - kl_loss: 3.6442 - lr: 1.2500e-05\n",
      "Epoch 172/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 7.4728 - reconstruction_loss: 3.8235 - kl_loss: 3.6427\n",
      "Epoch 172: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4728 - reconstruction_loss: 3.8235 - kl_loss: 3.6427 - lr: 1.2500e-05\n",
      "Epoch 173/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4892 - reconstruction_loss: 3.8249 - kl_loss: 3.6469 - lr: 6.2500e-06\n",
      "Epoch 174/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4627 - reconstruction_loss: 3.8169 - kl_loss: 3.6452 - lr: 6.2500e-06\n",
      "Epoch 175/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4859 - reconstruction_loss: 3.8202 - kl_loss: 3.6440 - lr: 6.2500e-06\n",
      "Epoch 176/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4780 - reconstruction_loss: 3.8195 - kl_loss: 3.6430 - lr: 6.2500e-06\n",
      "Epoch 177/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 7.4878 - reconstruction_loss: 3.8371 - kl_loss: 3.6430\n",
      "Epoch 177: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4874 - reconstruction_loss: 3.8229 - kl_loss: 3.6432 - lr: 6.2500e-06\n",
      "Epoch 178/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4633 - reconstruction_loss: 3.8128 - kl_loss: 3.6434 - lr: 3.1250e-06\n",
      "Epoch 179/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4634 - reconstruction_loss: 3.8139 - kl_loss: 3.6421 - lr: 3.1250e-06\n",
      "Epoch 180/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4717 - reconstruction_loss: 3.8124 - kl_loss: 3.6419 - lr: 3.1250e-06\n",
      "Epoch 181/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4504 - reconstruction_loss: 3.8162 - kl_loss: 3.6385 - lr: 3.1250e-06\n",
      "Epoch 182/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4771 - reconstruction_loss: 3.8186 - kl_loss: 3.6406 - lr: 3.1250e-06\n",
      "Epoch 183/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 7.4717 - reconstruction_loss: 3.8147 - kl_loss: 3.6432\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4717 - reconstruction_loss: 3.8147 - kl_loss: 3.6432 - lr: 3.1250e-06\n",
      "Epoch 184/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4649 - reconstruction_loss: 3.8124 - kl_loss: 3.6435 - lr: 1.5625e-06\n",
      "Epoch 185/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4904 - reconstruction_loss: 3.8187 - kl_loss: 3.6440 - lr: 1.5625e-06\n",
      "Epoch 186/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 7.4665 - reconstruction_loss: 3.8109 - kl_loss: 3.6456\n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4664 - reconstruction_loss: 3.8109 - kl_loss: 3.6456 - lr: 1.5625e-06\n",
      "Epoch 187/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4664 - reconstruction_loss: 3.8111 - kl_loss: 3.6453 - lr: 1.0000e-06\n",
      "Epoch 188/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4788 - reconstruction_loss: 3.8130 - kl_loss: 3.6449 - lr: 1.0000e-06\n",
      "Epoch 189/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4713 - reconstruction_loss: 3.8136 - kl_loss: 3.6443 - lr: 1.0000e-06\n",
      "Epoch 190/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4674 - reconstruction_loss: 3.8077 - kl_loss: 3.6448 - lr: 1.0000e-06\n",
      "Epoch 191/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4592 - reconstruction_loss: 3.8150 - kl_loss: 3.6434 - lr: 1.0000e-06\n",
      "Epoch 192/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4771 - reconstruction_loss: 3.8249 - kl_loss: 3.6461 - lr: 1.0000e-06\n",
      "Epoch 193/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4936 - reconstruction_loss: 3.8202 - kl_loss: 3.6472 - lr: 1.0000e-06\n",
      "Epoch 194/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4953 - reconstruction_loss: 3.8149 - kl_loss: 3.6482 - lr: 1.0000e-06\n",
      "Epoch 195/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.4715 - reconstruction_loss: 3.8059 - kl_loss: 3.6490 - lr: 1.0000e-06\n",
      "Epoch 196/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4748 - reconstruction_loss: 3.8111 - kl_loss: 3.6478 - lr: 1.0000e-06\n",
      "Epoch 197/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4793 - reconstruction_loss: 3.8066 - kl_loss: 3.6471 - lr: 1.0000e-06\n",
      "Epoch 198/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4855 - reconstruction_loss: 3.8107 - kl_loss: 3.6461 - lr: 1.0000e-06\n",
      "Epoch 199/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 7.4762 - reconstruction_loss: 3.8137 - kl_loss: 3.6441 - lr: 1.0000e-06\n",
      "Epoch 200/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.4771 - reconstruction_loss: 3.8086 - kl_loss: 3.6452 - lr: 1.0000e-06\n",
      "Epoch 200: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 16:44:41,070]\u001b[0m Trial 86 finished with value: 0.007479913002984119 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 30, 'encoder_units_l2': 591, 'encoder_units_l3': 94, 'decoder_layers': 4, 'decoder_units_l1': 163, 'decoder_units_l2': 68, 'decoder_units_l3': 547, 'decoder_units_l4': 72, 'beta': 1, 'lr': 0.0001, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 13.5225 - reconstruction_loss: 9.1363 - kl_loss: 3.9874 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 7.9035 - reconstruction_loss: 6.8671 - kl_loss: 4.3018 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 6.3520 - reconstruction_loss: 4.9921 - kl_loss: 5.0234 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 5.4045 - reconstruction_loss: 4.2094 - kl_loss: 5.3880 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 5.0533 - reconstruction_loss: 3.9093 - kl_loss: 5.5218 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 5.0133 - reconstruction_loss: 3.7777 - kl_loss: 5.5788 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.7866 - reconstruction_loss: 3.6077 - kl_loss: 5.6585 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.6992 - reconstruction_loss: 3.5357 - kl_loss: 5.6753 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.6315 - reconstruction_loss: 3.4490 - kl_loss: 5.7144 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.5193 - reconstruction_loss: 3.3349 - kl_loss: 5.7726 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.4697 - reconstruction_loss: 3.2703 - kl_loss: 5.8044 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.4514 - reconstruction_loss: 3.2105 - kl_loss: 5.8055 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.3546 - reconstruction_loss: 3.1515 - kl_loss: 5.8412 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.2725 - reconstruction_loss: 3.0978 - kl_loss: 5.8653 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.2202 - reconstruction_loss: 3.0242 - kl_loss: 5.8669 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.1971 - reconstruction_loss: 2.9751 - kl_loss: 5.8434 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0912 - reconstruction_loss: 2.9095 - kl_loss: 5.8855 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0798 - reconstruction_loss: 2.8877 - kl_loss: 5.8741 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0252 - reconstruction_loss: 2.8501 - kl_loss: 5.8603 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0393 - reconstruction_loss: 2.8596 - kl_loss: 5.8254 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.9849 - reconstruction_loss: 2.7895 - kl_loss: 5.8726 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.9672 - reconstruction_loss: 2.7747 - kl_loss: 5.8868 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.9193 - reconstruction_loss: 2.7161 - kl_loss: 5.8756 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8695 - reconstruction_loss: 2.6951 - kl_loss: 5.8463 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8550 - reconstruction_loss: 2.6977 - kl_loss: 5.8375 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8727 - reconstruction_loss: 2.6922 - kl_loss: 5.8487 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8909 - reconstruction_loss: 2.6819 - kl_loss: 5.8469 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8468 - reconstruction_loss: 2.6719 - kl_loss: 5.8453 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8242 - reconstruction_loss: 2.6260 - kl_loss: 5.8238 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8175 - reconstruction_loss: 2.6494 - kl_loss: 5.8074 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7670 - reconstruction_loss: 2.6130 - kl_loss: 5.8452 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7816 - reconstruction_loss: 2.6030 - kl_loss: 5.8396 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7588 - reconstruction_loss: 2.5843 - kl_loss: 5.8373 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8056 - reconstruction_loss: 2.6769 - kl_loss: 5.8619 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8186 - reconstruction_loss: 2.5955 - kl_loss: 5.8391 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7206 - reconstruction_loss: 2.5570 - kl_loss: 5.8263 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7318 - reconstruction_loss: 2.5472 - kl_loss: 5.8321 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7249 - reconstruction_loss: 2.5793 - kl_loss: 5.8142 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6970 - reconstruction_loss: 2.5431 - kl_loss: 5.8153 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7683 - reconstruction_loss: 2.5672 - kl_loss: 5.8375 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7022 - reconstruction_loss: 2.5431 - kl_loss: 5.7997 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7282 - reconstruction_loss: 2.5548 - kl_loss: 5.8078 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6991 - reconstruction_loss: 2.5108 - kl_loss: 5.7928 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6922 - reconstruction_loss: 2.5285 - kl_loss: 5.8246 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6810 - reconstruction_loss: 2.5221 - kl_loss: 5.8140 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.8739 - reconstruction_loss: 2.6771 - kl_loss: 5.8684\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8736 - reconstruction_loss: 2.6685 - kl_loss: 5.8698 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7009 - reconstruction_loss: 2.4777 - kl_loss: 5.8471 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6065 - reconstruction_loss: 2.4301 - kl_loss: 5.8119 - lr: 5.0000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5622 - reconstruction_loss: 2.3866 - kl_loss: 5.8236 - lr: 5.0000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5589 - reconstruction_loss: 2.3924 - kl_loss: 5.8160 - lr: 5.0000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5464 - reconstruction_loss: 2.3826 - kl_loss: 5.8140 - lr: 5.0000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6112 - reconstruction_loss: 2.4528 - kl_loss: 5.8760 - lr: 5.0000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7130 - reconstruction_loss: 2.4724 - kl_loss: 5.8947 - lr: 5.0000e-04\n",
      "Epoch 54/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.6114 - reconstruction_loss: 2.4423 - kl_loss: 5.8771\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6113 - reconstruction_loss: 2.4338 - kl_loss: 5.8774 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5914 - reconstruction_loss: 2.3960 - kl_loss: 5.8533 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5445 - reconstruction_loss: 2.3688 - kl_loss: 5.8591 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5992 - reconstruction_loss: 2.3927 - kl_loss: 5.8716 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5463 - reconstruction_loss: 2.3689 - kl_loss: 5.8457 - lr: 2.5000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5186 - reconstruction_loss: 2.3498 - kl_loss: 5.8377 - lr: 2.5000e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5326 - reconstruction_loss: 2.3490 - kl_loss: 5.8262 - lr: 2.5000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5120 - reconstruction_loss: 2.3343 - kl_loss: 5.8084 - lr: 2.5000e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5087 - reconstruction_loss: 2.3378 - kl_loss: 5.7991 - lr: 2.5000e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4837 - reconstruction_loss: 2.3119 - kl_loss: 5.8092 - lr: 2.5000e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4837 - reconstruction_loss: 2.3115 - kl_loss: 5.8111 - lr: 2.5000e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4940 - reconstruction_loss: 2.3212 - kl_loss: 5.8180 - lr: 2.5000e-04\n",
      "Epoch 66/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4954 - reconstruction_loss: 2.3303 - kl_loss: 5.8259\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4953 - reconstruction_loss: 2.3222 - kl_loss: 5.8260 - lr: 2.5000e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4605 - reconstruction_loss: 2.3026 - kl_loss: 5.8081 - lr: 1.2500e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4648 - reconstruction_loss: 2.3006 - kl_loss: 5.8076 - lr: 1.2500e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4558 - reconstruction_loss: 2.2909 - kl_loss: 5.8025 - lr: 1.2500e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4631 - reconstruction_loss: 2.2839 - kl_loss: 5.8138 - lr: 1.2500e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4453 - reconstruction_loss: 2.2881 - kl_loss: 5.7979 - lr: 1.2500e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4686 - reconstruction_loss: 2.3050 - kl_loss: 5.8074 - lr: 1.2500e-04\n",
      "Epoch 73/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4913 - reconstruction_loss: 2.3085 - kl_loss: 5.8261\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4910 - reconstruction_loss: 2.3012 - kl_loss: 5.8261 - lr: 1.2500e-04\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4654 - reconstruction_loss: 2.2912 - kl_loss: 5.8172 - lr: 6.2500e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4437 - reconstruction_loss: 2.2770 - kl_loss: 5.8209 - lr: 6.2500e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4448 - reconstruction_loss: 2.2824 - kl_loss: 5.8136 - lr: 6.2500e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4544 - reconstruction_loss: 2.2751 - kl_loss: 5.8167 - lr: 6.2500e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4295 - reconstruction_loss: 2.2739 - kl_loss: 5.8192 - lr: 6.2500e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4400 - reconstruction_loss: 2.2692 - kl_loss: 5.8370 - lr: 6.2500e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4461 - reconstruction_loss: 2.2753 - kl_loss: 5.8118 - lr: 6.2500e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4470 - reconstruction_loss: 2.2687 - kl_loss: 5.8186 - lr: 6.2500e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4252 - reconstruction_loss: 2.2652 - kl_loss: 5.8177 - lr: 6.2500e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4464 - reconstruction_loss: 2.2708 - kl_loss: 5.8118 - lr: 6.2500e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4483 - reconstruction_loss: 2.2783 - kl_loss: 5.8130 - lr: 6.2500e-05\n",
      "Epoch 85/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4499 - reconstruction_loss: 2.2733 - kl_loss: 5.8322\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4496 - reconstruction_loss: 2.2653 - kl_loss: 5.8322 - lr: 6.2500e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4447 - reconstruction_loss: 2.2601 - kl_loss: 5.8312 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4246 - reconstruction_loss: 2.2572 - kl_loss: 5.8268 - lr: 3.1250e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4241 - reconstruction_loss: 2.2533 - kl_loss: 5.8283 - lr: 3.1250e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4148 - reconstruction_loss: 2.2574 - kl_loss: 5.8244 - lr: 3.1250e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4245 - reconstruction_loss: 2.2544 - kl_loss: 5.8222 - lr: 3.1250e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4317 - reconstruction_loss: 2.2549 - kl_loss: 5.8146 - lr: 3.1250e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4124 - reconstruction_loss: 2.2518 - kl_loss: 5.8284 - lr: 3.1250e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4204 - reconstruction_loss: 2.2541 - kl_loss: 5.8228 - lr: 3.1250e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4203 - reconstruction_loss: 2.2514 - kl_loss: 5.8285 - lr: 3.1250e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4307 - reconstruction_loss: 2.2575 - kl_loss: 5.8244 - lr: 3.1250e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4323 - reconstruction_loss: 2.2547 - kl_loss: 5.8276 - lr: 3.1250e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4179 - reconstruction_loss: 2.2469 - kl_loss: 5.8321 - lr: 3.1250e-05\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4187 - reconstruction_loss: 2.2462 - kl_loss: 5.8270 - lr: 3.1250e-05\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4149 - reconstruction_loss: 2.2468 - kl_loss: 5.8271 - lr: 3.1250e-05\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4145 - reconstruction_loss: 2.2513 - kl_loss: 5.8246 - lr: 3.1250e-05\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4199 - reconstruction_loss: 2.2442 - kl_loss: 5.8279 - lr: 3.1250e-05\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4227 - reconstruction_loss: 2.2441 - kl_loss: 5.8242 - lr: 3.1250e-05\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4199 - reconstruction_loss: 2.2372 - kl_loss: 5.8426 - lr: 3.1250e-05\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4236 - reconstruction_loss: 2.2432 - kl_loss: 5.8207 - lr: 3.1250e-05\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4278 - reconstruction_loss: 2.2424 - kl_loss: 5.8323 - lr: 3.1250e-05\n",
      "Epoch 106/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4127 - reconstruction_loss: 2.2508 - kl_loss: 5.8393\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4127 - reconstruction_loss: 2.2428 - kl_loss: 5.8388 - lr: 3.1250e-05\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4122 - reconstruction_loss: 2.2329 - kl_loss: 5.8374 - lr: 1.5625e-05\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4078 - reconstruction_loss: 2.2344 - kl_loss: 5.8200 - lr: 1.5625e-05\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4047 - reconstruction_loss: 2.2358 - kl_loss: 5.8212 - lr: 1.5625e-05\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.3950 - reconstruction_loss: 2.2345 - kl_loss: 5.8209 - lr: 1.5625e-05\n",
      "Epoch 111/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4136 - reconstruction_loss: 2.2446 - kl_loss: 5.8294\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4134 - reconstruction_loss: 2.2357 - kl_loss: 5.8298 - lr: 1.5625e-05\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4072 - reconstruction_loss: 2.2280 - kl_loss: 5.8334 - lr: 7.8125e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4096 - reconstruction_loss: 2.2324 - kl_loss: 5.8256 - lr: 7.8125e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4002 - reconstruction_loss: 2.2303 - kl_loss: 5.8189 - lr: 7.8125e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4095 - reconstruction_loss: 2.2303 - kl_loss: 5.8406 - lr: 7.8125e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.3959 - reconstruction_loss: 2.2277 - kl_loss: 5.8329 - lr: 7.8125e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4115 - reconstruction_loss: 2.2270 - kl_loss: 5.8338 - lr: 7.8125e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4035 - reconstruction_loss: 2.2309 - kl_loss: 5.8293 - lr: 7.8125e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4004 - reconstruction_loss: 2.2277 - kl_loss: 5.8264 - lr: 7.8125e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4053 - reconstruction_loss: 2.2311 - kl_loss: 5.8244 - lr: 7.8125e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4071 - reconstruction_loss: 2.2299 - kl_loss: 5.8280 - lr: 7.8125e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.3987 - reconstruction_loss: 2.2267 - kl_loss: 5.8227 - lr: 7.8125e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.3880 - reconstruction_loss: 2.2260 - kl_loss: 5.8210 - lr: 7.8125e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4004 - reconstruction_loss: 2.2271 - kl_loss: 5.8260 - lr: 7.8125e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4173 - reconstruction_loss: 2.2276 - kl_loss: 5.8342 - lr: 7.8125e-06\n",
      "Epoch 126/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4024 - reconstruction_loss: 2.2360 - kl_loss: 5.8286\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4023 - reconstruction_loss: 2.2280 - kl_loss: 5.8286 - lr: 7.8125e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4086 - reconstruction_loss: 2.2255 - kl_loss: 5.8290 - lr: 3.9063e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4045 - reconstruction_loss: 2.2261 - kl_loss: 5.8274 - lr: 3.9063e-06\n",
      "Epoch 129/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.3986 - reconstruction_loss: 2.2364 - kl_loss: 5.8252\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.3986 - reconstruction_loss: 2.2278 - kl_loss: 5.8247 - lr: 3.9063e-06\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4199 - reconstruction_loss: 2.2260 - kl_loss: 5.8355 - lr: 1.9531e-06\n",
      "Epoch 131/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4004 - reconstruction_loss: 2.2264 - kl_loss: 5.8365 - lr: 1.9531e-06\n",
      "Epoch 132/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.3976 - reconstruction_loss: 2.2326 - kl_loss: 5.8337\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.3975 - reconstruction_loss: 2.2246 - kl_loss: 5.8338 - lr: 1.9531e-06\n",
      "Epoch 133/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4021 - reconstruction_loss: 2.2229 - kl_loss: 5.8330 - lr: 1.0000e-06\n",
      "Epoch 134/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.3974 - reconstruction_loss: 2.2259 - kl_loss: 5.8328 - lr: 1.0000e-06\n",
      "Epoch 135/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4038 - reconstruction_loss: 2.2257 - kl_loss: 5.8333 - lr: 1.0000e-06\n",
      "Epoch 136/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4001 - reconstruction_loss: 2.2254 - kl_loss: 5.8340 - lr: 1.0000e-06\n",
      "Epoch 137/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.3900 - reconstruction_loss: 2.2255 - kl_loss: 5.8343 - lr: 1.0000e-06\n",
      "Epoch 138/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4051 - reconstruction_loss: 2.2237 - kl_loss: 5.8319 - lr: 1.0000e-06\n",
      "Epoch 139/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4005 - reconstruction_loss: 2.2232 - kl_loss: 5.8309 - lr: 1.0000e-06\n",
      "Epoch 140/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.3933 - reconstruction_loss: 2.2250 - kl_loss: 5.8304 - lr: 1.0000e-06\n",
      "Epoch 141/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.3944 - reconstruction_loss: 2.2251 - kl_loss: 5.8303 - lr: 1.0000e-06\n",
      "Epoch 142/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4060 - reconstruction_loss: 2.2251 - kl_loss: 5.8310 - lr: 1.0000e-06\n",
      "Epoch 143/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.3898 - reconstruction_loss: 2.2248 - kl_loss: 5.8323 - lr: 1.0000e-06\n",
      "Epoch 144/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.3915 - reconstruction_loss: 2.2230 - kl_loss: 5.8304 - lr: 1.0000e-06\n",
      "Epoch 145/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.3960 - reconstruction_loss: 2.2241 - kl_loss: 5.8284 - lr: 1.0000e-06\n",
      "Epoch 146/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4082 - reconstruction_loss: 2.2252 - kl_loss: 5.8298 - lr: 1.0000e-06\n",
      "Epoch 147/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.3981 - reconstruction_loss: 2.2250 - kl_loss: 5.8320 - lr: 1.0000e-06\n",
      "Epoch 148/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.3933 - reconstruction_loss: 2.2227 - kl_loss: 5.8302 - lr: 1.0000e-06\n",
      "Epoch 149/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.3896 - reconstruction_loss: 2.2243 - kl_loss: 5.8264 - lr: 1.0000e-06\n",
      "Epoch 150/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4168 - reconstruction_loss: 2.2258 - kl_loss: 5.8269 - lr: 1.0000e-06\n",
      "Epoch 151/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4009 - reconstruction_loss: 2.2224 - kl_loss: 5.8263 - lr: 1.0000e-06\n",
      "Epoch 152/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4017 - reconstruction_loss: 2.2246 - kl_loss: 5.8268 - lr: 1.0000e-06\n",
      "Epoch 153/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.3989 - reconstruction_loss: 2.2244 - kl_loss: 5.8289 - lr: 1.0000e-06\n",
      "Epoch 154/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4007 - reconstruction_loss: 2.2257 - kl_loss: 5.8292 - lr: 1.0000e-06\n",
      "Epoch 155/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4024 - reconstruction_loss: 2.2236 - kl_loss: 5.8310 - lr: 1.0000e-06\n",
      "Epoch 156/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.3943 - reconstruction_loss: 2.2227 - kl_loss: 5.8302 - lr: 1.0000e-06\n",
      "Epoch 157/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.3960 - reconstruction_loss: 2.2226 - kl_loss: 5.8292 - lr: 1.0000e-06\n",
      "Epoch 158/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.3957 - reconstruction_loss: 2.2244 - kl_loss: 5.8304 - lr: 1.0000e-06\n",
      "Epoch 159/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.3886 - reconstruction_loss: 2.2252 - kl_loss: 5.8301 - lr: 1.0000e-06\n",
      "Epoch 160/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.3990 - reconstruction_loss: 2.2239 - kl_loss: 5.8326 - lr: 1.0000e-06\n",
      "Epoch 161/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4092 - reconstruction_loss: 2.2246 - kl_loss: 5.8321 - lr: 1.0000e-06\n",
      "Epoch 161: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 16:51:21,247]\u001b[0m Trial 87 finished with value: 0.004354063317045315 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 19, 'encoder_units_l2': 534, 'encoder_units_l3': 61, 'decoder_layers': 5, 'decoder_units_l1': 100, 'decoder_units_l2': 31, 'decoder_units_l3': 892, 'decoder_units_l4': 42, 'decoder_units_l5': 442, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 20.7004 - reconstruction_loss: 16.9358 - kl_loss: 0.4471 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.9840 - reconstruction_loss: 11.8722 - kl_loss: 1.0045 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.8594 - reconstruction_loss: 11.7836 - kl_loss: 1.0138 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.8507 - reconstruction_loss: 11.7482 - kl_loss: 1.0182 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.7993 - reconstruction_loss: 11.7336 - kl_loss: 1.0163 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.8231 - reconstruction_loss: 11.7512 - kl_loss: 1.0121 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.8171 - reconstruction_loss: 11.7186 - kl_loss: 1.0191 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.9225 - reconstruction_loss: 11.7122 - kl_loss: 1.0228 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.8029 - reconstruction_loss: 11.7054 - kl_loss: 1.0203 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.8045 - reconstruction_loss: 11.7032 - kl_loss: 1.0238 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.8324 - reconstruction_loss: 11.7040 - kl_loss: 1.0210 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 16.8505 - reconstruction_loss: 11.6677 - kl_loss: 1.0253 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.8676 - reconstruction_loss: 11.6654 - kl_loss: 1.0272 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.8200 - reconstruction_loss: 11.6257 - kl_loss: 1.0281 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.8497 - reconstruction_loss: 11.5258 - kl_loss: 1.0437 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.7511 - reconstruction_loss: 11.0775 - kl_loss: 1.1031 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.5594 - reconstruction_loss: 10.6111 - kl_loss: 1.1786 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.5171 - reconstruction_loss: 10.3296 - kl_loss: 1.2223 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.3999 - reconstruction_loss: 10.1625 - kl_loss: 1.2409 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.4393 - reconstruction_loss: 10.1355 - kl_loss: 1.2499 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.4086 - reconstruction_loss: 10.0366 - kl_loss: 1.2694 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 16.4222 - reconstruction_loss: 10.0573 - kl_loss: 1.2719\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.4217 - reconstruction_loss: 10.0243 - kl_loss: 1.2713 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.3549 - reconstruction_loss: 9.9415 - kl_loss: 1.2704 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.3662 - reconstruction_loss: 9.9195 - kl_loss: 1.2798 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.3186 - reconstruction_loss: 9.8924 - kl_loss: 1.2808 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 16.3587 - reconstruction_loss: 9.9187 - kl_loss: 1.2889\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.3585 - reconstruction_loss: 9.8868 - kl_loss: 1.2889 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.3151 - reconstruction_loss: 9.8537 - kl_loss: 1.2853 - lr: 2.5000e-04\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.3435 - reconstruction_loss: 9.8261 - kl_loss: 1.2902 - lr: 2.5000e-04\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2974 - reconstruction_loss: 9.8403 - kl_loss: 1.2894 - lr: 2.5000e-04\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2888 - reconstruction_loss: 9.8244 - kl_loss: 1.2840 - lr: 2.5000e-04\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2968 - reconstruction_loss: 9.8093 - kl_loss: 1.2927 - lr: 2.5000e-04\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.3297 - reconstruction_loss: 9.8313 - kl_loss: 1.2925 - lr: 2.5000e-04\n",
      "Epoch 33/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 16.3118 - reconstruction_loss: 9.8291 - kl_loss: 1.2922\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.3109 - reconstruction_loss: 9.7921 - kl_loss: 1.2925 - lr: 2.5000e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2538 - reconstruction_loss: 9.8066 - kl_loss: 1.2884 - lr: 1.2500e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2857 - reconstruction_loss: 9.7734 - kl_loss: 1.2942 - lr: 1.2500e-04\n",
      "Epoch 36/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 16.2811 - reconstruction_loss: 9.8079 - kl_loss: 1.2964\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2811 - reconstruction_loss: 9.7762 - kl_loss: 1.2969 - lr: 1.2500e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2798 - reconstruction_loss: 9.7684 - kl_loss: 1.2970 - lr: 6.2500e-05\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2568 - reconstruction_loss: 9.7445 - kl_loss: 1.2989 - lr: 6.2500e-05\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2506 - reconstruction_loss: 9.7551 - kl_loss: 1.2942 - lr: 6.2500e-05\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2695 - reconstruction_loss: 9.7495 - kl_loss: 1.2921 - lr: 6.2500e-05\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2859 - reconstruction_loss: 9.7646 - kl_loss: 1.2993 - lr: 6.2500e-05\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.3040 - reconstruction_loss: 9.7513 - kl_loss: 1.3043 - lr: 6.2500e-05\n",
      "Epoch 43/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 16.2500 - reconstruction_loss: 9.7716 - kl_loss: 1.2991\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2500 - reconstruction_loss: 9.7349 - kl_loss: 1.2988 - lr: 6.2500e-05\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.3015 - reconstruction_loss: 9.7729 - kl_loss: 1.3005 - lr: 3.1250e-05\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2697 - reconstruction_loss: 9.7439 - kl_loss: 1.3025 - lr: 3.1250e-05\n",
      "Epoch 46/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 16.2238 - reconstruction_loss: 9.7682 - kl_loss: 1.2964\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2240 - reconstruction_loss: 9.7337 - kl_loss: 1.2963 - lr: 3.1250e-05\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2933 - reconstruction_loss: 9.7505 - kl_loss: 1.2979 - lr: 1.5625e-05\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2451 - reconstruction_loss: 9.7306 - kl_loss: 1.3013 - lr: 1.5625e-05\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2334 - reconstruction_loss: 9.7311 - kl_loss: 1.2947 - lr: 1.5625e-05\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.3015 - reconstruction_loss: 9.7518 - kl_loss: 1.3006 - lr: 1.5625e-05\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2665 - reconstruction_loss: 9.7471 - kl_loss: 1.3025 - lr: 1.5625e-05\n",
      "Epoch 52/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 16.2966 - reconstruction_loss: 9.7616 - kl_loss: 1.3021\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2957 - reconstruction_loss: 9.7231 - kl_loss: 1.3023 - lr: 1.5625e-05\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2734 - reconstruction_loss: 9.7464 - kl_loss: 1.2988 - lr: 7.8125e-06\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.3354 - reconstruction_loss: 9.7319 - kl_loss: 1.3037 - lr: 7.8125e-06\n",
      "Epoch 55/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 16.2458 - reconstruction_loss: 9.7524 - kl_loss: 1.3008\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2457 - reconstruction_loss: 9.7171 - kl_loss: 1.3003 - lr: 7.8125e-06\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2922 - reconstruction_loss: 9.7517 - kl_loss: 1.2986 - lr: 3.9063e-06\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2628 - reconstruction_loss: 9.7528 - kl_loss: 1.2998 - lr: 3.9063e-06\n",
      "Epoch 58/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 16.2764 - reconstruction_loss: 9.7701 - kl_loss: 1.3020\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2761 - reconstruction_loss: 9.7346 - kl_loss: 1.3018 - lr: 3.9063e-06\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 16.2782 - reconstruction_loss: 9.7141 - kl_loss: 1.3031 - lr: 1.9531e-06\n",
      "Epoch 59: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 16:53:07,231]\u001b[0m Trial 88 finished with value: 0.01903185321085519 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 16, 'encoder_units_l2': 540, 'encoder_units_l3': 65, 'decoder_layers': 5, 'decoder_units_l1': 101, 'decoder_units_l2': 16, 'decoder_units_l3': 482, 'decoder_units_l4': 43, 'decoder_units_l5': 30, 'beta': 5, 'lr': 0.001, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 12.8362 - reconstruction_loss: 8.8331 - kl_loss: 3.9590 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 7.8557 - reconstruction_loss: 6.7701 - kl_loss: 4.3368 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 6.4534 - reconstruction_loss: 5.1152 - kl_loss: 5.1350 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 5.7212 - reconstruction_loss: 4.4889 - kl_loss: 5.3679 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 5.3174 - reconstruction_loss: 4.1591 - kl_loss: 5.5108 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 5.1548 - reconstruction_loss: 3.9456 - kl_loss: 5.6074 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.9532 - reconstruction_loss: 3.7895 - kl_loss: 5.6619 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.8076 - reconstruction_loss: 3.6497 - kl_loss: 5.6600 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.6910 - reconstruction_loss: 3.5207 - kl_loss: 5.6918 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.6374 - reconstruction_loss: 3.4136 - kl_loss: 5.7345 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.5189 - reconstruction_loss: 3.3536 - kl_loss: 5.7274 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.4658 - reconstruction_loss: 3.2684 - kl_loss: 5.7149 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.3730 - reconstruction_loss: 3.1930 - kl_loss: 5.7534 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.2827 - reconstruction_loss: 3.1457 - kl_loss: 5.7504 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.2565 - reconstruction_loss: 3.1129 - kl_loss: 5.7604 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.2183 - reconstruction_loss: 3.0569 - kl_loss: 5.7849 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.1873 - reconstruction_loss: 3.0160 - kl_loss: 5.7558 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.1379 - reconstruction_loss: 2.9340 - kl_loss: 5.8112 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 4.0942 - reconstruction_loss: 2.9215 - kl_loss: 5.8304 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.0905 - reconstruction_loss: 2.9247 - kl_loss: 5.7833 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.0619 - reconstruction_loss: 2.8599 - kl_loss: 5.7654 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.0611 - reconstruction_loss: 2.8871 - kl_loss: 5.7892 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.0379 - reconstruction_loss: 2.8714 - kl_loss: 5.7850 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.9877 - reconstruction_loss: 2.8278 - kl_loss: 5.7806 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.9642 - reconstruction_loss: 2.7753 - kl_loss: 5.8046 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.9430 - reconstruction_loss: 2.7807 - kl_loss: 5.7855 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.8926 - reconstruction_loss: 2.7456 - kl_loss: 5.7898 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.9520 - reconstruction_loss: 2.7940 - kl_loss: 5.8211 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.9323 - reconstruction_loss: 2.7418 - kl_loss: 5.8122 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.8826 - reconstruction_loss: 2.6808 - kl_loss: 5.7894 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.8398 - reconstruction_loss: 2.6768 - kl_loss: 5.8036 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.9202 - reconstruction_loss: 2.6796 - kl_loss: 5.8453 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.8094 - reconstruction_loss: 2.6472 - kl_loss: 5.8158 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7921 - reconstruction_loss: 2.6331 - kl_loss: 5.8074 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.8150 - reconstruction_loss: 2.6472 - kl_loss: 5.8004 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.7914 - reconstruction_loss: 2.6571 - kl_loss: 5.7972 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.8593 - reconstruction_loss: 2.6738 - kl_loss: 5.7916\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.8589 - reconstruction_loss: 2.6648 - kl_loss: 5.7918 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7057 - reconstruction_loss: 2.5384 - kl_loss: 5.7857 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7089 - reconstruction_loss: 2.5416 - kl_loss: 5.8126 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.7356 - reconstruction_loss: 2.5579 - kl_loss: 5.8026 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7135 - reconstruction_loss: 2.5366 - kl_loss: 5.7777 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7051 - reconstruction_loss: 2.5259 - kl_loss: 5.7748 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6658 - reconstruction_loss: 2.5042 - kl_loss: 5.7740 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6695 - reconstruction_loss: 2.4988 - kl_loss: 5.8013 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.7118 - reconstruction_loss: 2.5267 - kl_loss: 5.7992 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.6783 - reconstruction_loss: 2.5159 - kl_loss: 5.7995\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6782 - reconstruction_loss: 2.5073 - kl_loss: 5.7993 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6394 - reconstruction_loss: 2.4675 - kl_loss: 5.7911 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6105 - reconstruction_loss: 2.4571 - kl_loss: 5.7768 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6316 - reconstruction_loss: 2.4661 - kl_loss: 5.7991 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6247 - reconstruction_loss: 2.4612 - kl_loss: 5.8020 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6232 - reconstruction_loss: 2.4486 - kl_loss: 5.7856 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6358 - reconstruction_loss: 2.4766 - kl_loss: 5.7836 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6245 - reconstruction_loss: 2.4520 - kl_loss: 5.7962 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.6411 - reconstruction_loss: 2.4723 - kl_loss: 5.8209\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.6409 - reconstruction_loss: 2.4622 - kl_loss: 5.8219 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6059 - reconstruction_loss: 2.4316 - kl_loss: 5.8012 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5897 - reconstruction_loss: 2.4175 - kl_loss: 5.8169 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6019 - reconstruction_loss: 2.4175 - kl_loss: 5.7874 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5875 - reconstruction_loss: 2.4102 - kl_loss: 5.8010 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5773 - reconstruction_loss: 2.4083 - kl_loss: 5.8063 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5593 - reconstruction_loss: 2.4150 - kl_loss: 5.7698 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5862 - reconstruction_loss: 2.4050 - kl_loss: 5.7828 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5659 - reconstruction_loss: 2.3966 - kl_loss: 5.8143 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5491 - reconstruction_loss: 2.3917 - kl_loss: 5.7995 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5627 - reconstruction_loss: 2.3923 - kl_loss: 5.8067 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5513 - reconstruction_loss: 2.3892 - kl_loss: 5.8120 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.5668 - reconstruction_loss: 2.4040 - kl_loss: 5.8159\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5667 - reconstruction_loss: 2.3945 - kl_loss: 5.8154 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5588 - reconstruction_loss: 2.3788 - kl_loss: 5.7980 - lr: 6.2500e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5402 - reconstruction_loss: 2.3746 - kl_loss: 5.7913 - lr: 6.2500e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5500 - reconstruction_loss: 2.3735 - kl_loss: 5.8170 - lr: 6.2500e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5471 - reconstruction_loss: 2.3706 - kl_loss: 5.8044 - lr: 6.2500e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5407 - reconstruction_loss: 2.3648 - kl_loss: 5.8310 - lr: 6.2500e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5394 - reconstruction_loss: 2.3681 - kl_loss: 5.8171 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5320 - reconstruction_loss: 2.3629 - kl_loss: 5.8268 - lr: 6.2500e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.5329 - reconstruction_loss: 2.3642 - kl_loss: 5.8043 - lr: 6.2500e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5353 - reconstruction_loss: 2.3669 - kl_loss: 5.8123 - lr: 6.2500e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5320 - reconstruction_loss: 2.3641 - kl_loss: 5.8176 - lr: 6.2500e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5189 - reconstruction_loss: 2.3587 - kl_loss: 5.8220 - lr: 6.2500e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5395 - reconstruction_loss: 2.3588 - kl_loss: 5.8331 - lr: 6.2500e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5303 - reconstruction_loss: 2.3601 - kl_loss: 5.8196 - lr: 6.2500e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5309 - reconstruction_loss: 2.3574 - kl_loss: 5.8187 - lr: 6.2500e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5408 - reconstruction_loss: 2.3585 - kl_loss: 5.8224 - lr: 6.2500e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5223 - reconstruction_loss: 2.3548 - kl_loss: 5.8357 - lr: 6.2500e-05\n",
      "Epoch 83/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5426 - reconstruction_loss: 2.3757 - kl_loss: 5.8395\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5425 - reconstruction_loss: 2.3669 - kl_loss: 5.8396 - lr: 6.2500e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5277 - reconstruction_loss: 2.3578 - kl_loss: 5.8181 - lr: 3.1250e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5220 - reconstruction_loss: 2.3522 - kl_loss: 5.8357 - lr: 3.1250e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5197 - reconstruction_loss: 2.3551 - kl_loss: 5.8212 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5288 - reconstruction_loss: 2.3467 - kl_loss: 5.8403 - lr: 3.1250e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5182 - reconstruction_loss: 2.3446 - kl_loss: 5.8323 - lr: 3.1250e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5158 - reconstruction_loss: 2.3385 - kl_loss: 5.8337 - lr: 3.1250e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5219 - reconstruction_loss: 2.3456 - kl_loss: 5.8214 - lr: 3.1250e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5158 - reconstruction_loss: 2.3421 - kl_loss: 5.8180 - lr: 3.1250e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5145 - reconstruction_loss: 2.3372 - kl_loss: 5.8336 - lr: 3.1250e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5075 - reconstruction_loss: 2.3358 - kl_loss: 5.8420 - lr: 3.1250e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5079 - reconstruction_loss: 2.3330 - kl_loss: 5.8381 - lr: 3.1250e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5050 - reconstruction_loss: 2.3374 - kl_loss: 5.8225 - lr: 3.1250e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4868 - reconstruction_loss: 2.3335 - kl_loss: 5.8495 - lr: 3.1250e-05\n",
      "Epoch 97/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.5213 - reconstruction_loss: 2.3386 - kl_loss: 5.8789\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5211 - reconstruction_loss: 2.3300 - kl_loss: 5.8787 - lr: 3.1250e-05\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5036 - reconstruction_loss: 2.3318 - kl_loss: 5.8468 - lr: 1.5625e-05\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5127 - reconstruction_loss: 2.3310 - kl_loss: 5.8422 - lr: 1.5625e-05\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5045 - reconstruction_loss: 2.3287 - kl_loss: 5.8445 - lr: 1.5625e-05\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5110 - reconstruction_loss: 2.3261 - kl_loss: 5.8513 - lr: 1.5625e-05\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4850 - reconstruction_loss: 2.3284 - kl_loss: 5.8485 - lr: 1.5625e-05\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5008 - reconstruction_loss: 2.3250 - kl_loss: 5.8359 - lr: 1.5625e-05\n",
      "Epoch 104/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4904 - reconstruction_loss: 2.3270 - kl_loss: 5.8248 - lr: 1.5625e-05\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5151 - reconstruction_loss: 2.3333 - kl_loss: 5.8147 - lr: 1.5625e-05\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4959 - reconstruction_loss: 2.3255 - kl_loss: 5.8477 - lr: 1.5625e-05\n",
      "Epoch 107/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5029 - reconstruction_loss: 2.3328 - kl_loss: 5.8496\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5028 - reconstruction_loss: 2.3245 - kl_loss: 5.8489 - lr: 1.5625e-05\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4913 - reconstruction_loss: 2.3227 - kl_loss: 5.8467 - lr: 7.8125e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4985 - reconstruction_loss: 2.3243 - kl_loss: 5.8442 - lr: 7.8125e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5063 - reconstruction_loss: 2.3258 - kl_loss: 5.8346\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5062 - reconstruction_loss: 2.3258 - kl_loss: 5.8346 - lr: 7.8125e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4835 - reconstruction_loss: 2.3250 - kl_loss: 5.8200 - lr: 3.9063e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5100 - reconstruction_loss: 2.3251 - kl_loss: 5.8343 - lr: 3.9063e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5117 - reconstruction_loss: 2.3206 - kl_loss: 5.8370 - lr: 3.9063e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5112 - reconstruction_loss: 2.3238 - kl_loss: 5.8351 - lr: 3.9063e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4987 - reconstruction_loss: 2.3227 - kl_loss: 5.8402 - lr: 3.9063e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4976 - reconstruction_loss: 2.3213 - kl_loss: 5.8318 - lr: 3.9063e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4991 - reconstruction_loss: 2.3222 - kl_loss: 5.8372 - lr: 3.9063e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4964 - reconstruction_loss: 2.3242 - kl_loss: 5.8339 - lr: 3.9063e-06\n",
      "Epoch 119/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.5003 - reconstruction_loss: 2.3316 - kl_loss: 5.8423\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5001 - reconstruction_loss: 2.3218 - kl_loss: 5.8419 - lr: 3.9063e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5084 - reconstruction_loss: 2.3213 - kl_loss: 5.8516 - lr: 1.9531e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4897 - reconstruction_loss: 2.3170 - kl_loss: 5.8543 - lr: 1.9531e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4963 - reconstruction_loss: 2.3222 - kl_loss: 5.8542\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4963 - reconstruction_loss: 2.3222 - kl_loss: 5.8542 - lr: 1.9531e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4969 - reconstruction_loss: 2.3191 - kl_loss: 5.8565 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4904 - reconstruction_loss: 2.3172 - kl_loss: 5.8562 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4956 - reconstruction_loss: 2.3150 - kl_loss: 5.8536 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4949 - reconstruction_loss: 2.3182 - kl_loss: 5.8528 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5026 - reconstruction_loss: 2.3190 - kl_loss: 5.8528 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4947 - reconstruction_loss: 2.3183 - kl_loss: 5.8526 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4876 - reconstruction_loss: 2.3181 - kl_loss: 5.8523 - lr: 1.0000e-06\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4930 - reconstruction_loss: 2.3157 - kl_loss: 5.8483 - lr: 1.0000e-06\n",
      "Epoch 131/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5002 - reconstruction_loss: 2.3210 - kl_loss: 5.8469 - lr: 1.0000e-06\n",
      "Epoch 132/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4868 - reconstruction_loss: 2.3185 - kl_loss: 5.8465 - lr: 1.0000e-06\n",
      "Epoch 133/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4936 - reconstruction_loss: 2.3197 - kl_loss: 5.8459 - lr: 1.0000e-06\n",
      "Epoch 134/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4898 - reconstruction_loss: 2.3180 - kl_loss: 5.8459 - lr: 1.0000e-06\n",
      "Epoch 135/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4968 - reconstruction_loss: 2.3187 - kl_loss: 5.8452 - lr: 1.0000e-06\n",
      "Epoch 136/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4988 - reconstruction_loss: 2.3178 - kl_loss: 5.8449 - lr: 1.0000e-06\n",
      "Epoch 137/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4994 - reconstruction_loss: 2.3205 - kl_loss: 5.8456 - lr: 1.0000e-06\n",
      "Epoch 138/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5081 - reconstruction_loss: 2.3200 - kl_loss: 5.8481 - lr: 1.0000e-06\n",
      "Epoch 139/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5087 - reconstruction_loss: 2.3193 - kl_loss: 5.8475 - lr: 1.0000e-06\n",
      "Epoch 140/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5017 - reconstruction_loss: 2.3193 - kl_loss: 5.8471 - lr: 1.0000e-06\n",
      "Epoch 140: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 16:57:14,330]\u001b[0m Trial 89 finished with value: 0.00454067322791208 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 19, 'encoder_units_l2': 648, 'encoder_units_l3': 92, 'decoder_layers': 5, 'decoder_units_l1': 125, 'decoder_units_l2': 32, 'decoder_units_l3': 727, 'decoder_units_l4': 41, 'decoder_units_l5': 582, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 14.0258 - reconstruction_loss: 9.6559 - kl_loss: 3.6542 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 6.8231 - reconstruction_loss: 5.1574 - kl_loss: 4.7445 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 5.2461 - reconstruction_loss: 4.1855 - kl_loss: 5.0425 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 5.1065 - reconstruction_loss: 4.0243 - kl_loss: 5.1899 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.9950 - reconstruction_loss: 3.8582 - kl_loss: 5.4449 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.9082 - reconstruction_loss: 3.7956 - kl_loss: 5.5448 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.6790 - reconstruction_loss: 3.5594 - kl_loss: 5.7156 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.3463 - reconstruction_loss: 3.3478 - kl_loss: 5.7421 - lr: 0.0100\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.5131 - reconstruction_loss: 3.1383 - kl_loss: 5.7578 - lr: 0.0100\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.3153 - reconstruction_loss: 3.2747 - kl_loss: 5.8164 - lr: 0.0100\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.6162 - reconstruction_loss: 3.3514 - kl_loss: 5.8560 - lr: 0.0100\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.4359 - reconstruction_loss: 3.0203 - kl_loss: 5.8374 - lr: 0.0100\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.1977 - reconstruction_loss: 2.8504 - kl_loss: 5.8463 - lr: 0.0100\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.8786 - reconstruction_loss: 2.7594 - kl_loss: 5.7930 - lr: 0.0100\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.9790 - reconstruction_loss: 3.5320 - kl_loss: 5.7919 - lr: 0.0100\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 5.6899 - reconstruction_loss: 3.8958 - kl_loss: 5.8855 - lr: 0.0100\n",
      "Epoch 17/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 4.7851 - reconstruction_loss: 3.3885 - kl_loss: 5.9884\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 4.7809 - reconstruction_loss: 3.3772 - kl_loss: 5.9857 - lr: 0.0100\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.9376 - reconstruction_loss: 2.6203 - kl_loss: 5.8344 - lr: 0.0050\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.6952 - reconstruction_loss: 2.5156 - kl_loss: 5.8536 - lr: 0.0050\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6425 - reconstruction_loss: 2.4791 - kl_loss: 5.7260 - lr: 0.0050\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6197 - reconstruction_loss: 2.4721 - kl_loss: 5.7165 - lr: 0.0050\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.8446 - reconstruction_loss: 2.6778 - kl_loss: 5.7556 - lr: 0.0050\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6096 - reconstruction_loss: 2.4694 - kl_loss: 5.7163 - lr: 0.0050\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.6121 - reconstruction_loss: 2.4662 - kl_loss: 5.7156 - lr: 0.0050\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5736 - reconstruction_loss: 2.4277 - kl_loss: 5.6658 - lr: 0.0050\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5398 - reconstruction_loss: 2.4138 - kl_loss: 5.6367 - lr: 0.0050\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5797 - reconstruction_loss: 2.4509 - kl_loss: 5.6397 - lr: 0.0050\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5550 - reconstruction_loss: 2.4019 - kl_loss: 5.6299 - lr: 0.0050\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5471 - reconstruction_loss: 2.4039 - kl_loss: 5.6612 - lr: 0.0050\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5570 - reconstruction_loss: 2.4187 - kl_loss: 5.6089 - lr: 0.0050\n",
      "Epoch 31/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5679 - reconstruction_loss: 2.4225 - kl_loss: 5.6505\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5675 - reconstruction_loss: 2.4141 - kl_loss: 5.6517 - lr: 0.0050\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5075 - reconstruction_loss: 2.3646 - kl_loss: 5.5908 - lr: 0.0025\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4999 - reconstruction_loss: 2.3712 - kl_loss: 5.5908 - lr: 0.0025\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5145 - reconstruction_loss: 2.3725 - kl_loss: 5.6098 - lr: 0.0025\n",
      "Epoch 35/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5593 - reconstruction_loss: 2.4272 - kl_loss: 5.6151\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5590 - reconstruction_loss: 2.4195 - kl_loss: 5.6171 - lr: 0.0025\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.5882 - reconstruction_loss: 2.3923 - kl_loss: 5.6078 - lr: 0.0012\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4827 - reconstruction_loss: 2.3430 - kl_loss: 5.5671 - lr: 0.0012\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4454 - reconstruction_loss: 2.3312 - kl_loss: 5.5627 - lr: 0.0012\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4591 - reconstruction_loss: 2.3392 - kl_loss: 5.5651 - lr: 0.0012\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4332 - reconstruction_loss: 2.3328 - kl_loss: 5.5421 - lr: 0.0012\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4586 - reconstruction_loss: 2.3348 - kl_loss: 5.5427 - lr: 0.0012\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4979 - reconstruction_loss: 2.4095 - kl_loss: 5.5957 - lr: 0.0012\n",
      "Epoch 43/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4823 - reconstruction_loss: 2.3657 - kl_loss: 5.5578\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4822 - reconstruction_loss: 2.3575 - kl_loss: 5.5580 - lr: 0.0012\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4617 - reconstruction_loss: 2.3349 - kl_loss: 5.5580 - lr: 6.2500e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4416 - reconstruction_loss: 2.3250 - kl_loss: 5.5613 - lr: 6.2500e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4453 - reconstruction_loss: 2.3238 - kl_loss: 5.5503 - lr: 6.2500e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4281 - reconstruction_loss: 2.3197 - kl_loss: 5.5367 - lr: 6.2500e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4347 - reconstruction_loss: 2.3142 - kl_loss: 5.5319 - lr: 6.2500e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4358 - reconstruction_loss: 2.3137 - kl_loss: 5.5396 - lr: 6.2500e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4336 - reconstruction_loss: 2.3201 - kl_loss: 5.5209 - lr: 6.2500e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4362 - reconstruction_loss: 2.3102 - kl_loss: 5.5319 - lr: 6.2500e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4195 - reconstruction_loss: 2.3112 - kl_loss: 5.5207 - lr: 6.2500e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4144 - reconstruction_loss: 2.3106 - kl_loss: 5.5119 - lr: 6.2500e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4230 - reconstruction_loss: 2.3129 - kl_loss: 5.5205 - lr: 6.2500e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4290 - reconstruction_loss: 2.3130 - kl_loss: 5.5251 - lr: 6.2500e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4060 - reconstruction_loss: 2.3066 - kl_loss: 5.5251 - lr: 6.2500e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4109 - reconstruction_loss: 2.3050 - kl_loss: 5.5289 - lr: 6.2500e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4102 - reconstruction_loss: 2.3052 - kl_loss: 5.5140 - lr: 6.2500e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4133 - reconstruction_loss: 2.3088 - kl_loss: 5.5065 - lr: 6.2500e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4129 - reconstruction_loss: 2.3059 - kl_loss: 5.5109 - lr: 6.2500e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4140 - reconstruction_loss: 2.3058 - kl_loss: 5.5059 - lr: 6.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4242 - reconstruction_loss: 2.3069 - kl_loss: 5.5024 - lr: 6.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4117 - reconstruction_loss: 2.3078 - kl_loss: 5.5058 - lr: 6.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4135 - reconstruction_loss: 2.3059 - kl_loss: 5.5039 - lr: 6.2500e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4219 - reconstruction_loss: 2.3129 - kl_loss: 5.4924 - lr: 6.2500e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4247 - reconstruction_loss: 2.3032 - kl_loss: 5.4964 - lr: 6.2500e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4280 - reconstruction_loss: 2.3259 - kl_loss: 5.5259 - lr: 6.2500e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4383 - reconstruction_loss: 2.3142 - kl_loss: 5.5128 - lr: 6.2500e-04\n",
      "Epoch 69/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4191 - reconstruction_loss: 2.3130 - kl_loss: 5.5104\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4190 - reconstruction_loss: 2.3054 - kl_loss: 5.5105 - lr: 6.2500e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4128 - reconstruction_loss: 2.3014 - kl_loss: 5.4980 - lr: 3.1250e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3912 - reconstruction_loss: 2.2978 - kl_loss: 5.5007 - lr: 3.1250e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4081 - reconstruction_loss: 2.2949 - kl_loss: 5.5052 - lr: 3.1250e-04\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4087 - reconstruction_loss: 2.2954 - kl_loss: 5.4897 - lr: 3.1250e-04\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3958 - reconstruction_loss: 2.2979 - kl_loss: 5.4902 - lr: 3.1250e-04\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3971 - reconstruction_loss: 2.2958 - kl_loss: 5.4930 - lr: 3.1250e-04\n",
      "Epoch 76/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4013 - reconstruction_loss: 2.3036 - kl_loss: 5.4893\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.4013 - reconstruction_loss: 2.2966 - kl_loss: 5.4905 - lr: 3.1250e-04\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3967 - reconstruction_loss: 2.2886 - kl_loss: 5.4958 - lr: 1.5625e-04\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 3.3810 - reconstruction_loss: 2.2875 - kl_loss: 5.4801 - lr: 1.5625e-04\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3879 - reconstruction_loss: 2.2908 - kl_loss: 5.4814 - lr: 1.5625e-04\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3829 - reconstruction_loss: 2.2916 - kl_loss: 5.4821 - lr: 1.5625e-04\n",
      "Epoch 81/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.4000 - reconstruction_loss: 2.2974 - kl_loss: 5.4865\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3998 - reconstruction_loss: 2.2900 - kl_loss: 5.4871 - lr: 1.5625e-04\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3912 - reconstruction_loss: 2.2882 - kl_loss: 5.4844 - lr: 7.8125e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3810 - reconstruction_loss: 2.2889 - kl_loss: 5.4770 - lr: 7.8125e-05\n",
      "Epoch 84/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.3970 - reconstruction_loss: 2.2964 - kl_loss: 5.4780\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3969 - reconstruction_loss: 2.2880 - kl_loss: 5.4776 - lr: 7.8125e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3956 - reconstruction_loss: 2.2869 - kl_loss: 5.4794 - lr: 3.9062e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3746 - reconstruction_loss: 2.2864 - kl_loss: 5.4750 - lr: 3.9062e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3818 - reconstruction_loss: 2.2869 - kl_loss: 5.4801 - lr: 3.9062e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3841 - reconstruction_loss: 2.2861 - kl_loss: 5.4779 - lr: 3.9062e-05\n",
      "Epoch 89/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4001 - reconstruction_loss: 2.2943 - kl_loss: 5.4822\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3999 - reconstruction_loss: 2.2867 - kl_loss: 5.4826 - lr: 3.9062e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3927 - reconstruction_loss: 2.2837 - kl_loss: 5.4798 - lr: 1.9531e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3804 - reconstruction_loss: 2.2862 - kl_loss: 5.4770 - lr: 1.9531e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3769 - reconstruction_loss: 2.2847 - kl_loss: 5.4795 - lr: 1.9531e-05\n",
      "Epoch 93/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.3960 - reconstruction_loss: 2.2958 - kl_loss: 5.4763\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3958 - reconstruction_loss: 2.2877 - kl_loss: 5.4758 - lr: 1.9531e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3867 - reconstruction_loss: 2.2851 - kl_loss: 5.4736 - lr: 9.7656e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3907 - reconstruction_loss: 2.2877 - kl_loss: 5.4768 - lr: 9.7656e-06\n",
      "Epoch 96/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.3839 - reconstruction_loss: 2.2939 - kl_loss: 5.4802\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3839 - reconstruction_loss: 2.2865 - kl_loss: 5.4801 - lr: 9.7656e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3853 - reconstruction_loss: 2.2856 - kl_loss: 5.4815 - lr: 4.8828e-06\n",
      "Epoch 98/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3752 - reconstruction_loss: 2.2838 - kl_loss: 5.4820 - lr: 4.8828e-06\n",
      "Epoch 99/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.3879 - reconstruction_loss: 2.2941 - kl_loss: 5.4819\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3878 - reconstruction_loss: 2.2866 - kl_loss: 5.4814 - lr: 4.8828e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 3.3935 - reconstruction_loss: 2.2836 - kl_loss: 5.4806 - lr: 2.4414e-06\n",
      "Epoch 100: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 17:00:14,006]\u001b[0m Trial 90 finished with value: 0.004479457376321536 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 29, 'encoder_units_l2': 702, 'encoder_units_l3': 45, 'decoder_layers': 5, 'decoder_units_l1': 75, 'decoder_units_l2': 23, 'decoder_units_l3': 643, 'decoder_units_l4': 34, 'decoder_units_l5': 709, 'beta': 0.2, 'lr': 0.01, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 8s 39ms/step - loss: 18.1394 - reconstruction_loss: 14.2629 - kl_loss: 2.2998 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 8s 39ms/step - loss: 12.2964 - reconstruction_loss: 11.6300 - kl_loss: 2.4693 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 14.5705 - reconstruction_loss: 14.0454 - kl_loss: 2.7052 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 11.8028 - reconstruction_loss: 10.9229 - kl_loss: 2.8984 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 8s 40ms/step - loss: 10.9340 - reconstruction_loss: 10.1738 - kl_loss: 2.9781 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 8s 40ms/step - loss: 12.5008 - reconstruction_loss: 11.9248 - kl_loss: 3.1225 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 8s 39ms/step - loss: 11.0508 - reconstruction_loss: 9.8033 - kl_loss: 3.3675 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 8.2403 - reconstruction_loss: 7.2441 - kl_loss: 4.3327 - lr: 0.0100\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 8.0479 - reconstruction_loss: 7.1602 - kl_loss: 4.2015 - lr: 0.0100\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 7.9682 - reconstruction_loss: 7.0920 - kl_loss: 4.1804 - lr: 0.0100\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 7.8947 - reconstruction_loss: 7.0642 - kl_loss: 4.1410 - lr: 0.0100\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 8s 38ms/step - loss: 7.8977 - reconstruction_loss: 7.0305 - kl_loss: 4.1373 - lr: 0.0100\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 8s 38ms/step - loss: 7.8668 - reconstruction_loss: 7.0200 - kl_loss: 4.1195 - lr: 0.0100\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 7.8307 - reconstruction_loss: 6.9011 - kl_loss: 4.2033 - lr: 0.0100\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 7.4711 - reconstruction_loss: 6.4912 - kl_loss: 4.4558 - lr: 0.0100\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 8.5732 - reconstruction_loss: 6.0321 - kl_loss: 4.9917 - lr: 0.0100\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 6.6966 - reconstruction_loss: 5.8712 - kl_loss: 4.9530 - lr: 0.0100\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 8s 39ms/step - loss: 6.6552 - reconstruction_loss: 5.1190 - kl_loss: 5.1929 - lr: 0.0100\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 8s 38ms/step - loss: 6.5422 - reconstruction_loss: 4.8885 - kl_loss: 5.2165 - lr: 0.0100\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 8s 39ms/step - loss: 5.6687 - reconstruction_loss: 4.2581 - kl_loss: 5.2873 - lr: 0.0100\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 8s 38ms/step - loss: 5.5408 - reconstruction_loss: 4.5301 - kl_loss: 5.3263 - lr: 0.0100\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 8s 39ms/step - loss: 6.9115 - reconstruction_loss: 5.1311 - kl_loss: 5.4230 - lr: 0.0100\n",
      "Epoch 23/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 6.7846 - reconstruction_loss: 5.8812 - kl_loss: 5.0148\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 6.7854 - reconstruction_loss: 5.8591 - kl_loss: 5.0144 - lr: 0.0100\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 8s 40ms/step - loss: 5.6369 - reconstruction_loss: 4.4216 - kl_loss: 5.1757 - lr: 0.0050\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 5.4301 - reconstruction_loss: 4.3275 - kl_loss: 5.2373 - lr: 0.0050\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 5.1703 - reconstruction_loss: 4.3358 - kl_loss: 5.3553\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 5.1715 - reconstruction_loss: 4.3358 - kl_loss: 5.3553 - lr: 0.0050\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 5.4535 - reconstruction_loss: 4.1261 - kl_loss: 5.5608 - lr: 0.0025\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.9094 - reconstruction_loss: 3.7685 - kl_loss: 5.4294 - lr: 0.0025\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.8847 - reconstruction_loss: 3.9957 - kl_loss: 5.4590 - lr: 0.0025\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 5.0346 - reconstruction_loss: 3.7912 - kl_loss: 5.5154 - lr: 0.0025\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 4.8315 - reconstruction_loss: 3.7099 - kl_loss: 5.5362 - lr: 0.0025\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.7248 - reconstruction_loss: 3.7010 - kl_loss: 5.5808 - lr: 0.0025\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.8198 - reconstruction_loss: 3.5707 - kl_loss: 5.4317 - lr: 0.0025\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.6326 - reconstruction_loss: 3.5673 - kl_loss: 5.4524 - lr: 0.0025\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.7629 - reconstruction_loss: 3.7722 - kl_loss: 5.4567 - lr: 0.0025\n",
      "Epoch 36/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 5.0379 - reconstruction_loss: 3.8010 - kl_loss: 5.4658\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 5.0363 - reconstruction_loss: 3.7870 - kl_loss: 5.4656 - lr: 0.0025\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.5722 - reconstruction_loss: 3.5035 - kl_loss: 5.4930 - lr: 0.0012\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.5654 - reconstruction_loss: 3.4088 - kl_loss: 5.4886 - lr: 0.0012\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.4535 - reconstruction_loss: 3.3202 - kl_loss: 5.4904 - lr: 0.0012\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.3812 - reconstruction_loss: 3.2708 - kl_loss: 5.4976 - lr: 0.0012\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.3543 - reconstruction_loss: 3.2633 - kl_loss: 5.5211 - lr: 0.0012\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.3019 - reconstruction_loss: 3.2167 - kl_loss: 5.5634 - lr: 0.0012\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.2884 - reconstruction_loss: 3.1391 - kl_loss: 5.5835 - lr: 0.0012\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.2949 - reconstruction_loss: 3.1534 - kl_loss: 5.5780 - lr: 0.0012\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.1993 - reconstruction_loss: 3.1034 - kl_loss: 5.5957 - lr: 0.0012\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.2097 - reconstruction_loss: 3.0442 - kl_loss: 5.6682 - lr: 0.0012\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.1997 - reconstruction_loss: 3.0826 - kl_loss: 5.7680 - lr: 0.0012\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.2783 - reconstruction_loss: 3.0607 - kl_loss: 5.8076 - lr: 0.0012\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.2372 - reconstruction_loss: 2.9817 - kl_loss: 5.8640 - lr: 0.0012\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.0578 - reconstruction_loss: 2.8911 - kl_loss: 5.7961 - lr: 0.0012\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.0320 - reconstruction_loss: 2.8561 - kl_loss: 5.7996 - lr: 0.0012\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.0839 - reconstruction_loss: 2.9500 - kl_loss: 5.8056 - lr: 0.0012\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.2446 - reconstruction_loss: 3.0555 - kl_loss: 5.7490 - lr: 0.0012\n",
      "Epoch 54/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 4.7852 - reconstruction_loss: 3.4156 - kl_loss: 5.8251\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.7830 - reconstruction_loss: 3.4028 - kl_loss: 5.8253 - lr: 0.0012\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.2199 - reconstruction_loss: 2.9713 - kl_loss: 5.8177 - lr: 6.2500e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.0707 - reconstruction_loss: 2.8993 - kl_loss: 5.8311 - lr: 6.2500e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 4.0614 - reconstruction_loss: 2.8805 - kl_loss: 5.7848\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.0613 - reconstruction_loss: 2.8805 - kl_loss: 5.7848 - lr: 6.2500e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.0065 - reconstruction_loss: 2.7970 - kl_loss: 5.8244 - lr: 3.1250e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 3.9346 - reconstruction_loss: 2.7681 - kl_loss: 5.7997 - lr: 3.1250e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.9018 - reconstruction_loss: 2.7257 - kl_loss: 5.7945 - lr: 3.1250e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.8674 - reconstruction_loss: 2.7171 - kl_loss: 5.7896 - lr: 3.1250e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.8690 - reconstruction_loss: 2.7108 - kl_loss: 5.7941 - lr: 3.1250e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.8812 - reconstruction_loss: 2.7318 - kl_loss: 5.8340 - lr: 3.1250e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 3.9846 - reconstruction_loss: 2.8276 - kl_loss: 5.8788 - lr: 3.1250e-04\n",
      "Epoch 65/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 4.0502 - reconstruction_loss: 2.8545 - kl_loss: 5.9087\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 4.0500 - reconstruction_loss: 2.8444 - kl_loss: 5.9087 - lr: 3.1250e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.9852 - reconstruction_loss: 2.7720 - kl_loss: 5.9030 - lr: 1.5625e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.9133 - reconstruction_loss: 2.7172 - kl_loss: 5.8904 - lr: 1.5625e-04\n",
      "Epoch 68/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.9090 - reconstruction_loss: 2.7206 - kl_loss: 5.8895\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.9088 - reconstruction_loss: 2.7112 - kl_loss: 5.8893 - lr: 1.5625e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.8848 - reconstruction_loss: 2.6885 - kl_loss: 5.8818 - lr: 7.8125e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.8683 - reconstruction_loss: 2.6768 - kl_loss: 5.8784 - lr: 7.8125e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.8748 - reconstruction_loss: 2.6751 - kl_loss: 5.8891 - lr: 7.8125e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 3.8749 - reconstruction_loss: 2.6720 - kl_loss: 5.8824 - lr: 7.8125e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.8708 - reconstruction_loss: 2.6917 - kl_loss: 5.8894 - lr: 7.8125e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.8669 - reconstruction_loss: 2.6892 - kl_loss: 5.8878 - lr: 7.8125e-05\n",
      "Epoch 75/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.8689 - reconstruction_loss: 2.6973 - kl_loss: 5.8932\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.8689 - reconstruction_loss: 2.6882 - kl_loss: 5.8931 - lr: 7.8125e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 3.8638 - reconstruction_loss: 2.6734 - kl_loss: 5.8799 - lr: 3.9062e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.8495 - reconstruction_loss: 2.6639 - kl_loss: 5.8937 - lr: 3.9062e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.8610 - reconstruction_loss: 2.6584 - kl_loss: 5.8852 - lr: 3.9062e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.8455 - reconstruction_loss: 2.6571 - kl_loss: 5.8805 - lr: 3.9062e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.8391 - reconstruction_loss: 2.6494 - kl_loss: 5.8855 - lr: 3.9062e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.8322 - reconstruction_loss: 2.6490 - kl_loss: 5.8741 - lr: 3.9062e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.8378 - reconstruction_loss: 2.6485 - kl_loss: 5.8662 - lr: 3.9062e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.8266 - reconstruction_loss: 2.6367 - kl_loss: 5.8686 - lr: 3.9062e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.8116 - reconstruction_loss: 2.6309 - kl_loss: 5.8680 - lr: 3.9062e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.8151 - reconstruction_loss: 2.6299 - kl_loss: 5.8761 - lr: 3.9062e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.8098 - reconstruction_loss: 2.6294 - kl_loss: 5.8636 - lr: 3.9062e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.8140 - reconstruction_loss: 2.6294 - kl_loss: 5.8546 - lr: 3.9062e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.8043 - reconstruction_loss: 2.6196 - kl_loss: 5.8616 - lr: 3.9062e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.8064 - reconstruction_loss: 2.6275 - kl_loss: 5.8630 - lr: 3.9062e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7959 - reconstruction_loss: 2.6177 - kl_loss: 5.8617 - lr: 3.9062e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.8031 - reconstruction_loss: 2.6188 - kl_loss: 5.8559 - lr: 3.9062e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.7900 - reconstruction_loss: 2.6127 - kl_loss: 5.8530 - lr: 3.9062e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7835 - reconstruction_loss: 2.6147 - kl_loss: 5.8483 - lr: 3.9062e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.8069 - reconstruction_loss: 2.6138 - kl_loss: 5.8605 - lr: 3.9062e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.7730 - reconstruction_loss: 2.6078 - kl_loss: 5.8596 - lr: 3.9062e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.7987 - reconstruction_loss: 2.6185 - kl_loss: 5.8517 - lr: 3.9062e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.7948 - reconstruction_loss: 2.6131 - kl_loss: 5.8587 - lr: 3.9062e-05\n",
      "Epoch 98/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.7922 - reconstruction_loss: 2.6187 - kl_loss: 5.8554\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7921 - reconstruction_loss: 2.6093 - kl_loss: 5.8555 - lr: 3.9062e-05\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.7929 - reconstruction_loss: 2.6104 - kl_loss: 5.8497 - lr: 1.9531e-05\n",
      "Epoch 100/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 7s 35ms/step - loss: 3.7864 - reconstruction_loss: 2.6142 - kl_loss: 5.8610 - lr: 1.9531e-05\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.7836 - reconstruction_loss: 2.6083 - kl_loss: 5.8587\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7836 - reconstruction_loss: 2.6083 - kl_loss: 5.8587 - lr: 1.9531e-05\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7876 - reconstruction_loss: 2.6062 - kl_loss: 5.8612 - lr: 9.7656e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.7702 - reconstruction_loss: 2.6002 - kl_loss: 5.8634 - lr: 9.7656e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.7668 - reconstruction_loss: 2.5986 - kl_loss: 5.8598 - lr: 9.7656e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7878 - reconstruction_loss: 2.6065 - kl_loss: 5.8601 - lr: 9.7656e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.7871 - reconstruction_loss: 2.6048 - kl_loss: 5.8632 - lr: 9.7656e-06\n",
      "Epoch 107/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.7912 - reconstruction_loss: 2.6141 - kl_loss: 5.8636\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7911 - reconstruction_loss: 2.6048 - kl_loss: 5.8636 - lr: 9.7656e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 3.7714 - reconstruction_loss: 2.6007 - kl_loss: 5.8651 - lr: 4.8828e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.7741 - reconstruction_loss: 2.5998 - kl_loss: 5.8624 - lr: 4.8828e-06\n",
      "Epoch 110/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.7635 - reconstruction_loss: 2.6085 - kl_loss: 5.8624\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7636 - reconstruction_loss: 2.5993 - kl_loss: 5.8624 - lr: 4.8828e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7825 - reconstruction_loss: 2.5970 - kl_loss: 5.8623 - lr: 2.4414e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7762 - reconstruction_loss: 2.5955 - kl_loss: 5.8598 - lr: 2.4414e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.7681 - reconstruction_loss: 2.5962 - kl_loss: 5.8596 - lr: 2.4414e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.7789 - reconstruction_loss: 2.5972 - kl_loss: 5.8606 - lr: 2.4414e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.7795 - reconstruction_loss: 2.5948 - kl_loss: 5.8608 - lr: 2.4414e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.7644 - reconstruction_loss: 2.5968 - kl_loss: 5.8593 - lr: 2.4414e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.7829 - reconstruction_loss: 2.5976 - kl_loss: 5.8588 - lr: 2.4414e-06\n",
      "Epoch 118/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.7892 - reconstruction_loss: 2.6084 - kl_loss: 5.8603\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7890 - reconstruction_loss: 2.5992 - kl_loss: 5.8603 - lr: 2.4414e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.7742 - reconstruction_loss: 2.5983 - kl_loss: 5.8613 - lr: 1.2207e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.7719 - reconstruction_loss: 2.5938 - kl_loss: 5.8607 - lr: 1.2207e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7845 - reconstruction_loss: 2.5973 - kl_loss: 5.8614 - lr: 1.2207e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 7s 35ms/step - loss: 3.7863 - reconstruction_loss: 2.5976 - kl_loss: 5.8611 - lr: 1.2207e-06\n",
      "Epoch 123/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.7628 - reconstruction_loss: 2.6045 - kl_loss: 5.8608\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7628 - reconstruction_loss: 2.5954 - kl_loss: 5.8605 - lr: 1.2207e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7890 - reconstruction_loss: 2.5990 - kl_loss: 5.8608 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7740 - reconstruction_loss: 2.5971 - kl_loss: 5.8622 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7820 - reconstruction_loss: 2.5940 - kl_loss: 5.8623 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7640 - reconstruction_loss: 2.5939 - kl_loss: 5.8614 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7801 - reconstruction_loss: 2.5950 - kl_loss: 5.8618 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7796 - reconstruction_loss: 2.5960 - kl_loss: 5.8619 - lr: 1.0000e-06\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 7s 36ms/step - loss: 3.7723 - reconstruction_loss: 2.5939 - kl_loss: 5.8620 - lr: 1.0000e-06\n",
      "Epoch 130: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 17:15:53,508]\u001b[0m Trial 91 finished with value: 0.005086956139153075 and parameters: {'encoder_layers': 8, 'encoder_units_l1': 28, 'encoder_units_l2': 912, 'encoder_units_l3': 45, 'encoder_units_l4': 36, 'encoder_units_l5': 961, 'encoder_units_l6': 16, 'encoder_units_l7': 114, 'encoder_units_l8': 604, 'decoder_layers': 5, 'decoder_units_l1': 84, 'decoder_units_l2': 20, 'decoder_units_l3': 620, 'decoder_units_l4': 54, 'decoder_units_l5': 498, 'beta': 0.2, 'lr': 0.01, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 4s 16ms/step - loss: 15.1840 - reconstruction_loss: 12.5657 - kl_loss: 2.3850 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 9.3956 - reconstruction_loss: 7.2527 - kl_loss: 4.3707 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 6.2052 - reconstruction_loss: 4.8468 - kl_loss: 5.1730 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 5.1622 - reconstruction_loss: 3.9003 - kl_loss: 5.4829 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.9875 - reconstruction_loss: 3.8703 - kl_loss: 5.4944 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.6389 - reconstruction_loss: 3.6048 - kl_loss: 5.6980 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.6692 - reconstruction_loss: 3.2807 - kl_loss: 5.9071 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.2631 - reconstruction_loss: 3.0274 - kl_loss: 5.8982 - lr: 0.0100\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.9403 - reconstruction_loss: 2.7987 - kl_loss: 5.8873 - lr: 0.0100\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.0619 - reconstruction_loss: 2.8902 - kl_loss: 5.8038 - lr: 0.0100\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.0109 - reconstruction_loss: 2.8035 - kl_loss: 5.8338 - lr: 0.0100\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.0365 - reconstruction_loss: 2.8750 - kl_loss: 5.8226 - lr: 0.0100\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.9665 - reconstruction_loss: 2.6914 - kl_loss: 5.7786 - lr: 0.0100\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.7782 - reconstruction_loss: 2.6348 - kl_loss: 5.7054 - lr: 0.0100\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.7486 - reconstruction_loss: 2.5856 - kl_loss: 5.6300 - lr: 0.0100\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6935 - reconstruction_loss: 2.5622 - kl_loss: 5.5917 - lr: 0.0100\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.7477 - reconstruction_loss: 2.5621 - kl_loss: 5.5467 - lr: 0.0100\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.8131 - reconstruction_loss: 3.3687 - kl_loss: 5.5508 - lr: 0.0100\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 5.4366 - reconstruction_loss: 4.1101 - kl_loss: 5.3888 - lr: 0.0100\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 5.0241 - reconstruction_loss: 3.8448 - kl_loss: 5.6796\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 5.0239 - reconstruction_loss: 3.8448 - kl_loss: 5.6796 - lr: 0.0100\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.8323 - reconstruction_loss: 3.4647 - kl_loss: 5.8245 - lr: 0.0050\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 4.4553 - reconstruction_loss: 3.2241 - kl_loss: 5.9425 - lr: 0.0050\n",
      "Epoch 23/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.9959 - reconstruction_loss: 2.8018 - kl_loss: 5.9565\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.9958 - reconstruction_loss: 2.7942 - kl_loss: 5.9593 - lr: 0.0050\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.0999 - reconstruction_loss: 2.7467 - kl_loss: 5.9130 - lr: 0.0025\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.8414 - reconstruction_loss: 2.6672 - kl_loss: 5.9091 - lr: 0.0025\n",
      "Epoch 26/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.8131 - reconstruction_loss: 2.6155 - kl_loss: 5.8990\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.8127 - reconstruction_loss: 2.6073 - kl_loss: 5.8994 - lr: 0.0025\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.7102 - reconstruction_loss: 2.4956 - kl_loss: 5.8721 - lr: 0.0012\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6588 - reconstruction_loss: 2.4831 - kl_loss: 5.8149 - lr: 0.0012\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6290 - reconstruction_loss: 2.4619 - kl_loss: 5.8139 - lr: 0.0012\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6410 - reconstruction_loss: 2.4639 - kl_loss: 5.8066 - lr: 0.0012\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6078 - reconstruction_loss: 2.4460 - kl_loss: 5.7845 - lr: 0.0012\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6086 - reconstruction_loss: 2.4645 - kl_loss: 5.7949 - lr: 0.0012\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6057 - reconstruction_loss: 2.4498 - kl_loss: 5.7825 - lr: 0.0012\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5937 - reconstruction_loss: 2.4328 - kl_loss: 5.7664 - lr: 0.0012\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6484 - reconstruction_loss: 2.4830 - kl_loss: 5.8131 - lr: 0.0012\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6850 - reconstruction_loss: 2.5138 - kl_loss: 5.7986 - lr: 0.0012\n",
      "Epoch 37/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.6232 - reconstruction_loss: 2.4578 - kl_loss: 5.7883\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6230 - reconstruction_loss: 2.4484 - kl_loss: 5.7878 - lr: 0.0012\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6017 - reconstruction_loss: 2.4216 - kl_loss: 5.7717 - lr: 6.2500e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5859 - reconstruction_loss: 2.4216 - kl_loss: 5.7743 - lr: 6.2500e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5743 - reconstruction_loss: 2.4205 - kl_loss: 5.7708 - lr: 6.2500e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5738 - reconstruction_loss: 2.4289 - kl_loss: 5.7490 - lr: 6.2500e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5555 - reconstruction_loss: 2.4092 - kl_loss: 5.7208 - lr: 6.2500e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5579 - reconstruction_loss: 2.3959 - kl_loss: 5.7220 - lr: 6.2500e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5622 - reconstruction_loss: 2.4051 - kl_loss: 5.7320 - lr: 6.2500e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5400 - reconstruction_loss: 2.3900 - kl_loss: 5.7118 - lr: 6.2500e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5527 - reconstruction_loss: 2.3998 - kl_loss: 5.7138 - lr: 6.2500e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5716 - reconstruction_loss: 2.4138 - kl_loss: 5.7311 - lr: 6.2500e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5612 - reconstruction_loss: 2.4119 - kl_loss: 5.7023\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5611 - reconstruction_loss: 2.4119 - kl_loss: 5.7023 - lr: 6.2500e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5638 - reconstruction_loss: 2.4066 - kl_loss: 5.7137 - lr: 3.1250e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5508 - reconstruction_loss: 2.4015 - kl_loss: 5.7159 - lr: 3.1250e-04\n",
      "Epoch 51/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.5392 - reconstruction_loss: 2.4137 - kl_loss: 5.7142\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5394 - reconstruction_loss: 2.4049 - kl_loss: 5.7152 - lr: 3.1250e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5340 - reconstruction_loss: 2.3797 - kl_loss: 5.7346 - lr: 1.5625e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5403 - reconstruction_loss: 2.3798 - kl_loss: 5.7154 - lr: 1.5625e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5206 - reconstruction_loss: 2.3722 - kl_loss: 5.7044 - lr: 1.5625e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5029 - reconstruction_loss: 2.3742 - kl_loss: 5.6961 - lr: 1.5625e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5202 - reconstruction_loss: 2.3746 - kl_loss: 5.7023 - lr: 1.5625e-04\n",
      "Epoch 57/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.5340 - reconstruction_loss: 2.3832 - kl_loss: 5.7124\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5338 - reconstruction_loss: 2.3753 - kl_loss: 5.7126 - lr: 1.5625e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5272 - reconstruction_loss: 2.3705 - kl_loss: 5.7074 - lr: 7.8125e-05\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5204 - reconstruction_loss: 2.3687 - kl_loss: 5.6969 - lr: 7.8125e-05\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5167 - reconstruction_loss: 2.3700 - kl_loss: 5.7012 - lr: 7.8125e-05\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5151 - reconstruction_loss: 2.3679 - kl_loss: 5.7037 - lr: 7.8125e-05\n",
      "Epoch 62/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5161 - reconstruction_loss: 2.3798 - kl_loss: 5.7019\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5161 - reconstruction_loss: 2.3707 - kl_loss: 5.7025 - lr: 7.8125e-05\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5168 - reconstruction_loss: 2.3648 - kl_loss: 5.7073 - lr: 3.9062e-05\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5198 - reconstruction_loss: 2.3651 - kl_loss: 5.7021 - lr: 3.9062e-05\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5014 - reconstruction_loss: 2.3690 - kl_loss: 5.6948 - lr: 3.9062e-05\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5272 - reconstruction_loss: 2.3700 - kl_loss: 5.6965 - lr: 3.9062e-05\n",
      "Epoch 67/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5132 - reconstruction_loss: 2.3763 - kl_loss: 5.6912\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5131 - reconstruction_loss: 2.3679 - kl_loss: 5.6914 - lr: 3.9062e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5180 - reconstruction_loss: 2.3698 - kl_loss: 5.6829 - lr: 1.9531e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5311 - reconstruction_loss: 2.3705 - kl_loss: 5.6902 - lr: 1.9531e-05\n",
      "Epoch 70/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5275 - reconstruction_loss: 2.3764 - kl_loss: 5.6960\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5272 - reconstruction_loss: 2.3679 - kl_loss: 5.6970 - lr: 1.9531e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5057 - reconstruction_loss: 2.3651 - kl_loss: 5.6952 - lr: 9.7656e-06\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5173 - reconstruction_loss: 2.3624 - kl_loss: 5.6968 - lr: 9.7656e-06\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.5156 - reconstruction_loss: 2.3613 - kl_loss: 5.6972 - lr: 9.7656e-06\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4963 - reconstruction_loss: 2.3641 - kl_loss: 5.6935 - lr: 9.7656e-06\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5048 - reconstruction_loss: 2.3620 - kl_loss: 5.6992 - lr: 9.7656e-06\n",
      "Epoch 76/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5093 - reconstruction_loss: 2.3715 - kl_loss: 5.6980\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5092 - reconstruction_loss: 2.3631 - kl_loss: 5.6982 - lr: 9.7656e-06\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.4930 - reconstruction_loss: 2.3595 - kl_loss: 5.7006 - lr: 4.8828e-06\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4950 - reconstruction_loss: 2.3608 - kl_loss: 5.6996 - lr: 4.8828e-06\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5231 - reconstruction_loss: 2.3608 - kl_loss: 5.6995 - lr: 4.8828e-06\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5043 - reconstruction_loss: 2.3590 - kl_loss: 5.6980 - lr: 4.8828e-06\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5018 - reconstruction_loss: 2.3616 - kl_loss: 5.6971 - lr: 4.8828e-06\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5176 - reconstruction_loss: 2.3592 - kl_loss: 5.6981 - lr: 4.8828e-06\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5063 - reconstruction_loss: 2.3583 - kl_loss: 5.6972 - lr: 4.8828e-06\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5021 - reconstruction_loss: 2.3600 - kl_loss: 5.6962 - lr: 4.8828e-06\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.4953 - reconstruction_loss: 2.3569 - kl_loss: 5.6955 - lr: 4.8828e-06\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4991 - reconstruction_loss: 2.3596 - kl_loss: 5.6948 - lr: 4.8828e-06\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5159 - reconstruction_loss: 2.3623 - kl_loss: 5.6954 - lr: 4.8828e-06\n",
      "Epoch 88/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5135 - reconstruction_loss: 2.3697 - kl_loss: 5.6953\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5134 - reconstruction_loss: 2.3613 - kl_loss: 5.6952 - lr: 4.8828e-06\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5109 - reconstruction_loss: 2.3596 - kl_loss: 5.6953 - lr: 2.4414e-06\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5000 - reconstruction_loss: 2.3575 - kl_loss: 5.6951 - lr: 2.4414e-06\n",
      "Epoch 91/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5178 - reconstruction_loss: 2.3675 - kl_loss: 5.6955\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5175 - reconstruction_loss: 2.3601 - kl_loss: 5.6944 - lr: 2.4414e-06\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5126 - reconstruction_loss: 2.3605 - kl_loss: 5.6945 - lr: 1.2207e-06\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5012 - reconstruction_loss: 2.3579 - kl_loss: 5.6943 - lr: 1.2207e-06\n",
      "Epoch 94/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/196 [============================>.] - ETA: 0s - loss: 3.5067 - reconstruction_loss: 2.3664 - kl_loss: 5.6940\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5066 - reconstruction_loss: 2.3585 - kl_loss: 5.6930 - lr: 1.2207e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.5234 - reconstruction_loss: 2.3600 - kl_loss: 5.6934 - lr: 1.0000e-06\n",
      "Epoch 95: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 17:21:03,915]\u001b[0m Trial 92 finished with value: 0.004629707992184772 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 34, 'encoder_units_l2': 700, 'encoder_units_l3': 49, 'decoder_layers': 5, 'decoder_units_l1': 67, 'decoder_units_l2': 29, 'decoder_units_l3': 890, 'decoder_units_l4': 25, 'decoder_units_l5': 657, 'beta': 0.2, 'lr': 0.01, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 13.5571 - reconstruction_loss: 9.5316 - kl_loss: 3.6278 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 7.1368 - reconstruction_loss: 5.7449 - kl_loss: 4.8657 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.9680 - reconstruction_loss: 4.6246 - kl_loss: 5.2215 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.2560 - reconstruction_loss: 3.9786 - kl_loss: 5.5475 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.6107 - reconstruction_loss: 3.5310 - kl_loss: 5.7426 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.8003 - reconstruction_loss: 4.2092 - kl_loss: 5.8221 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.7832 - reconstruction_loss: 3.4982 - kl_loss: 5.8453 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.9343 - reconstruction_loss: 3.4855 - kl_loss: 5.7514 - lr: 0.0100\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.6713 - reconstruction_loss: 3.5662 - kl_loss: 5.8842 - lr: 0.0100\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.1336 - reconstruction_loss: 3.8563 - kl_loss: 5.8724 - lr: 0.0100\n",
      "Epoch 11/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 5.5165 - reconstruction_loss: 3.9030 - kl_loss: 5.7716\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.5069 - reconstruction_loss: 3.8787 - kl_loss: 5.7719 - lr: 0.0100\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.2591 - reconstruction_loss: 3.0037 - kl_loss: 5.9409 - lr: 0.0050\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0112 - reconstruction_loss: 2.7798 - kl_loss: 5.8629 - lr: 0.0050\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0049 - reconstruction_loss: 2.8873 - kl_loss: 5.8447 - lr: 0.0050\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.1914 - reconstruction_loss: 3.1688 - kl_loss: 5.8853 - lr: 0.0050\n",
      "Epoch 16/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 4.7745 - reconstruction_loss: 3.3500 - kl_loss: 6.0196\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.7698 - reconstruction_loss: 3.3329 - kl_loss: 6.0191 - lr: 0.0050\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.9932 - reconstruction_loss: 2.7567 - kl_loss: 5.9043 - lr: 0.0025\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.9136 - reconstruction_loss: 2.6788 - kl_loss: 5.8745 - lr: 0.0025\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.8440 - reconstruction_loss: 2.5961 - kl_loss: 5.8895 - lr: 0.0025\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.7372 - reconstruction_loss: 2.5618 - kl_loss: 5.9061 - lr: 0.0025\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.8492 - reconstruction_loss: 2.6090 - kl_loss: 5.8900 - lr: 0.0025\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.7415 - reconstruction_loss: 2.5809 - kl_loss: 5.7977 - lr: 0.0025\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7396 - reconstruction_loss: 2.5418 - kl_loss: 5.8006 - lr: 0.0025\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6837 - reconstruction_loss: 2.4919 - kl_loss: 5.8527 - lr: 0.0025\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6103 - reconstruction_loss: 2.4265 - kl_loss: 5.8017 - lr: 0.0025\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5547 - reconstruction_loss: 2.4057 - kl_loss: 5.7125 - lr: 0.0025\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5291 - reconstruction_loss: 2.3995 - kl_loss: 5.6833 - lr: 0.0025\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5409 - reconstruction_loss: 2.3942 - kl_loss: 5.6052 - lr: 0.0025\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5120 - reconstruction_loss: 2.4051 - kl_loss: 5.6118 - lr: 0.0025\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5699 - reconstruction_loss: 2.4000 - kl_loss: 5.6115 - lr: 0.0025\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5417 - reconstruction_loss: 2.3937 - kl_loss: 5.6004 - lr: 0.0025\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4934 - reconstruction_loss: 2.3745 - kl_loss: 5.6048 - lr: 0.0025\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4868 - reconstruction_loss: 2.3575 - kl_loss: 5.5585 - lr: 0.0025\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4775 - reconstruction_loss: 2.3577 - kl_loss: 5.5470 - lr: 0.0025\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4695 - reconstruction_loss: 2.3509 - kl_loss: 5.5156 - lr: 0.0025\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4582 - reconstruction_loss: 2.3492 - kl_loss: 5.5249 - lr: 0.0025\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5032 - reconstruction_loss: 2.4142 - kl_loss: 5.5888 - lr: 0.0025\n",
      "Epoch 38/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5680 - reconstruction_loss: 2.4258 - kl_loss: 5.6050\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5675 - reconstruction_loss: 2.4180 - kl_loss: 5.6057 - lr: 0.0025\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5854 - reconstruction_loss: 2.4206 - kl_loss: 5.6571 - lr: 0.0012\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5346 - reconstruction_loss: 2.3802 - kl_loss: 5.6269 - lr: 0.0012\n",
      "Epoch 41/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4670 - reconstruction_loss: 2.3548 - kl_loss: 5.5717\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4669 - reconstruction_loss: 2.3469 - kl_loss: 5.5724 - lr: 0.0012\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4687 - reconstruction_loss: 2.3509 - kl_loss: 5.5701 - lr: 6.2500e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4762 - reconstruction_loss: 2.3478 - kl_loss: 5.5670 - lr: 6.2500e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4464 - reconstruction_loss: 2.3304 - kl_loss: 5.5668 - lr: 6.2500e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4698 - reconstruction_loss: 2.3360 - kl_loss: 5.5431 - lr: 6.2500e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4362 - reconstruction_loss: 2.3214 - kl_loss: 5.5302 - lr: 6.2500e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4480 - reconstruction_loss: 2.3302 - kl_loss: 5.5304 - lr: 6.2500e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4683 - reconstruction_loss: 2.3473 - kl_loss: 5.5560 - lr: 6.2500e-04\n",
      "Epoch 49/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4574 - reconstruction_loss: 2.3451 - kl_loss: 5.5485\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4572 - reconstruction_loss: 2.3364 - kl_loss: 5.5484 - lr: 6.2500e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4256 - reconstruction_loss: 2.3206 - kl_loss: 5.5338 - lr: 3.1250e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4241 - reconstruction_loss: 2.3143 - kl_loss: 5.5220 - lr: 3.1250e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4452 - reconstruction_loss: 2.3252 - kl_loss: 5.5163 - lr: 3.1250e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4313 - reconstruction_loss: 2.3107 - kl_loss: 5.5182 - lr: 3.1250e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4329 - reconstruction_loss: 2.3147 - kl_loss: 5.5158 - lr: 3.1250e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4143 - reconstruction_loss: 2.3148 - kl_loss: 5.5180 - lr: 3.1250e-04\n",
      "Epoch 56/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4364 - reconstruction_loss: 2.3280 - kl_loss: 5.5181\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4362 - reconstruction_loss: 2.3190 - kl_loss: 5.5179 - lr: 3.1250e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4237 - reconstruction_loss: 2.3075 - kl_loss: 5.5050 - lr: 1.5625e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4169 - reconstruction_loss: 2.3131 - kl_loss: 5.5072 - lr: 1.5625e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4183 - reconstruction_loss: 2.3093 - kl_loss: 5.5079 - lr: 1.5625e-04\n",
      "Epoch 60/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4266 - reconstruction_loss: 2.3190 - kl_loss: 5.5057\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4264 - reconstruction_loss: 2.3098 - kl_loss: 5.5059 - lr: 1.5625e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4344 - reconstruction_loss: 2.3103 - kl_loss: 5.5116 - lr: 7.8125e-05\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.4199 - reconstruction_loss: 2.3129 - kl_loss: 5.5122 - lr: 7.8125e-05\n",
      "Epoch 63/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4127 - reconstruction_loss: 2.3224 - kl_loss: 5.5151\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4128 - reconstruction_loss: 2.3140 - kl_loss: 5.5136 - lr: 7.8125e-05\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4103 - reconstruction_loss: 2.3068 - kl_loss: 5.5165 - lr: 3.9062e-05\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4275 - reconstruction_loss: 2.3071 - kl_loss: 5.5269 - lr: 3.9062e-05\n",
      "Epoch 66/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.4236 - reconstruction_loss: 2.3182 - kl_loss: 5.5163\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4235 - reconstruction_loss: 2.3112 - kl_loss: 5.5164 - lr: 3.9062e-05\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4234 - reconstruction_loss: 2.3093 - kl_loss: 5.5138 - lr: 1.9531e-05\n",
      "Epoch 67: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 17:24:01,095]\u001b[0m Trial 93 finished with value: 0.004527083436865485 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 42, 'encoder_units_l2': 544, 'encoder_units_l3': 35, 'decoder_layers': 4, 'decoder_units_l1': 77, 'decoder_units_l2': 25, 'decoder_units_l3': 658, 'decoder_units_l4': 34, 'beta': 0.2, 'lr': 0.01, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 15.2328 - reconstruction_loss: 12.6627 - kl_loss: 2.4307 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 11.2956 - reconstruction_loss: 9.0773 - kl_loss: 3.5221 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 7.6449 - reconstruction_loss: 6.2478 - kl_loss: 4.5487 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.8259 - reconstruction_loss: 4.5950 - kl_loss: 5.1228 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 5.2128 - reconstruction_loss: 4.0036 - kl_loss: 5.3447 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.9564 - reconstruction_loss: 3.7563 - kl_loss: 5.4657 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 4.6358 - reconstruction_loss: 3.4902 - kl_loss: 5.5250 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.4885 - reconstruction_loss: 3.4009 - kl_loss: 5.5527 - lr: 0.0100\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.4347 - reconstruction_loss: 3.2976 - kl_loss: 5.7345 - lr: 0.0100\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.3861 - reconstruction_loss: 3.3865 - kl_loss: 5.7363 - lr: 0.0100\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.7242 - reconstruction_loss: 3.5934 - kl_loss: 5.7896 - lr: 0.0100\n",
      "Epoch 12/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 4.7383 - reconstruction_loss: 3.3705 - kl_loss: 5.8957\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.7344 - reconstruction_loss: 3.3653 - kl_loss: 5.8933 - lr: 0.0100\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.4272 - reconstruction_loss: 3.0409 - kl_loss: 5.8289 - lr: 0.0050\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.9937 - reconstruction_loss: 2.8160 - kl_loss: 5.8421 - lr: 0.0050\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.9692 - reconstruction_loss: 2.7895 - kl_loss: 5.8538 - lr: 0.0050\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.9063 - reconstruction_loss: 2.7237 - kl_loss: 5.8259 - lr: 0.0050\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.9161 - reconstruction_loss: 2.7919 - kl_loss: 5.8450 - lr: 0.0050\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.0793 - reconstruction_loss: 2.9988 - kl_loss: 5.8323 - lr: 0.0050\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.8917 - reconstruction_loss: 2.7175 - kl_loss: 5.7962 - lr: 0.0050\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 4.0061 - reconstruction_loss: 2.8300 - kl_loss: 5.8729 - lr: 0.0050\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.9313 - reconstruction_loss: 2.7804 - kl_loss: 5.7794 - lr: 0.0050\n",
      "Epoch 22/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.8958 - reconstruction_loss: 2.8704 - kl_loss: 5.8411\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.8986 - reconstruction_loss: 2.8632 - kl_loss: 5.8372 - lr: 0.0050\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.9131 - reconstruction_loss: 2.6867 - kl_loss: 5.8124 - lr: 0.0025\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.7671 - reconstruction_loss: 2.5813 - kl_loss: 5.8035 - lr: 0.0025\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.7328 - reconstruction_loss: 2.5958 - kl_loss: 5.7967 - lr: 0.0025\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.8282 - reconstruction_loss: 2.6515 - kl_loss: 5.8373 - lr: 0.0025\n",
      "Epoch 27/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.7978 - reconstruction_loss: 2.6021 - kl_loss: 5.8047\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.7970 - reconstruction_loss: 2.5934 - kl_loss: 5.8055 - lr: 0.0025\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.7112 - reconstruction_loss: 2.5400 - kl_loss: 5.8245 - lr: 0.0012\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6950 - reconstruction_loss: 2.5184 - kl_loss: 5.8092 - lr: 0.0012\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6861 - reconstruction_loss: 2.5341 - kl_loss: 5.8058 - lr: 0.0012\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.7082 - reconstruction_loss: 2.5266 - kl_loss: 5.8088 - lr: 0.0012\n",
      "Epoch 32/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.7140 - reconstruction_loss: 2.5349 - kl_loss: 5.8272\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.7137 - reconstruction_loss: 2.5272 - kl_loss: 5.8272 - lr: 0.0012\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.7154 - reconstruction_loss: 2.5133 - kl_loss: 5.8003 - lr: 6.2500e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6530 - reconstruction_loss: 2.4813 - kl_loss: 5.8200 - lr: 6.2500e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6236 - reconstruction_loss: 2.4710 - kl_loss: 5.8073 - lr: 6.2500e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6633 - reconstruction_loss: 2.4878 - kl_loss: 5.8150 - lr: 6.2500e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.6326 - reconstruction_loss: 2.4533 - kl_loss: 5.8178 - lr: 6.2500e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6129 - reconstruction_loss: 2.4546 - kl_loss: 5.7633 - lr: 6.2500e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6094 - reconstruction_loss: 2.4398 - kl_loss: 5.7567 - lr: 6.2500e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6051 - reconstruction_loss: 2.4493 - kl_loss: 5.7658 - lr: 6.2500e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.6043 - reconstruction_loss: 2.4415 - kl_loss: 5.7704 - lr: 6.2500e-04\n",
      "Epoch 42/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.6141 - reconstruction_loss: 2.4539 - kl_loss: 5.7528\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6139 - reconstruction_loss: 2.4453 - kl_loss: 5.7523 - lr: 6.2500e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.6011 - reconstruction_loss: 2.4356 - kl_loss: 5.7550 - lr: 3.1250e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 3.5815 - reconstruction_loss: 2.4316 - kl_loss: 5.7410 - lr: 3.1250e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5931 - reconstruction_loss: 2.4231 - kl_loss: 5.7597 - lr: 3.1250e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5639 - reconstruction_loss: 2.4098 - kl_loss: 5.7604 - lr: 3.1250e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5632 - reconstruction_loss: 2.4167 - kl_loss: 5.7586 - lr: 3.1250e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5647 - reconstruction_loss: 2.4155 - kl_loss: 5.7615 - lr: 3.1250e-04\n",
      "Epoch 49/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5662 - reconstruction_loss: 2.4278 - kl_loss: 5.7503\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5663 - reconstruction_loss: 2.4190 - kl_loss: 5.7502 - lr: 3.1250e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5510 - reconstruction_loss: 2.4038 - kl_loss: 5.7390 - lr: 1.5625e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5565 - reconstruction_loss: 2.4024 - kl_loss: 5.7163 - lr: 1.5625e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5541 - reconstruction_loss: 2.4020 - kl_loss: 5.7261 - lr: 1.5625e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5574 - reconstruction_loss: 2.3954 - kl_loss: 5.7303 - lr: 1.5625e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5513 - reconstruction_loss: 2.3982 - kl_loss: 5.7316 - lr: 1.5625e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5450 - reconstruction_loss: 2.3888 - kl_loss: 5.7432 - lr: 1.5625e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5283 - reconstruction_loss: 2.3928 - kl_loss: 5.7279 - lr: 1.5625e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5403 - reconstruction_loss: 2.3952 - kl_loss: 5.7242 - lr: 1.5625e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5361 - reconstruction_loss: 2.3911 - kl_loss: 5.7154 - lr: 1.5625e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5455 - reconstruction_loss: 2.3924 - kl_loss: 5.7155 - lr: 1.5625e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5389 - reconstruction_loss: 2.3898 - kl_loss: 5.7179 - lr: 1.5625e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5444 - reconstruction_loss: 2.3971 - kl_loss: 5.7278 - lr: 1.5625e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5287 - reconstruction_loss: 2.3875 - kl_loss: 5.7166 - lr: 1.5625e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5397 - reconstruction_loss: 2.3862 - kl_loss: 5.7202 - lr: 1.5625e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5424 - reconstruction_loss: 2.3947 - kl_loss: 5.7166 - lr: 1.5625e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5488 - reconstruction_loss: 2.3873 - kl_loss: 5.7123 - lr: 1.5625e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5355 - reconstruction_loss: 2.3867 - kl_loss: 5.7136 - lr: 1.5625e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5422 - reconstruction_loss: 2.3928 - kl_loss: 5.7196 - lr: 1.5625e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5392 - reconstruction_loss: 2.3875 - kl_loss: 5.7022 - lr: 1.5625e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5371 - reconstruction_loss: 2.3880 - kl_loss: 5.7077 - lr: 1.5625e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5390 - reconstruction_loss: 2.3916 - kl_loss: 5.7050 - lr: 1.5625e-04\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5239 - reconstruction_loss: 2.3888 - kl_loss: 5.7129\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5239 - reconstruction_loss: 2.3888 - kl_loss: 5.7129 - lr: 1.5625e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5185 - reconstruction_loss: 2.3816 - kl_loss: 5.7103 - lr: 7.8125e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5470 - reconstruction_loss: 2.3781 - kl_loss: 5.7144 - lr: 7.8125e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5345 - reconstruction_loss: 2.3791 - kl_loss: 5.7046 - lr: 7.8125e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5214 - reconstruction_loss: 2.3775 - kl_loss: 5.7032 - lr: 7.8125e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5240 - reconstruction_loss: 2.3781 - kl_loss: 5.7057 - lr: 7.8125e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5246 - reconstruction_loss: 2.3786 - kl_loss: 5.7035 - lr: 7.8125e-05\n",
      "Epoch 78/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5273 - reconstruction_loss: 2.3939 - kl_loss: 5.7011\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5274 - reconstruction_loss: 2.3854 - kl_loss: 5.7008 - lr: 7.8125e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5343 - reconstruction_loss: 2.3774 - kl_loss: 5.7092 - lr: 3.9062e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5301 - reconstruction_loss: 2.3745 - kl_loss: 5.7044 - lr: 3.9062e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5286 - reconstruction_loss: 2.3781 - kl_loss: 5.7052 - lr: 3.9062e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5304 - reconstruction_loss: 2.3730 - kl_loss: 5.7107 - lr: 3.9062e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5149 - reconstruction_loss: 2.3702 - kl_loss: 5.7055 - lr: 3.9062e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5188 - reconstruction_loss: 2.3700 - kl_loss: 5.7009 - lr: 3.9062e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5296 - reconstruction_loss: 2.3709 - kl_loss: 5.6940 - lr: 3.9062e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5122 - reconstruction_loss: 2.3720 - kl_loss: 5.6907 - lr: 3.9062e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5195 - reconstruction_loss: 2.3703 - kl_loss: 5.7000 - lr: 3.9062e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5187 - reconstruction_loss: 2.3688 - kl_loss: 5.6973 - lr: 3.9062e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5172 - reconstruction_loss: 2.3720 - kl_loss: 5.6998 - lr: 3.9062e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5190 - reconstruction_loss: 2.3709 - kl_loss: 5.7049 - lr: 3.9062e-05\n",
      "Epoch 91/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5068 - reconstruction_loss: 2.3784 - kl_loss: 5.7021\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5069 - reconstruction_loss: 2.3688 - kl_loss: 5.7019 - lr: 3.9062e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5163 - reconstruction_loss: 2.3671 - kl_loss: 5.7029 - lr: 1.9531e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 3.5132 - reconstruction_loss: 2.3666 - kl_loss: 5.7005 - lr: 1.9531e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5075 - reconstruction_loss: 2.3661 - kl_loss: 5.7013 - lr: 1.9531e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5064 - reconstruction_loss: 2.3664 - kl_loss: 5.6959 - lr: 1.9531e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5009 - reconstruction_loss: 2.3637 - kl_loss: 5.6959 - lr: 1.9531e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5157 - reconstruction_loss: 2.3653 - kl_loss: 5.6897 - lr: 1.9531e-05\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5059 - reconstruction_loss: 2.3666 - kl_loss: 5.6927 - lr: 1.9531e-05\n",
      "Epoch 99/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5058 - reconstruction_loss: 2.3778 - kl_loss: 5.6978\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5059 - reconstruction_loss: 2.3680 - kl_loss: 5.6985 - lr: 1.9531e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5194 - reconstruction_loss: 2.3643 - kl_loss: 5.6974 - lr: 9.7656e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5146 - reconstruction_loss: 2.3658 - kl_loss: 5.6989 - lr: 9.7656e-06\n",
      "Epoch 102/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4969 - reconstruction_loss: 2.3730 - kl_loss: 5.6995\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4971 - reconstruction_loss: 2.3634 - kl_loss: 5.6995 - lr: 9.7656e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5126 - reconstruction_loss: 2.3622 - kl_loss: 5.6971 - lr: 4.8828e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5037 - reconstruction_loss: 2.3635 - kl_loss: 5.6973 - lr: 4.8828e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5021 - reconstruction_loss: 2.3598 - kl_loss: 5.6985 - lr: 4.8828e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.4981 - reconstruction_loss: 2.3634 - kl_loss: 5.6985 - lr: 4.8828e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5038 - reconstruction_loss: 2.3636 - kl_loss: 5.7001 - lr: 4.8828e-06\n",
      "Epoch 108/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5165 - reconstruction_loss: 2.3741 - kl_loss: 5.7000\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5164 - reconstruction_loss: 2.3656 - kl_loss: 5.6995 - lr: 4.8828e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5087 - reconstruction_loss: 2.3609 - kl_loss: 5.7003 - lr: 2.4414e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5274 - reconstruction_loss: 2.3636 - kl_loss: 5.6990 - lr: 2.4414e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5164 - reconstruction_loss: 2.3650 - kl_loss: 5.6986\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5163 - reconstruction_loss: 2.3650 - kl_loss: 5.6986 - lr: 2.4414e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5034 - reconstruction_loss: 2.3601 - kl_loss: 5.6993 - lr: 1.2207e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5040 - reconstruction_loss: 2.3622 - kl_loss: 5.6987 - lr: 1.2207e-06\n",
      "Epoch 114/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5130 - reconstruction_loss: 2.3695 - kl_loss: 5.6990\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5128 - reconstruction_loss: 2.3610 - kl_loss: 5.6985 - lr: 1.2207e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 3.5082 - reconstruction_loss: 2.3634 - kl_loss: 5.6978 - lr: 1.0000e-06\n",
      "Epoch 115: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 17:29:20,024]\u001b[0m Trial 94 finished with value: 0.004632812529088553 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 20, 'encoder_units_l2': 776, 'encoder_units_l3': 142, 'decoder_layers': 5, 'decoder_units_l1': 101, 'decoder_units_l2': 50, 'decoder_units_l3': 758, 'decoder_units_l4': 34, 'decoder_units_l5': 386, 'beta': 0.2, 'lr': 0.01, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 11ms/step - loss: 16.8129 - reconstruction_loss: 13.4818 - kl_loss: 2.4190 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 12.0292 - reconstruction_loss: 10.1902 - kl_loss: 3.3214 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.9169 - reconstruction_loss: 6.5778 - kl_loss: 4.4785 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 5.7427 - reconstruction_loss: 4.4103 - kl_loss: 5.1489 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 5.0668 - reconstruction_loss: 3.9774 - kl_loss: 5.3234 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 5.1082 - reconstruction_loss: 3.8136 - kl_loss: 5.4005 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.7389 - reconstruction_loss: 3.6488 - kl_loss: 5.4724 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.8748 - reconstruction_loss: 3.6332 - kl_loss: 5.5203 - lr: 0.0100\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.5821 - reconstruction_loss: 3.4814 - kl_loss: 5.5760 - lr: 0.0100\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.5419 - reconstruction_loss: 3.2334 - kl_loss: 5.6575 - lr: 0.0100\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.3520 - reconstruction_loss: 3.1663 - kl_loss: 5.7319 - lr: 0.0100\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.4084 - reconstruction_loss: 3.2958 - kl_loss: 5.7820 - lr: 0.0100\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.3141 - reconstruction_loss: 3.2236 - kl_loss: 5.9059 - lr: 0.0100\n",
      "Epoch 14/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 4.9020 - reconstruction_loss: 3.2631 - kl_loss: 5.8083\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.8897 - reconstruction_loss: 3.2491 - kl_loss: 5.8059 - lr: 0.0100\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9441 - reconstruction_loss: 2.6737 - kl_loss: 5.8293 - lr: 0.0050\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7353 - reconstruction_loss: 2.6101 - kl_loss: 5.7564 - lr: 0.0050\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7671 - reconstruction_loss: 2.6588 - kl_loss: 5.8212 - lr: 0.0050\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.0960 - reconstruction_loss: 2.7444 - kl_loss: 5.8735 - lr: 0.0050\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7886 - reconstruction_loss: 2.5906 - kl_loss: 5.8145 - lr: 0.0050\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.1500 - reconstruction_loss: 2.7844 - kl_loss: 5.8306 - lr: 0.0050\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9407 - reconstruction_loss: 2.9884 - kl_loss: 5.9685 - lr: 0.0050\n",
      "Epoch 22/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 4.0023 - reconstruction_loss: 2.7707 - kl_loss: 6.0352\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.0020 - reconstruction_loss: 2.7621 - kl_loss: 6.0342 - lr: 0.0050\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9261 - reconstruction_loss: 2.6809 - kl_loss: 5.9884 - lr: 0.0025\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8075 - reconstruction_loss: 2.5726 - kl_loss: 6.0045 - lr: 0.0025\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7464 - reconstruction_loss: 2.5549 - kl_loss: 5.9329 - lr: 0.0025\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7335 - reconstruction_loss: 2.5787 - kl_loss: 5.9491 - lr: 0.0025\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7291 - reconstruction_loss: 2.5863 - kl_loss: 5.9180 - lr: 0.0025\n",
      "Epoch 28/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.9079 - reconstruction_loss: 2.6899 - kl_loss: 6.0162\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9072 - reconstruction_loss: 2.6749 - kl_loss: 6.0152 - lr: 0.0025\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7127 - reconstruction_loss: 2.4908 - kl_loss: 5.9548 - lr: 0.0012\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7010 - reconstruction_loss: 2.5049 - kl_loss: 5.9739 - lr: 0.0012\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6810 - reconstruction_loss: 2.4916 - kl_loss: 5.9069 - lr: 0.0012\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6280 - reconstruction_loss: 2.4397 - kl_loss: 5.8355 - lr: 0.0012\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5920 - reconstruction_loss: 2.4203 - kl_loss: 5.8058 - lr: 0.0012\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5915 - reconstruction_loss: 2.4258 - kl_loss: 5.7835 - lr: 0.0012\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5906 - reconstruction_loss: 2.4386 - kl_loss: 5.8029 - lr: 0.0012\n",
      "Epoch 36/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.5877 - reconstruction_loss: 2.4528 - kl_loss: 5.8043\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5884 - reconstruction_loss: 2.4447 - kl_loss: 5.7993 - lr: 0.0012\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5712 - reconstruction_loss: 2.4066 - kl_loss: 5.7878 - lr: 6.2500e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5620 - reconstruction_loss: 2.3954 - kl_loss: 5.7854 - lr: 6.2500e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5578 - reconstruction_loss: 2.4035 - kl_loss: 5.7795 - lr: 6.2500e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5557 - reconstruction_loss: 2.3938 - kl_loss: 5.7685 - lr: 6.2500e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5631 - reconstruction_loss: 2.3915 - kl_loss: 5.7242 - lr: 6.2500e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5251 - reconstruction_loss: 2.3844 - kl_loss: 5.7178 - lr: 6.2500e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5201 - reconstruction_loss: 2.3783 - kl_loss: 5.7024 - lr: 6.2500e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5335 - reconstruction_loss: 2.3809 - kl_loss: 5.6995 - lr: 6.2500e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5176 - reconstruction_loss: 2.3749 - kl_loss: 5.7085 - lr: 6.2500e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5116 - reconstruction_loss: 2.3696 - kl_loss: 5.6761 - lr: 6.2500e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5087 - reconstruction_loss: 2.3666 - kl_loss: 5.6756 - lr: 6.2500e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5176 - reconstruction_loss: 2.3748 - kl_loss: 5.6722 - lr: 6.2500e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5500 - reconstruction_loss: 2.3916 - kl_loss: 5.7076 - lr: 6.2500e-04\n",
      "Epoch 50/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.5171 - reconstruction_loss: 2.3898 - kl_loss: 5.7030\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5172 - reconstruction_loss: 2.3814 - kl_loss: 5.7030 - lr: 6.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5281 - reconstruction_loss: 2.3773 - kl_loss: 5.6993 - lr: 3.1250e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5127 - reconstruction_loss: 2.3661 - kl_loss: 5.6664 - lr: 3.1250e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5216 - reconstruction_loss: 2.3695 - kl_loss: 5.6669 - lr: 3.1250e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4873 - reconstruction_loss: 2.3634 - kl_loss: 5.6543 - lr: 3.1250e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4987 - reconstruction_loss: 2.3628 - kl_loss: 5.6758 - lr: 3.1250e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4875 - reconstruction_loss: 2.3555 - kl_loss: 5.6666 - lr: 3.1250e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5137 - reconstruction_loss: 2.3606 - kl_loss: 5.6774 - lr: 3.1250e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5006 - reconstruction_loss: 2.3594 - kl_loss: 5.6615 - lr: 3.1250e-04\n",
      "Epoch 59/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4973 - reconstruction_loss: 2.3663 - kl_loss: 5.6630\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4972 - reconstruction_loss: 2.3584 - kl_loss: 5.6629 - lr: 3.1250e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4981 - reconstruction_loss: 2.3549 - kl_loss: 5.6569 - lr: 1.5625e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5041 - reconstruction_loss: 2.3563 - kl_loss: 5.6545 - lr: 1.5625e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5079 - reconstruction_loss: 2.3703 - kl_loss: 5.6837 - lr: 1.5625e-04\n",
      "Epoch 63/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.4940 - reconstruction_loss: 2.3678 - kl_loss: 5.6727\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4942 - reconstruction_loss: 2.3614 - kl_loss: 5.6724 - lr: 1.5625e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4971 - reconstruction_loss: 2.3514 - kl_loss: 5.6774 - lr: 7.8125e-05\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4862 - reconstruction_loss: 2.3482 - kl_loss: 5.6692 - lr: 7.8125e-05\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4870 - reconstruction_loss: 2.3526 - kl_loss: 5.6587 - lr: 7.8125e-05\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4945 - reconstruction_loss: 2.3534 - kl_loss: 5.6494 - lr: 7.8125e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4904 - reconstruction_loss: 2.3495 - kl_loss: 5.6540 - lr: 7.8125e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4948 - reconstruction_loss: 2.3486 - kl_loss: 5.6534 - lr: 7.8125e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4758 - reconstruction_loss: 2.3476 - kl_loss: 5.6471 - lr: 7.8125e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4806 - reconstruction_loss: 2.3481 - kl_loss: 5.6372 - lr: 7.8125e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4773 - reconstruction_loss: 2.3486 - kl_loss: 5.6352 - lr: 7.8125e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4699 - reconstruction_loss: 2.3484 - kl_loss: 5.6335 - lr: 7.8125e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4789 - reconstruction_loss: 2.3470 - kl_loss: 5.6360 - lr: 7.8125e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4848 - reconstruction_loss: 2.3471 - kl_loss: 5.6314 - lr: 7.8125e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.4695 - reconstruction_loss: 2.3452 - kl_loss: 5.6228 - lr: 7.8125e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4886 - reconstruction_loss: 2.3458 - kl_loss: 5.6444 - lr: 7.8125e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4837 - reconstruction_loss: 2.3393 - kl_loss: 5.6396 - lr: 7.8125e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4730 - reconstruction_loss: 2.3450 - kl_loss: 5.6281 - lr: 7.8125e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4743 - reconstruction_loss: 2.3419 - kl_loss: 5.6239 - lr: 7.8125e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4797 - reconstruction_loss: 2.3393 - kl_loss: 5.6315 - lr: 7.8125e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4702 - reconstruction_loss: 2.3386 - kl_loss: 5.6334 - lr: 7.8125e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4815 - reconstruction_loss: 2.3415 - kl_loss: 5.6278 - lr: 7.8125e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4700 - reconstruction_loss: 2.3418 - kl_loss: 5.6299 - lr: 7.8125e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4755 - reconstruction_loss: 2.3368 - kl_loss: 5.6385 - lr: 7.8125e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4757 - reconstruction_loss: 2.3402 - kl_loss: 5.6377 - lr: 7.8125e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4637 - reconstruction_loss: 2.3375 - kl_loss: 5.6240 - lr: 7.8125e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4693 - reconstruction_loss: 2.3410 - kl_loss: 5.6135 - lr: 7.8125e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4555 - reconstruction_loss: 2.3344 - kl_loss: 5.6237 - lr: 7.8125e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4745 - reconstruction_loss: 2.3412 - kl_loss: 5.6160 - lr: 7.8125e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4740 - reconstruction_loss: 2.3369 - kl_loss: 5.6396 - lr: 7.8125e-05\n",
      "Epoch 92/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4719 - reconstruction_loss: 2.3498 - kl_loss: 5.6194\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4718 - reconstruction_loss: 2.3412 - kl_loss: 5.6194 - lr: 7.8125e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4652 - reconstruction_loss: 2.3378 - kl_loss: 5.6221 - lr: 3.9062e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4714 - reconstruction_loss: 2.3347 - kl_loss: 5.6289 - lr: 3.9062e-05\n",
      "Epoch 95/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4686 - reconstruction_loss: 2.3449 - kl_loss: 5.6227\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4685 - reconstruction_loss: 2.3358 - kl_loss: 5.6233 - lr: 3.9062e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4701 - reconstruction_loss: 2.3315 - kl_loss: 5.6205 - lr: 1.9531e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4648 - reconstruction_loss: 2.3318 - kl_loss: 5.6179 - lr: 1.9531e-05\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4733 - reconstruction_loss: 2.3324 - kl_loss: 5.6183 - lr: 1.9531e-05\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4564 - reconstruction_loss: 2.3338 - kl_loss: 5.6177 - lr: 1.9531e-05\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4605 - reconstruction_loss: 2.3319 - kl_loss: 5.6245\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4605 - reconstruction_loss: 2.3319 - kl_loss: 5.6245 - lr: 1.9531e-05\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4690 - reconstruction_loss: 2.3312 - kl_loss: 5.6242 - lr: 9.7656e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4648 - reconstruction_loss: 2.3302 - kl_loss: 5.6248 - lr: 9.7656e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4619 - reconstruction_loss: 2.3282 - kl_loss: 5.6206 - lr: 9.7656e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4634 - reconstruction_loss: 2.3329 - kl_loss: 5.6192 - lr: 9.7656e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4607 - reconstruction_loss: 2.3289 - kl_loss: 5.6145 - lr: 9.7656e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4655 - reconstruction_loss: 2.3303 - kl_loss: 5.6139 - lr: 9.7656e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4620 - reconstruction_loss: 2.3315 - kl_loss: 5.6152 - lr: 9.7656e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4781 - reconstruction_loss: 2.3299 - kl_loss: 5.6200\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4779 - reconstruction_loss: 2.3299 - kl_loss: 5.6200 - lr: 9.7656e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4614 - reconstruction_loss: 2.3291 - kl_loss: 5.6192 - lr: 4.8828e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4481 - reconstruction_loss: 2.3304 - kl_loss: 5.6157 - lr: 4.8828e-06\n",
      "Epoch 111/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4530 - reconstruction_loss: 2.3376 - kl_loss: 5.6154\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4531 - reconstruction_loss: 2.3289 - kl_loss: 5.6160 - lr: 4.8828e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4746 - reconstruction_loss: 2.3300 - kl_loss: 5.6177 - lr: 2.4414e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4565 - reconstruction_loss: 2.3305 - kl_loss: 5.6179 - lr: 2.4414e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4631 - reconstruction_loss: 2.3262 - kl_loss: 5.6187 - lr: 2.4414e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4612 - reconstruction_loss: 2.3266 - kl_loss: 5.6178 - lr: 2.4414e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4622 - reconstruction_loss: 2.3294 - kl_loss: 5.6167 - lr: 2.4414e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4519 - reconstruction_loss: 2.3292 - kl_loss: 5.6160\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4519 - reconstruction_loss: 2.3292 - kl_loss: 5.6160 - lr: 2.4414e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4684 - reconstruction_loss: 2.3295 - kl_loss: 5.6147 - lr: 1.2207e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4582 - reconstruction_loss: 2.3302 - kl_loss: 5.6150 - lr: 1.2207e-06\n",
      "Epoch 120/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4600 - reconstruction_loss: 2.3373 - kl_loss: 5.6157\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4600 - reconstruction_loss: 2.3292 - kl_loss: 5.6161 - lr: 1.2207e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4713 - reconstruction_loss: 2.3301 - kl_loss: 5.6172 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4566 - reconstruction_loss: 2.3282 - kl_loss: 5.6171 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4651 - reconstruction_loss: 2.3297 - kl_loss: 5.6169 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4556 - reconstruction_loss: 2.3303 - kl_loss: 5.6167 - lr: 1.0000e-06\n",
      "Epoch 124: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 17:33:48,067]\u001b[0m Trial 95 finished with value: 0.0045608283067451036 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 24, 'encoder_units_l2': 641, 'encoder_units_l3': 63, 'decoder_layers': 4, 'decoder_units_l1': 136, 'decoder_units_l2': 22, 'decoder_units_l3': 570, 'decoder_units_l4': 72, 'beta': 0.2, 'lr': 0.01, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6687 - reconstruction_loss: 1.1252 - kl_loss: 2.0393 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3533 - reconstruction_loss: 0.8002 - kl_loss: 2.5544 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2614 - reconstruction_loss: 0.7265 - kl_loss: 2.6532 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2421 - reconstruction_loss: 0.7020 - kl_loss: 2.7003 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2314 - reconstruction_loss: 0.6818 - kl_loss: 2.7448 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2290 - reconstruction_loss: 0.6757 - kl_loss: 2.7689 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2207 - reconstruction_loss: 0.6591 - kl_loss: 2.8016 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2183 - reconstruction_loss: 0.6531 - kl_loss: 2.8213 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2111 - reconstruction_loss: 0.6431 - kl_loss: 2.8370 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2099 - reconstruction_loss: 0.6423 - kl_loss: 2.8520 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2067 - reconstruction_loss: 0.6315 - kl_loss: 2.8730 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2037 - reconstruction_loss: 0.6262 - kl_loss: 2.8946 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2050 - reconstruction_loss: 0.6232 - kl_loss: 2.9000 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2006 - reconstruction_loss: 0.6231 - kl_loss: 2.9138 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2094 - reconstruction_loss: 0.6162 - kl_loss: 2.9339 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1987 - reconstruction_loss: 0.6031 - kl_loss: 2.9641 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1934 - reconstruction_loss: 0.5973 - kl_loss: 2.9790 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1952 - reconstruction_loss: 0.5882 - kl_loss: 3.0206 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1929 - reconstruction_loss: 0.5868 - kl_loss: 3.0216 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1873 - reconstruction_loss: 0.5795 - kl_loss: 3.0439 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1865 - reconstruction_loss: 0.5779 - kl_loss: 3.0530 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1890 - reconstruction_loss: 0.5738 - kl_loss: 3.0700 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1869 - reconstruction_loss: 0.5704 - kl_loss: 3.0786 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1824 - reconstruction_loss: 0.5631 - kl_loss: 3.0928 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1809 - reconstruction_loss: 0.5630 - kl_loss: 3.0964 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1861 - reconstruction_loss: 0.5616 - kl_loss: 3.1136 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1813 - reconstruction_loss: 0.5554 - kl_loss: 3.1249 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1804 - reconstruction_loss: 0.5536 - kl_loss: 3.1391 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1797 - reconstruction_loss: 0.5546 - kl_loss: 3.1291 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1816 - reconstruction_loss: 0.5471 - kl_loss: 3.1614 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1798 - reconstruction_loss: 0.5446 - kl_loss: 3.1623 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1780 - reconstruction_loss: 0.5495 - kl_loss: 3.1548 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2019 - reconstruction_loss: 0.5604 - kl_loss: 3.1651 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1738 - reconstruction_loss: 0.5406 - kl_loss: 3.1659 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1757 - reconstruction_loss: 0.5419 - kl_loss: 3.1798 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1721 - reconstruction_loss: 0.5369 - kl_loss: 3.1760 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1766 - reconstruction_loss: 0.5398 - kl_loss: 3.1852 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1753 - reconstruction_loss: 0.5371 - kl_loss: 3.1856 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 1.1744 - reconstruction_loss: 0.5385 - kl_loss: 3.1766\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1744 - reconstruction_loss: 0.5385 - kl_loss: 3.1766 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1654 - reconstruction_loss: 0.5260 - kl_loss: 3.1896 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1614 - reconstruction_loss: 0.5163 - kl_loss: 3.2146 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1601 - reconstruction_loss: 0.5189 - kl_loss: 3.2052 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1556 - reconstruction_loss: 0.5175 - kl_loss: 3.1981 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1575 - reconstruction_loss: 0.5143 - kl_loss: 3.2073 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1592 - reconstruction_loss: 0.5149 - kl_loss: 3.2079 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1542 - reconstruction_loss: 0.5121 - kl_loss: 3.2186 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "1551/1563 [============================>.] - ETA: 0s - loss: 1.1568 - reconstruction_loss: 0.5145 - kl_loss: 3.2214\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1568 - reconstruction_loss: 0.5145 - kl_loss: 3.2213 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1540 - reconstruction_loss: 0.5103 - kl_loss: 3.2069 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1524 - reconstruction_loss: 0.5082 - kl_loss: 3.2279 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1524 - reconstruction_loss: 0.5061 - kl_loss: 3.2253 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1504 - reconstruction_loss: 0.5082 - kl_loss: 3.2092 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1495 - reconstruction_loss: 0.5048 - kl_loss: 3.2250 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1498 - reconstruction_loss: 0.5069 - kl_loss: 3.2214 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1490 - reconstruction_loss: 0.5030 - kl_loss: 3.2313 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1507 - reconstruction_loss: 0.5033 - kl_loss: 3.2332 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1535 - reconstruction_loss: 0.5069 - kl_loss: 3.2328 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "1555/1563 [============================>.] - ETA: 0s - loss: 1.1625 - reconstruction_loss: 0.5076 - kl_loss: 3.2428\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1624 - reconstruction_loss: 0.5074 - kl_loss: 3.2428 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1504 - reconstruction_loss: 0.5035 - kl_loss: 3.2447 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1472 - reconstruction_loss: 0.5000 - kl_loss: 3.2446 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1515 - reconstruction_loss: 0.5012 - kl_loss: 3.2419 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1489 - reconstruction_loss: 0.5010 - kl_loss: 3.2357 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1465 - reconstruction_loss: 0.5020 - kl_loss: 3.2276 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1518 - reconstruction_loss: 0.5010 - kl_loss: 3.2427 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1443 - reconstruction_loss: 0.4994 - kl_loss: 3.2254 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1426 - reconstruction_loss: 0.4989 - kl_loss: 3.2310 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1471 - reconstruction_loss: 0.4980 - kl_loss: 3.2323 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 1.1430 - reconstruction_loss: 0.4969 - kl_loss: 3.2391\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1430 - reconstruction_loss: 0.4967 - kl_loss: 3.2393 - lr: 1.2500e-04\n",
      "Epoch 68/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1443 - reconstruction_loss: 0.4953 - kl_loss: 3.2407 - lr: 6.2500e-05\n",
      "Epoch 69/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1420 - reconstruction_loss: 0.4950 - kl_loss: 3.2380 - lr: 6.2500e-05\n",
      "Epoch 70/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1438 - reconstruction_loss: 0.4944 - kl_loss: 3.2443 - lr: 6.2500e-05\n",
      "Epoch 71/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1428 - reconstruction_loss: 0.4929 - kl_loss: 3.2443 - lr: 6.2500e-05\n",
      "Epoch 72/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1445 - reconstruction_loss: 0.4934 - kl_loss: 3.2437 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1420 - reconstruction_loss: 0.4946 - kl_loss: 3.2430 - lr: 6.2500e-05\n",
      "Epoch 74/300\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 1.1437 - reconstruction_loss: 0.4948 - kl_loss: 3.2415\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1437 - reconstruction_loss: 0.4946 - kl_loss: 3.2414 - lr: 6.2500e-05\n",
      "Epoch 75/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1410 - reconstruction_loss: 0.4937 - kl_loss: 3.2473 - lr: 3.1250e-05\n",
      "Epoch 76/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1423 - reconstruction_loss: 0.4924 - kl_loss: 3.2438 - lr: 3.1250e-05\n",
      "Epoch 77/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1422 - reconstruction_loss: 0.4932 - kl_loss: 3.2459 - lr: 3.1250e-05\n",
      "Epoch 78/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1427 - reconstruction_loss: 0.4910 - kl_loss: 3.2509 - lr: 3.1250e-05\n",
      "Epoch 79/300\n",
      "1550/1563 [============================>.] - ETA: 0s - loss: 1.1422 - reconstruction_loss: 0.4916 - kl_loss: 3.2508\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1421 - reconstruction_loss: 0.4913 - kl_loss: 3.2508 - lr: 3.1250e-05\n",
      "Epoch 80/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1409 - reconstruction_loss: 0.4915 - kl_loss: 3.2446 - lr: 1.5625e-05\n",
      "Epoch 81/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1396 - reconstruction_loss: 0.4912 - kl_loss: 3.2476 - lr: 1.5625e-05\n",
      "Epoch 82/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1429 - reconstruction_loss: 0.4917 - kl_loss: 3.2473 - lr: 1.5625e-05\n",
      "Epoch 83/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1397 - reconstruction_loss: 0.4913 - kl_loss: 3.2433 - lr: 1.5625e-05\n",
      "Epoch 84/300\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.1413 - reconstruction_loss: 0.4907 - kl_loss: 3.2422 - lr: 1.5625e-05\n",
      "Epoch 85/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1409 - reconstruction_loss: 0.4918 - kl_loss: 3.2387 - lr: 1.5625e-05\n",
      "Epoch 86/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1404 - reconstruction_loss: 0.4925 - kl_loss: 3.2404 - lr: 1.5625e-05\n",
      "Epoch 87/300\n",
      "1549/1563 [============================>.] - ETA: 0s - loss: 1.1410 - reconstruction_loss: 0.4905 - kl_loss: 3.2507\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1410 - reconstruction_loss: 0.4904 - kl_loss: 3.2508 - lr: 1.5625e-05\n",
      "Epoch 88/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1379 - reconstruction_loss: 0.4893 - kl_loss: 3.2501 - lr: 7.8125e-06\n",
      "Epoch 89/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1407 - reconstruction_loss: 0.4897 - kl_loss: 3.2504 - lr: 7.8125e-06\n",
      "Epoch 90/300\n",
      "1553/1563 [============================>.] - ETA: 0s - loss: 1.1388 - reconstruction_loss: 0.4898 - kl_loss: 3.2495\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1388 - reconstruction_loss: 0.4898 - kl_loss: 3.2493 - lr: 7.8125e-06\n",
      "Epoch 91/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1391 - reconstruction_loss: 0.4896 - kl_loss: 3.2472 - lr: 3.9063e-06\n",
      "Epoch 92/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1436 - reconstruction_loss: 0.4906 - kl_loss: 3.2493 - lr: 3.9063e-06\n",
      "Epoch 93/300\n",
      "1557/1563 [============================>.] - ETA: 0s - loss: 1.1373 - reconstruction_loss: 0.4905 - kl_loss: 3.2459\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1373 - reconstruction_loss: 0.4904 - kl_loss: 3.2454 - lr: 3.9063e-06\n",
      "Epoch 94/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1383 - reconstruction_loss: 0.4898 - kl_loss: 3.2433 - lr: 1.9531e-06\n",
      "Epoch 95/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1393 - reconstruction_loss: 0.4905 - kl_loss: 3.2453 - lr: 1.9531e-06\n",
      "Epoch 96/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1384 - reconstruction_loss: 0.4895 - kl_loss: 3.2446 - lr: 1.9531e-06\n",
      "Epoch 97/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1384 - reconstruction_loss: 0.4887 - kl_loss: 3.2451 - lr: 1.9531e-06\n",
      "Epoch 98/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1380 - reconstruction_loss: 0.4895 - kl_loss: 3.2445 - lr: 1.9531e-06\n",
      "Epoch 99/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1405 - reconstruction_loss: 0.4902 - kl_loss: 3.2468 - lr: 1.9531e-06\n",
      "Epoch 100/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 1.1392 - reconstruction_loss: 0.4903 - kl_loss: 3.2492\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1392 - reconstruction_loss: 0.4902 - kl_loss: 3.2491 - lr: 1.9531e-06\n",
      "Epoch 101/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1349 - reconstruction_loss: 0.4886 - kl_loss: 3.2500 - lr: 1.0000e-06\n",
      "Epoch 102/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1381 - reconstruction_loss: 0.4883 - kl_loss: 3.2501 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1398 - reconstruction_loss: 0.4878 - kl_loss: 3.2495 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1395 - reconstruction_loss: 0.4901 - kl_loss: 3.2492 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1386 - reconstruction_loss: 0.4883 - kl_loss: 3.2492 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1384 - reconstruction_loss: 0.4890 - kl_loss: 3.2485 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1382 - reconstruction_loss: 0.4892 - kl_loss: 3.2491 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1400 - reconstruction_loss: 0.4879 - kl_loss: 3.2484 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1414 - reconstruction_loss: 0.4900 - kl_loss: 3.2488 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1383 - reconstruction_loss: 0.4890 - kl_loss: 3.2494 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1383 - reconstruction_loss: 0.4892 - kl_loss: 3.2503 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1401 - reconstruction_loss: 0.4883 - kl_loss: 3.2507 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1386 - reconstruction_loss: 0.4888 - kl_loss: 3.2508 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1393 - reconstruction_loss: 0.4891 - kl_loss: 3.2499 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1374 - reconstruction_loss: 0.4896 - kl_loss: 3.2482 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1403 - reconstruction_loss: 0.4894 - kl_loss: 3.2487 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1394 - reconstruction_loss: 0.4899 - kl_loss: 3.2492 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1403 - reconstruction_loss: 0.4895 - kl_loss: 3.2495 - lr: 1.0000e-06\n",
      "Epoch 118: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 17:45:20,582]\u001b[0m Trial 96 finished with value: 0.007646088609731352 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 31, 'encoder_units_l2': 912, 'encoder_units_l3': 172, 'decoder_layers': 4, 'decoder_units_l1': 73, 'decoder_units_l2': 34, 'decoder_units_l3': 887, 'decoder_units_l4': 49, 'beta': 0.2, 'lr': 0.001, 'batch_size': 64}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 12.6657 - reconstruction_loss: 8.6829 - kl_loss: 3.9985 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 7.5093 - reconstruction_loss: 6.0483 - kl_loss: 4.6914 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 5.8010 - reconstruction_loss: 4.5878 - kl_loss: 5.1952 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 5.2828 - reconstruction_loss: 4.1234 - kl_loss: 5.3757 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.9930 - reconstruction_loss: 3.8349 - kl_loss: 5.5255 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.8135 - reconstruction_loss: 3.6410 - kl_loss: 5.6166 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.6770 - reconstruction_loss: 3.4904 - kl_loss: 5.6845 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.5514 - reconstruction_loss: 3.3605 - kl_loss: 5.7163 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.4122 - reconstruction_loss: 3.2546 - kl_loss: 5.7444 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.3117 - reconstruction_loss: 3.1355 - kl_loss: 5.7650 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.2568 - reconstruction_loss: 3.0441 - kl_loss: 5.7766 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.1761 - reconstruction_loss: 3.0082 - kl_loss: 5.8107 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 4.1340 - reconstruction_loss: 2.9318 - kl_loss: 5.8102 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 4.0467 - reconstruction_loss: 2.8492 - kl_loss: 5.8038 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 4.0280 - reconstruction_loss: 2.8701 - kl_loss: 5.8054 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.9419 - reconstruction_loss: 2.7527 - kl_loss: 5.7985 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.8648 - reconstruction_loss: 2.6944 - kl_loss: 5.8034 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8860 - reconstruction_loss: 2.7181 - kl_loss: 5.8103 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8557 - reconstruction_loss: 2.6975 - kl_loss: 5.7916 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8186 - reconstruction_loss: 2.6697 - kl_loss: 5.7833 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.8121 - reconstruction_loss: 2.6540 - kl_loss: 5.7846 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8086 - reconstruction_loss: 2.6429 - kl_loss: 5.7678 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.7760 - reconstruction_loss: 2.6198 - kl_loss: 5.7569 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7829 - reconstruction_loss: 2.6026 - kl_loss: 5.7553 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7718 - reconstruction_loss: 2.6251 - kl_loss: 5.7714 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8726 - reconstruction_loss: 2.6844 - kl_loss: 5.7967 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7105 - reconstruction_loss: 2.5513 - kl_loss: 5.7631 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8431 - reconstruction_loss: 2.6740 - kl_loss: 5.7992 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.8193 - reconstruction_loss: 2.6517 - kl_loss: 5.8021 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.7172 - reconstruction_loss: 2.5714 - kl_loss: 5.7328\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.7171 - reconstruction_loss: 2.5625 - kl_loss: 5.7320 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6522 - reconstruction_loss: 2.4968 - kl_loss: 5.7144 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6517 - reconstruction_loss: 2.4802 - kl_loss: 5.7209 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.6449 - reconstruction_loss: 2.4917 - kl_loss: 5.7205 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6505 - reconstruction_loss: 2.4796 - kl_loss: 5.7103 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6287 - reconstruction_loss: 2.4832 - kl_loss: 5.7164 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.6327 - reconstruction_loss: 2.4738 - kl_loss: 5.7053 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5895 - reconstruction_loss: 2.4587 - kl_loss: 5.6943 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.6233 - reconstruction_loss: 2.4937 - kl_loss: 5.7363 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6656 - reconstruction_loss: 2.4737 - kl_loss: 5.7376 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5974 - reconstruction_loss: 2.4534 - kl_loss: 5.6906 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.6201 - reconstruction_loss: 2.4553 - kl_loss: 5.7176 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.6536 - reconstruction_loss: 2.5262 - kl_loss: 5.7279 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.6527 - reconstruction_loss: 2.4957 - kl_loss: 5.7389\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.6527 - reconstruction_loss: 2.4957 - kl_loss: 5.7389 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.6165 - reconstruction_loss: 2.4679 - kl_loss: 5.7613 - lr: 2.5000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5817 - reconstruction_loss: 2.4124 - kl_loss: 5.7200 - lr: 2.5000e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5548 - reconstruction_loss: 2.4089 - kl_loss: 5.7017 - lr: 2.5000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5476 - reconstruction_loss: 2.4029 - kl_loss: 5.6941 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5221 - reconstruction_loss: 2.4015 - kl_loss: 5.6885 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5313 - reconstruction_loss: 2.3949 - kl_loss: 5.6792 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5524 - reconstruction_loss: 2.4026 - kl_loss: 5.7123 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5434 - reconstruction_loss: 2.3910 - kl_loss: 5.7107 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.5561 - reconstruction_loss: 2.4038 - kl_loss: 5.7099\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5558 - reconstruction_loss: 2.3955 - kl_loss: 5.7098 - lr: 2.5000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5320 - reconstruction_loss: 2.3912 - kl_loss: 5.7037 - lr: 1.2500e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5217 - reconstruction_loss: 2.3723 - kl_loss: 5.6766 - lr: 1.2500e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5201 - reconstruction_loss: 2.3680 - kl_loss: 5.6737 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5122 - reconstruction_loss: 2.3706 - kl_loss: 5.6815 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5139 - reconstruction_loss: 2.3727 - kl_loss: 5.6764 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5069 - reconstruction_loss: 2.3676 - kl_loss: 5.6728 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4970 - reconstruction_loss: 2.3711 - kl_loss: 5.6565 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5205 - reconstruction_loss: 2.3706 - kl_loss: 5.6622 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5015 - reconstruction_loss: 2.3692 - kl_loss: 5.6519 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5114 - reconstruction_loss: 2.3684 - kl_loss: 5.6613 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5133 - reconstruction_loss: 2.3658 - kl_loss: 5.6555 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.5061 - reconstruction_loss: 2.3678 - kl_loss: 5.6462 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5096 - reconstruction_loss: 2.3687 - kl_loss: 5.6563 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5035 - reconstruction_loss: 2.3770 - kl_loss: 5.6647\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.5036 - reconstruction_loss: 2.3688 - kl_loss: 5.6653 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4906 - reconstruction_loss: 2.3615 - kl_loss: 5.6495 - lr: 6.2500e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5001 - reconstruction_loss: 2.3634 - kl_loss: 5.6561 - lr: 6.2500e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4967 - reconstruction_loss: 2.3640 - kl_loss: 5.6639 - lr: 6.2500e-05\n",
      "Epoch 70/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.5097 - reconstruction_loss: 2.3724 - kl_loss: 5.6553\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.5095 - reconstruction_loss: 2.3638 - kl_loss: 5.6554 - lr: 6.2500e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4918 - reconstruction_loss: 2.3570 - kl_loss: 5.6627 - lr: 3.1250e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4832 - reconstruction_loss: 2.3503 - kl_loss: 5.6580 - lr: 3.1250e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4933 - reconstruction_loss: 2.3465 - kl_loss: 5.6715 - lr: 3.1250e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4904 - reconstruction_loss: 2.3476 - kl_loss: 5.6651 - lr: 3.1250e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4859 - reconstruction_loss: 2.3515 - kl_loss: 5.6511 - lr: 3.1250e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4915 - reconstruction_loss: 2.3489 - kl_loss: 5.6612 - lr: 3.1250e-05\n",
      "Epoch 77/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.4975 - reconstruction_loss: 2.3566 - kl_loss: 5.6636\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4972 - reconstruction_loss: 2.3484 - kl_loss: 5.6632 - lr: 3.1250e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4775 - reconstruction_loss: 2.3410 - kl_loss: 5.6577 - lr: 1.5625e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4785 - reconstruction_loss: 2.3408 - kl_loss: 5.6567 - lr: 1.5625e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4810 - reconstruction_loss: 2.3384 - kl_loss: 5.6574 - lr: 1.5625e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4679 - reconstruction_loss: 2.3392 - kl_loss: 5.6483 - lr: 1.5625e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4687 - reconstruction_loss: 2.3385 - kl_loss: 5.6550 - lr: 1.5625e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4660 - reconstruction_loss: 2.3343 - kl_loss: 5.6557 - lr: 1.5625e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4774 - reconstruction_loss: 2.3381 - kl_loss: 5.6577 - lr: 1.5625e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4708 - reconstruction_loss: 2.3326 - kl_loss: 5.6641 - lr: 1.5625e-05\n",
      "Epoch 86/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4735 - reconstruction_loss: 2.3472 - kl_loss: 5.6521\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4735 - reconstruction_loss: 2.3387 - kl_loss: 5.6521 - lr: 1.5625e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4769 - reconstruction_loss: 2.3343 - kl_loss: 5.6577 - lr: 7.8125e-06\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4732 - reconstruction_loss: 2.3303 - kl_loss: 5.6619 - lr: 7.8125e-06\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4774 - reconstruction_loss: 2.3335 - kl_loss: 5.6561 - lr: 7.8125e-06\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4714 - reconstruction_loss: 2.3332 - kl_loss: 5.6566 - lr: 7.8125e-06\n",
      "Epoch 91/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4630 - reconstruction_loss: 2.3381 - kl_loss: 5.6631\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4631 - reconstruction_loss: 2.3313 - kl_loss: 5.6631 - lr: 7.8125e-06\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4670 - reconstruction_loss: 2.3298 - kl_loss: 5.6673 - lr: 3.9063e-06\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4682 - reconstruction_loss: 2.3292 - kl_loss: 5.6624 - lr: 3.9063e-06\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4644 - reconstruction_loss: 2.3303 - kl_loss: 5.6609 - lr: 3.9063e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4731 - reconstruction_loss: 2.3286 - kl_loss: 5.6600 - lr: 3.9063e-06\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4637 - reconstruction_loss: 2.3288 - kl_loss: 5.6718 - lr: 3.9063e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4572 - reconstruction_loss: 2.3245 - kl_loss: 5.6771 - lr: 3.9063e-06\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4740 - reconstruction_loss: 2.3290 - kl_loss: 5.6662 - lr: 3.9063e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4724 - reconstruction_loss: 2.3270 - kl_loss: 5.6571 - lr: 3.9063e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4574 - reconstruction_loss: 2.3302 - kl_loss: 5.6606 - lr: 3.9063e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4737 - reconstruction_loss: 2.3287 - kl_loss: 5.6707 - lr: 3.9063e-06\n",
      "Epoch 102/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4724 - reconstruction_loss: 2.3365 - kl_loss: 5.6640\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4723 - reconstruction_loss: 2.3282 - kl_loss: 5.6637 - lr: 3.9063e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4633 - reconstruction_loss: 2.3294 - kl_loss: 5.6618 - lr: 1.9531e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4721 - reconstruction_loss: 2.3270 - kl_loss: 5.6636 - lr: 1.9531e-06\n",
      "Epoch 105/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.4633 - reconstruction_loss: 2.3328 - kl_loss: 5.6652\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4633 - reconstruction_loss: 2.3256 - kl_loss: 5.6661 - lr: 1.9531e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4674 - reconstruction_loss: 2.3248 - kl_loss: 5.6709 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4610 - reconstruction_loss: 2.3245 - kl_loss: 5.6705 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4547 - reconstruction_loss: 2.3257 - kl_loss: 5.6695 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4768 - reconstruction_loss: 2.3236 - kl_loss: 5.6674 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4586 - reconstruction_loss: 2.3266 - kl_loss: 5.6653 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4532 - reconstruction_loss: 2.3275 - kl_loss: 5.6661 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 3.4737 - reconstruction_loss: 2.3266 - kl_loss: 5.6663 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4666 - reconstruction_loss: 2.3261 - kl_loss: 5.6672 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4707 - reconstruction_loss: 2.3229 - kl_loss: 5.6670 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4673 - reconstruction_loss: 2.3275 - kl_loss: 5.6665 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4750 - reconstruction_loss: 2.3271 - kl_loss: 5.6673 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4671 - reconstruction_loss: 2.3276 - kl_loss: 5.6686 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4710 - reconstruction_loss: 2.3244 - kl_loss: 5.6670 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4595 - reconstruction_loss: 2.3249 - kl_loss: 5.6640 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4614 - reconstruction_loss: 2.3278 - kl_loss: 5.6662 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 2s 13ms/step - loss: 3.4662 - reconstruction_loss: 2.3235 - kl_loss: 5.6660 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4601 - reconstruction_loss: 2.3252 - kl_loss: 5.6647 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4769 - reconstruction_loss: 2.3261 - kl_loss: 5.6662 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 3.4735 - reconstruction_loss: 2.3249 - kl_loss: 5.6674 - lr: 1.0000e-06\n",
      "Epoch 124: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 17:50:26,668]\u001b[0m Trial 97 finished with value: 0.0045546945932052755 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 36, 'encoder_units_l2': 438, 'encoder_units_l3': 54, 'decoder_layers': 5, 'decoder_units_l1': 185, 'decoder_units_l2': 44, 'decoder_units_l3': 507, 'decoder_units_l4': 64, 'decoder_units_l5': 809, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 12.7230 - reconstruction_loss: 8.7026 - kl_loss: 4.0288 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 6.9557 - reconstruction_loss: 5.5664 - kl_loss: 4.8462 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 5.7335 - reconstruction_loss: 4.5015 - kl_loss: 5.1970 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 5.2841 - reconstruction_loss: 4.1381 - kl_loss: 5.3072 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 5.0433 - reconstruction_loss: 3.9003 - kl_loss: 5.4489 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 4.8992 - reconstruction_loss: 3.7422 - kl_loss: 5.5069 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 4.7321 - reconstruction_loss: 3.5775 - kl_loss: 5.5878 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 4.6678 - reconstruction_loss: 3.4721 - kl_loss: 5.6276 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 4.5324 - reconstruction_loss: 3.3766 - kl_loss: 5.6428 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 4.4553 - reconstruction_loss: 3.3208 - kl_loss: 5.6696 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 4.3800 - reconstruction_loss: 3.1923 - kl_loss: 5.6920 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 4.2382 - reconstruction_loss: 3.0609 - kl_loss: 5.7906 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 4.2304 - reconstruction_loss: 3.0056 - kl_loss: 5.7934 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 4.0838 - reconstruction_loss: 2.8962 - kl_loss: 5.8238 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 4.0493 - reconstruction_loss: 2.8799 - kl_loss: 5.7898 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 4.0673 - reconstruction_loss: 2.8679 - kl_loss: 5.8419 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 4.0108 - reconstruction_loss: 2.7958 - kl_loss: 5.8327 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.9586 - reconstruction_loss: 2.7806 - kl_loss: 5.8582 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.9289 - reconstruction_loss: 2.7438 - kl_loss: 5.8274 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.8958 - reconstruction_loss: 2.7141 - kl_loss: 5.8605 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.8673 - reconstruction_loss: 2.6679 - kl_loss: 5.8503 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.8449 - reconstruction_loss: 2.6959 - kl_loss: 5.8377 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.8644 - reconstruction_loss: 2.6809 - kl_loss: 5.8331 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.8322 - reconstruction_loss: 2.6258 - kl_loss: 5.8505 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.8152 - reconstruction_loss: 2.6224 - kl_loss: 5.8503 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.7828 - reconstruction_loss: 2.6156 - kl_loss: 5.8457 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.7673 - reconstruction_loss: 2.5903 - kl_loss: 5.8449 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.8098 - reconstruction_loss: 2.6319 - kl_loss: 5.8638 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.7966 - reconstruction_loss: 2.6010 - kl_loss: 5.8492 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.8158 - reconstruction_loss: 2.6591 - kl_loss: 5.8279\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.8158 - reconstruction_loss: 2.6506 - kl_loss: 5.8273 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.7193 - reconstruction_loss: 2.5345 - kl_loss: 5.8322 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.6672 - reconstruction_loss: 2.4981 - kl_loss: 5.8272 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.6414 - reconstruction_loss: 2.4795 - kl_loss: 5.8158 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.6460 - reconstruction_loss: 2.4846 - kl_loss: 5.8098 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.6444 - reconstruction_loss: 2.4943 - kl_loss: 5.8112 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.6243 - reconstruction_loss: 2.4739 - kl_loss: 5.7847 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.6447 - reconstruction_loss: 2.4672 - kl_loss: 5.8101 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.6326 - reconstruction_loss: 2.4543 - kl_loss: 5.7718 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.5933 - reconstruction_loss: 2.4353 - kl_loss: 5.7450 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.5918 - reconstruction_loss: 2.4430 - kl_loss: 5.7576 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.5729 - reconstruction_loss: 2.4185 - kl_loss: 5.7302 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.5658 - reconstruction_loss: 2.4153 - kl_loss: 5.7179 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.5641 - reconstruction_loss: 2.4043 - kl_loss: 5.7110 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.5779 - reconstruction_loss: 2.4301 - kl_loss: 5.7255 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.6087 - reconstruction_loss: 2.4762 - kl_loss: 5.7725 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5711 - reconstruction_loss: 2.4279 - kl_loss: 5.7299\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.5710 - reconstruction_loss: 2.4193 - kl_loss: 5.7305 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.5438 - reconstruction_loss: 2.3895 - kl_loss: 5.7159 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.5255 - reconstruction_loss: 2.3774 - kl_loss: 5.7116 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.5170 - reconstruction_loss: 2.3682 - kl_loss: 5.7069 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.5129 - reconstruction_loss: 2.3680 - kl_loss: 5.6865 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.5212 - reconstruction_loss: 2.3635 - kl_loss: 5.6819 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.5126 - reconstruction_loss: 2.3769 - kl_loss: 5.6742 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.5119 - reconstruction_loss: 2.3663 - kl_loss: 5.6804 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5148 - reconstruction_loss: 2.3809 - kl_loss: 5.6835\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.5148 - reconstruction_loss: 2.3725 - kl_loss: 5.6837 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.5029 - reconstruction_loss: 2.3633 - kl_loss: 5.6820 - lr: 1.2500e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5070 - reconstruction_loss: 2.3581 - kl_loss: 5.6761 - lr: 1.2500e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.5088 - reconstruction_loss: 2.3518 - kl_loss: 5.6625 - lr: 1.2500e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.5007 - reconstruction_loss: 2.3516 - kl_loss: 5.6598 - lr: 1.2500e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4861 - reconstruction_loss: 2.3510 - kl_loss: 5.6701 - lr: 1.2500e-04\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4851 - reconstruction_loss: 2.3443 - kl_loss: 5.6667 - lr: 1.2500e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4823 - reconstruction_loss: 2.3466 - kl_loss: 5.6644 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4936 - reconstruction_loss: 2.3474 - kl_loss: 5.6623 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.4864 - reconstruction_loss: 2.3532 - kl_loss: 5.6502\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.4864 - reconstruction_loss: 2.3532 - kl_loss: 5.6502 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.4689 - reconstruction_loss: 2.3407 - kl_loss: 5.6538 - lr: 6.2500e-05\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4801 - reconstruction_loss: 2.3444 - kl_loss: 5.6626 - lr: 6.2500e-05\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4810 - reconstruction_loss: 2.3475 - kl_loss: 5.6669 - lr: 6.2500e-05\n",
      "Epoch 67/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4836 - reconstruction_loss: 2.3492 - kl_loss: 5.6595\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.4835 - reconstruction_loss: 2.3412 - kl_loss: 5.6595 - lr: 6.2500e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.4853 - reconstruction_loss: 2.3379 - kl_loss: 5.6579 - lr: 3.1250e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.4665 - reconstruction_loss: 2.3327 - kl_loss: 5.6580 - lr: 3.1250e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4772 - reconstruction_loss: 2.3318 - kl_loss: 5.6435 - lr: 3.1250e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4775 - reconstruction_loss: 2.3358 - kl_loss: 5.6395 - lr: 3.1250e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4678 - reconstruction_loss: 2.3308 - kl_loss: 5.6401 - lr: 3.1250e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4617 - reconstruction_loss: 2.3276 - kl_loss: 5.6451 - lr: 3.1250e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4683 - reconstruction_loss: 2.3312 - kl_loss: 5.6461 - lr: 3.1250e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4592 - reconstruction_loss: 2.3278 - kl_loss: 5.6586 - lr: 3.1250e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4805 - reconstruction_loss: 2.3278 - kl_loss: 5.6429 - lr: 3.1250e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4589 - reconstruction_loss: 2.3291 - kl_loss: 5.6369 - lr: 3.1250e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.4767 - reconstruction_loss: 2.3291 - kl_loss: 5.6400 - lr: 3.1250e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.4585 - reconstruction_loss: 2.3275 - kl_loss: 5.6379 - lr: 3.1250e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.4726 - reconstruction_loss: 2.3271 - kl_loss: 5.6490 - lr: 3.1250e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4589 - reconstruction_loss: 2.3244 - kl_loss: 5.6355 - lr: 3.1250e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4717 - reconstruction_loss: 2.3306 - kl_loss: 5.6389 - lr: 3.1250e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4642 - reconstruction_loss: 2.3232 - kl_loss: 5.6456 - lr: 3.1250e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4638 - reconstruction_loss: 2.3231 - kl_loss: 5.6353 - lr: 3.1250e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4514 - reconstruction_loss: 2.3275 - kl_loss: 5.6362 - lr: 3.1250e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4700 - reconstruction_loss: 2.3309 - kl_loss: 5.6388 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4623 - reconstruction_loss: 2.3341 - kl_loss: 5.6381\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4622 - reconstruction_loss: 2.3272 - kl_loss: 5.6387 - lr: 3.1250e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4511 - reconstruction_loss: 2.3233 - kl_loss: 5.6386 - lr: 1.5625e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4636 - reconstruction_loss: 2.3253 - kl_loss: 5.6421 - lr: 1.5625e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4752 - reconstruction_loss: 2.3194 - kl_loss: 5.6355 - lr: 1.5625e-05\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4477 - reconstruction_loss: 2.3219 - kl_loss: 5.6365 - lr: 1.5625e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4430 - reconstruction_loss: 2.3172 - kl_loss: 5.6277 - lr: 1.5625e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4651 - reconstruction_loss: 2.3187 - kl_loss: 5.6422 - lr: 1.5625e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4699 - reconstruction_loss: 2.3228 - kl_loss: 5.6417 - lr: 1.5625e-05\n",
      "Epoch 95/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4473 - reconstruction_loss: 2.3279 - kl_loss: 5.6326\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4474 - reconstruction_loss: 2.3211 - kl_loss: 5.6329 - lr: 1.5625e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4560 - reconstruction_loss: 2.3182 - kl_loss: 5.6424 - lr: 7.8125e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 3.4388 - reconstruction_loss: 2.3176 - kl_loss: 5.6344 - lr: 7.8125e-06\n",
      "Epoch 98/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4491 - reconstruction_loss: 2.3264 - kl_loss: 5.6373\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4490 - reconstruction_loss: 2.3182 - kl_loss: 5.6375 - lr: 7.8125e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4459 - reconstruction_loss: 2.3151 - kl_loss: 5.6312 - lr: 3.9063e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4437 - reconstruction_loss: 2.3144 - kl_loss: 5.6386 - lr: 3.9063e-06\n",
      "Epoch 101/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4465 - reconstruction_loss: 2.3101 - kl_loss: 5.6355 - lr: 3.9063e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4435 - reconstruction_loss: 2.3116 - kl_loss: 5.6301 - lr: 3.9063e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4420 - reconstruction_loss: 2.3093 - kl_loss: 5.6282 - lr: 3.9063e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4508 - reconstruction_loss: 2.3126 - kl_loss: 5.6255 - lr: 3.9063e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4443 - reconstruction_loss: 2.3115 - kl_loss: 5.6320 - lr: 3.9063e-06\n",
      "Epoch 106/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4635 - reconstruction_loss: 2.3186 - kl_loss: 5.6359\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4632 - reconstruction_loss: 2.3104 - kl_loss: 5.6359 - lr: 3.9063e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 3.4385 - reconstruction_loss: 2.3089 - kl_loss: 5.6384 - lr: 1.9531e-06\n",
      "Epoch 108/300\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 3.4337 - reconstruction_loss: 2.3095 - kl_loss: 5.6352 - lr: 1.9531e-06\n",
      "Epoch 109/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4393 - reconstruction_loss: 2.3197 - kl_loss: 5.6336\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4393 - reconstruction_loss: 2.3118 - kl_loss: 5.6338 - lr: 1.9531e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4417 - reconstruction_loss: 2.3092 - kl_loss: 5.6393 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4407 - reconstruction_loss: 2.3091 - kl_loss: 5.6408 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4537 - reconstruction_loss: 2.3107 - kl_loss: 5.6415 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 3.4409 - reconstruction_loss: 2.3084 - kl_loss: 5.6422 - lr: 1.0000e-06\n",
      "Epoch 113: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 17:59:31,318]\u001b[0m Trial 98 finished with value: 0.004521229270832642 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 52, 'encoder_units_l2': 570, 'encoder_units_l3': 32, 'decoder_layers': 4, 'decoder_units_l1': 245, 'decoder_units_l2': 29, 'decoder_units_l3': 696, 'decoder_units_l4': 34, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 13.3451 - reconstruction_loss: 9.3814 - kl_loss: 3.6570 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 7.9397 - reconstruction_loss: 6.7919 - kl_loss: 4.3738 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 6.8263 - reconstruction_loss: 5.5406 - kl_loss: 5.1078 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 5.9509 - reconstruction_loss: 4.6830 - kl_loss: 5.4037 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 5.3718 - reconstruction_loss: 4.0756 - kl_loss: 5.6923 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 4.9099 - reconstruction_loss: 3.5851 - kl_loss: 5.8774 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 4.5607 - reconstruction_loss: 3.2873 - kl_loss: 5.9175 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 4.3452 - reconstruction_loss: 3.1096 - kl_loss: 5.9808 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.2880 - reconstruction_loss: 3.0279 - kl_loss: 6.0066 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 4.1284 - reconstruction_loss: 2.9306 - kl_loss: 6.0107 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 4.1527 - reconstruction_loss: 2.8933 - kl_loss: 5.9746 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.9861 - reconstruction_loss: 2.8045 - kl_loss: 5.9858 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.9839 - reconstruction_loss: 2.7729 - kl_loss: 5.9323 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.9018 - reconstruction_loss: 2.7164 - kl_loss: 5.9142 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.9196 - reconstruction_loss: 2.6883 - kl_loss: 5.9054 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.8860 - reconstruction_loss: 2.6645 - kl_loss: 5.8877 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.8277 - reconstruction_loss: 2.6665 - kl_loss: 5.8797 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.8958 - reconstruction_loss: 2.7223 - kl_loss: 5.9030 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 4.0391 - reconstruction_loss: 2.7968 - kl_loss: 5.9482\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 4.0379 - reconstruction_loss: 2.7857 - kl_loss: 5.9485 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.7931 - reconstruction_loss: 2.5688 - kl_loss: 5.9003 - lr: 5.0000e-04\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.7202 - reconstruction_loss: 2.5357 - kl_loss: 5.8822 - lr: 5.0000e-04\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.7177 - reconstruction_loss: 2.5251 - kl_loss: 5.8695 - lr: 5.0000e-04\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 3.6866 - reconstruction_loss: 2.5087 - kl_loss: 5.8662 - lr: 5.0000e-04\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.7615 - reconstruction_loss: 2.5649 - kl_loss: 5.8739 - lr: 5.0000e-04\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.6995 - reconstruction_loss: 2.4874 - kl_loss: 5.8651 - lr: 5.0000e-04\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.6474 - reconstruction_loss: 2.4537 - kl_loss: 5.8413 - lr: 5.0000e-04\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.6330 - reconstruction_loss: 2.4624 - kl_loss: 5.8179 - lr: 5.0000e-04\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.6038 - reconstruction_loss: 2.4464 - kl_loss: 5.8142 - lr: 5.0000e-04\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.6374 - reconstruction_loss: 2.4567 - kl_loss: 5.8262 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.6575 - reconstruction_loss: 2.4566 - kl_loss: 5.8427 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5760 - reconstruction_loss: 2.4166 - kl_loss: 5.8150 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.6206 - reconstruction_loss: 2.4435 - kl_loss: 5.8265 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.6490 - reconstruction_loss: 2.4730 - kl_loss: 5.8508 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.6209 - reconstruction_loss: 2.4395 - kl_loss: 5.8310\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 3.6206 - reconstruction_loss: 2.4299 - kl_loss: 5.8313 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.6061 - reconstruction_loss: 2.4183 - kl_loss: 5.8502 - lr: 2.5000e-04\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5693 - reconstruction_loss: 2.3921 - kl_loss: 5.8340 - lr: 2.5000e-04\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5475 - reconstruction_loss: 2.3724 - kl_loss: 5.8335 - lr: 2.5000e-04\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.5873 - reconstruction_loss: 2.3873 - kl_loss: 5.8541 - lr: 2.5000e-04\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.5555 - reconstruction_loss: 2.4186 - kl_loss: 5.8920 - lr: 2.5000e-04\n",
      "Epoch 40/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.6740 - reconstruction_loss: 2.4503 - kl_loss: 5.8896\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.6737 - reconstruction_loss: 2.4503 - kl_loss: 5.8896 - lr: 2.5000e-04\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 3.5577 - reconstruction_loss: 2.3790 - kl_loss: 5.8711 - lr: 1.2500e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5496 - reconstruction_loss: 2.3693 - kl_loss: 5.8936 - lr: 1.2500e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5260 - reconstruction_loss: 2.3524 - kl_loss: 5.8598 - lr: 1.2500e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.5429 - reconstruction_loss: 2.3578 - kl_loss: 5.8493 - lr: 1.2500e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5236 - reconstruction_loss: 2.3467 - kl_loss: 5.8286 - lr: 1.2500e-04\n",
      "Epoch 46/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5259 - reconstruction_loss: 2.3419 - kl_loss: 5.8272 - lr: 1.2500e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5035 - reconstruction_loss: 2.3395 - kl_loss: 5.8368 - lr: 1.2500e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5216 - reconstruction_loss: 2.3444 - kl_loss: 5.8523 - lr: 1.2500e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.5349 - reconstruction_loss: 2.3468 - kl_loss: 5.8412 - lr: 1.2500e-04\n",
      "Epoch 50/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.5148 - reconstruction_loss: 2.3464 - kl_loss: 5.8443\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.5147 - reconstruction_loss: 2.3388 - kl_loss: 5.8439 - lr: 1.2500e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.5121 - reconstruction_loss: 2.3253 - kl_loss: 5.8570 - lr: 6.2500e-05\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.4819 - reconstruction_loss: 2.3203 - kl_loss: 5.8329 - lr: 6.2500e-05\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.4801 - reconstruction_loss: 2.3207 - kl_loss: 5.8230 - lr: 6.2500e-05\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.5164 - reconstruction_loss: 2.3136 - kl_loss: 5.8446 - lr: 6.2500e-05\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 3.4804 - reconstruction_loss: 2.3115 - kl_loss: 5.8273 - lr: 6.2500e-05\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.4914 - reconstruction_loss: 2.3145 - kl_loss: 5.8224 - lr: 6.2500e-05\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 3.4904 - reconstruction_loss: 2.3120 - kl_loss: 5.8358 - lr: 6.2500e-05\n",
      "Epoch 58/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4790 - reconstruction_loss: 2.3238 - kl_loss: 5.8204\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.4791 - reconstruction_loss: 2.3160 - kl_loss: 5.8206 - lr: 6.2500e-05\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4841 - reconstruction_loss: 2.3097 - kl_loss: 5.8196 - lr: 3.1250e-05\n",
      "Epoch 60/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4787 - reconstruction_loss: 2.3029 - kl_loss: 5.8313 - lr: 3.1250e-05\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4686 - reconstruction_loss: 2.3004 - kl_loss: 5.8167 - lr: 3.1250e-05\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4674 - reconstruction_loss: 2.3021 - kl_loss: 5.8251 - lr: 3.1250e-05\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4704 - reconstruction_loss: 2.2964 - kl_loss: 5.8191 - lr: 3.1250e-05\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.4787 - reconstruction_loss: 2.3044 - kl_loss: 5.8350 - lr: 3.1250e-05\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.4758 - reconstruction_loss: 2.3036 - kl_loss: 5.8282 - lr: 3.1250e-05\n",
      "Epoch 66/300\n",
      "194/196 [============================>.] - ETA: 0s - loss: 3.4809 - reconstruction_loss: 2.3062 - kl_loss: 5.8285\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4807 - reconstruction_loss: 2.2977 - kl_loss: 5.8285 - lr: 3.1250e-05\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4620 - reconstruction_loss: 2.2916 - kl_loss: 5.8237 - lr: 1.5625e-05\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4687 - reconstruction_loss: 2.2910 - kl_loss: 5.8296 - lr: 1.5625e-05\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4590 - reconstruction_loss: 2.2891 - kl_loss: 5.8261 - lr: 1.5625e-05\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.4502 - reconstruction_loss: 2.2904 - kl_loss: 5.8275 - lr: 1.5625e-05\n",
      "Epoch 71/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.4668 - reconstruction_loss: 2.2865 - kl_loss: 5.8268 - lr: 1.5625e-05\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.4586 - reconstruction_loss: 2.2887 - kl_loss: 5.8214 - lr: 1.5625e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4733 - reconstruction_loss: 2.2883 - kl_loss: 5.8284 - lr: 1.5625e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4623 - reconstruction_loss: 2.2860 - kl_loss: 5.8279 - lr: 1.5625e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.4609 - reconstruction_loss: 2.2844 - kl_loss: 5.8279 - lr: 1.5625e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.4529 - reconstruction_loss: 2.2845 - kl_loss: 5.8248 - lr: 1.5625e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.4496 - reconstruction_loss: 2.2849 - kl_loss: 5.8246 - lr: 1.5625e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 3.4491 - reconstruction_loss: 2.2834 - kl_loss: 5.8208 - lr: 1.5625e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4549 - reconstruction_loss: 2.2853 - kl_loss: 5.8273 - lr: 1.5625e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4505 - reconstruction_loss: 2.2835 - kl_loss: 5.8235 - lr: 1.5625e-05\n",
      "Epoch 81/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4670 - reconstruction_loss: 2.2955 - kl_loss: 5.8172\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.4668 - reconstruction_loss: 2.2861 - kl_loss: 5.8175 - lr: 1.5625e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 3.4636 - reconstruction_loss: 2.2833 - kl_loss: 5.8201 - lr: 7.8125e-06\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.4594 - reconstruction_loss: 2.2815 - kl_loss: 5.8288 - lr: 7.8125e-06\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4649 - reconstruction_loss: 2.2817 - kl_loss: 5.8240 - lr: 7.8125e-06\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4556 - reconstruction_loss: 2.2806 - kl_loss: 5.8170 - lr: 7.8125e-06\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.4473 - reconstruction_loss: 2.2787 - kl_loss: 5.8281 - lr: 7.8125e-06\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 3.4531 - reconstruction_loss: 2.2793 - kl_loss: 5.8252 - lr: 7.8125e-06\n",
      "Epoch 88/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.4724 - reconstruction_loss: 2.2901 - kl_loss: 5.8233\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.4720 - reconstruction_loss: 2.2817 - kl_loss: 5.8229 - lr: 7.8125e-06\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4533 - reconstruction_loss: 2.2806 - kl_loss: 5.8277 - lr: 3.9063e-06\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 3.4465 - reconstruction_loss: 2.2762 - kl_loss: 5.8255 - lr: 3.9063e-06\n",
      "Epoch 91/300\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 3.4651 - reconstruction_loss: 2.2802 - kl_loss: 5.8253 - lr: 3.9063e-06\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 3.4545 - reconstruction_loss: 2.2816 - kl_loss: 5.8260 - lr: 3.9063e-06\n",
      "Epoch 93/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4493 - reconstruction_loss: 2.2851 - kl_loss: 5.8294\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4492 - reconstruction_loss: 2.2769 - kl_loss: 5.8293 - lr: 3.9063e-06\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4444 - reconstruction_loss: 2.2777 - kl_loss: 5.8261 - lr: 1.9531e-06\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4658 - reconstruction_loss: 2.2778 - kl_loss: 5.8237 - lr: 1.9531e-06\n",
      "Epoch 96/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.4513 - reconstruction_loss: 2.2843 - kl_loss: 5.8231\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4512 - reconstruction_loss: 2.2766 - kl_loss: 5.8233 - lr: 1.9531e-06\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4561 - reconstruction_loss: 2.2759 - kl_loss: 5.8225 - lr: 1.0000e-06\n",
      "Epoch 98/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4509 - reconstruction_loss: 2.2768 - kl_loss: 5.8207 - lr: 1.0000e-06\n",
      "Epoch 99/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4523 - reconstruction_loss: 2.2775 - kl_loss: 5.8214 - lr: 1.0000e-06\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4501 - reconstruction_loss: 2.2773 - kl_loss: 5.8215 - lr: 1.0000e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4554 - reconstruction_loss: 2.2781 - kl_loss: 5.8213 - lr: 1.0000e-06\n",
      "Epoch 102/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4592 - reconstruction_loss: 2.2775 - kl_loss: 5.8219 - lr: 1.0000e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4437 - reconstruction_loss: 2.2764 - kl_loss: 5.8221 - lr: 1.0000e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4416 - reconstruction_loss: 2.2772 - kl_loss: 5.8220 - lr: 1.0000e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4509 - reconstruction_loss: 2.2777 - kl_loss: 5.8240 - lr: 1.0000e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4530 - reconstruction_loss: 2.2763 - kl_loss: 5.8236 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 3.4441 - reconstruction_loss: 2.2790 - kl_loss: 5.8244 - lr: 1.0000e-06\n",
      "Epoch 107: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 18:06:03,369]\u001b[0m Trial 99 finished with value: 0.004464578505576304 and parameters: {'encoder_layers': 3, 'encoder_units_l1': 27, 'encoder_units_l2': 376, 'encoder_units_l3': 91, 'decoder_layers': 4, 'decoder_units_l1': 58, 'decoder_units_l2': 40, 'decoder_units_l3': 818, 'decoder_units_l4': 88, 'beta': 0.2, 'lr': 0.001, 'batch_size': 512}. Best is trial 84 with value: 0.004318432477628052.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 100\n",
      "Best trial:\n",
      "  Value: 0.004318432477628052\n",
      "  Params: \n",
      "    encoder_layers: 3\n",
      "    encoder_units_l1: 31\n",
      "    encoder_units_l2: 452\n",
      "    encoder_units_l3: 88\n",
      "    decoder_layers: 4\n",
      "    decoder_units_l1: 127\n",
      "    decoder_units_l2: 22\n",
      "    decoder_units_l3: 448\n",
      "    decoder_units_l4: 71\n",
      "    beta: 0.2\n",
      "    lr: 0.001\n",
      "    batch_size: 512\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    latent_dim = 2\n",
    "    input_shape = (8,)\n",
    "    \n",
    "    #Building encoder\n",
    "    encoder_layers = trial.suggest_int(\"encoder_layers\", 3, 8)\n",
    "    encoder_inputs = keras.Input(shape=input_shape) \n",
    "    \n",
    "    num_hidden = trial.suggest_int(\"encoder_units_l1\", 16, 1024, log=True)\n",
    "    x = layers.Dense(num_hidden, activation=\"relu\")(encoder_inputs)\n",
    "    for i in range(encoder_layers-1):\n",
    "        num_hidden = trial.suggest_int(\"encoder_units_l{}\".format(i+2), 16, 1024, log=True)\n",
    "        x = layers.Dense(256, activation=\"relu\")(x)\n",
    "        \n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    \n",
    "    #Building decoder\n",
    "    decoder_layers = trial.suggest_int(\"decoder_layers\", 3, 8)\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))   \n",
    "    \n",
    "    num_hidden = trial.suggest_int(\"decoder_units_l1\", 16, 1024, log=True)\n",
    "    x = layers.Dense(num_hidden, activation=\"relu\")(latent_inputs)\n",
    "                                       \n",
    "    for i in range(decoder_layers-1):\n",
    "        num_hidden = trial.suggest_int(\"decoder_units_l{}\".format(i+2), 16, 1024, log=True)\n",
    "        x = layers.Dense(256, activation=\"relu\")(x)\n",
    "                                       \n",
    "    decoder_outputs = layers.Dense(input_shape[0], activation=\"sigmoid\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "                                \n",
    "    #VAE                                   \n",
    "    early_stop = EarlyStopping(monitor='loss',\n",
    "                                    patience=10,\n",
    "                                    mode='min',\n",
    "                                    verbose=1)\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', \n",
    "                                         factor=0.5, \n",
    "                                         patience=3, \n",
    "                                         verbose=1,\n",
    "                                         mode='min',\n",
    "                                         min_lr=1e-6)\n",
    "\n",
    "    beta = trial.suggest_categorical(\"beta\", [0.2, 1, 5])\n",
    "    vae = VAE(encoder, decoder, input_shape, beta)\n",
    "    vae.compile(optimizer=Adam(learning_rate = trial.suggest_categorical(\"lr\", [1e-2, 1e-3, 1e-4])))\n",
    "    vae.fit(X_train,\n",
    "            epochs=300, \n",
    "            batch_size= trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512]),\n",
    "            callbacks=[reduce_lr, early_stop]\n",
    "           )\n",
    "\n",
    "    z_mean, z_log_var, z = encoder(X_train) #Calculating z, mean and variance\n",
    "    reconstruction = decoder(z) #Reconstructing data using decoder\n",
    "    score = mean_squared_error(X_train, reconstruction) #Score is mean squared error of reconstruction vs original\n",
    "    clear_session()\n",
    "                                       \n",
    "    return score\n",
    "\n",
    "#Optimize\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "    \n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "\n",
    "#Print results and save to file\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "    \n",
    "#Save results\n",
    "with open('C:/Users/lucbu/Documents/ML Projects/MLforPandA/best_values.txt', \"w\") as hf:\n",
    "     hf.write(\"Value: {} \\n \\n\".format(trial.value))\n",
    "     hf.write(\"Params: \\n\")\n",
    "     for key, value in trial.params.items():\n",
    "         hf.write(\"   {}: {} \\n\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d1f13",
   "metadata": {},
   "source": [
    "## Best Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb18ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 31)           279         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 452)          14464       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 88)           39864       ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            178         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            178         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 54,963\n",
      "Trainable params: 54,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 127)               381       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 22)                2816      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 448)               10304     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 71)                31879     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8)                 576       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,956\n",
      "Trainable params: 45,956\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Defining variables\n",
    "latent_dim = 2\n",
    "input_shape = (8,)\n",
    "\n",
    "#Encoder building function\n",
    "def build_encoder():\n",
    "    encoder_inputs = keras.Input(shape=input_shape) \n",
    "    x = layers.Dense(31, activation=\"relu\")(encoder_inputs)\n",
    "    x = layers.Dense(452, activation=\"relu\")(x)\n",
    "    x = layers.Dense(88, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "encoder = build_encoder()\n",
    "encoder.summary()\n",
    "\n",
    "#Decoder building function\n",
    "def build_decoder():\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(127, activation=\"relu\")(latent_inputs)\n",
    "    x = layers.Dense(22, activation=\"relu\")(x)\n",
    "    x = layers.Dense(448, activation=\"relu\")(x)\n",
    "    x = layers.Dense(71, activation=\"relu\")(x)\n",
    "    decoder_outputs = layers.Dense(input_shape[0], activation=\"sigmoid\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    \n",
    "    return decoder\n",
    "\n",
    "decoder = build_decoder()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cf6d587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "196/196 [==============================] - 3s 11ms/step - loss: 14.4371 - reconstruction_loss: 9.9488 - kl_loss: 3.6485 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 7.7991 - reconstruction_loss: 6.6253 - kl_loss: 4.4102 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 6.3970 - reconstruction_loss: 5.1372 - kl_loss: 4.7888 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 5.6031 - reconstruction_loss: 4.4991 - kl_loss: 5.0154 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 5.2958 - reconstruction_loss: 4.2004 - kl_loss: 5.1557 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 5.0883 - reconstruction_loss: 3.9881 - kl_loss: 5.3125 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.9856 - reconstruction_loss: 3.8555 - kl_loss: 5.4032 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.8493 - reconstruction_loss: 3.7351 - kl_loss: 5.4740 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.7686 - reconstruction_loss: 3.6247 - kl_loss: 5.5495 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.6793 - reconstruction_loss: 3.5462 - kl_loss: 5.5765 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.6026 - reconstruction_loss: 3.4532 - kl_loss: 5.6259 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.5267 - reconstruction_loss: 3.3722 - kl_loss: 5.6453 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.4547 - reconstruction_loss: 3.3020 - kl_loss: 5.6733 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.4286 - reconstruction_loss: 3.2613 - kl_loss: 5.6986 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.3389 - reconstruction_loss: 3.1863 - kl_loss: 5.7180 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.2958 - reconstruction_loss: 3.1444 - kl_loss: 5.7464 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.2328 - reconstruction_loss: 3.0720 - kl_loss: 5.7758 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.2058 - reconstruction_loss: 3.0132 - kl_loss: 5.7688 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.1324 - reconstruction_loss: 2.9593 - kl_loss: 5.8125 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.1219 - reconstruction_loss: 2.9024 - kl_loss: 5.8223 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.0218 - reconstruction_loss: 2.8735 - kl_loss: 5.8173 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 4.1516 - reconstruction_loss: 2.8978 - kl_loss: 5.7736 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9727 - reconstruction_loss: 2.8083 - kl_loss: 5.8031 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 4.0107 - reconstruction_loss: 2.8306 - kl_loss: 5.8103 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9744 - reconstruction_loss: 2.8238 - kl_loss: 5.7693 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9520 - reconstruction_loss: 2.7500 - kl_loss: 5.7858 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.9198 - reconstruction_loss: 2.7312 - kl_loss: 5.7661 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8600 - reconstruction_loss: 2.7024 - kl_loss: 5.7769 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8752 - reconstruction_loss: 2.7093 - kl_loss: 5.7636 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.8488 - reconstruction_loss: 2.6868 - kl_loss: 5.7509 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8294 - reconstruction_loss: 2.6730 - kl_loss: 5.7610 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.8217 - reconstruction_loss: 2.6721 - kl_loss: 5.8011 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.8331 - reconstruction_loss: 2.6550 - kl_loss: 5.7509 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7829 - reconstruction_loss: 2.6168 - kl_loss: 5.7453 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.7416 - reconstruction_loss: 2.5907 - kl_loss: 5.7739 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.7755 - reconstruction_loss: 2.5859 - kl_loss: 5.7447 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7251 - reconstruction_loss: 2.5644 - kl_loss: 5.7446 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7206 - reconstruction_loss: 2.5997 - kl_loss: 5.7721 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.7425 - reconstruction_loss: 2.5683 - kl_loss: 5.7540 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.7137 - reconstruction_loss: 2.5773 - kl_loss: 5.7476\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.7139 - reconstruction_loss: 2.5687 - kl_loss: 5.7480 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6782 - reconstruction_loss: 2.5062 - kl_loss: 5.7333 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.6484 - reconstruction_loss: 2.4920 - kl_loss: 5.7253 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.6407 - reconstruction_loss: 2.4823 - kl_loss: 5.7363 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.6377 - reconstruction_loss: 2.4858 - kl_loss: 5.7373 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.6509 - reconstruction_loss: 2.4858 - kl_loss: 5.7366 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.6578 - reconstruction_loss: 2.5029 - kl_loss: 5.7270\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.6574 - reconstruction_loss: 2.4933 - kl_loss: 5.7261 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.6127 - reconstruction_loss: 2.4657 - kl_loss: 5.7116 - lr: 2.5000e-04\n",
      "Epoch 48/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.6195 - reconstruction_loss: 2.4622 - kl_loss: 5.7210 - lr: 2.5000e-04\n",
      "Epoch 49/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6073 - reconstruction_loss: 2.4517 - kl_loss: 5.7041 - lr: 2.5000e-04\n",
      "Epoch 50/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5898 - reconstruction_loss: 2.4373 - kl_loss: 5.7025 - lr: 2.5000e-04\n",
      "Epoch 51/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6010 - reconstruction_loss: 2.4386 - kl_loss: 5.7171 - lr: 2.5000e-04\n",
      "Epoch 52/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.6001 - reconstruction_loss: 2.4405 - kl_loss: 5.7253 - lr: 2.5000e-04\n",
      "Epoch 53/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5893 - reconstruction_loss: 2.4324 - kl_loss: 5.7103 - lr: 2.5000e-04\n",
      "Epoch 54/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5799 - reconstruction_loss: 2.4333 - kl_loss: 5.7213 - lr: 2.5000e-04\n",
      "Epoch 55/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5727 - reconstruction_loss: 2.4301 - kl_loss: 5.7174 - lr: 2.5000e-04\n",
      "Epoch 56/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5842 - reconstruction_loss: 2.4306 - kl_loss: 5.7091 - lr: 2.5000e-04\n",
      "Epoch 57/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5706 - reconstruction_loss: 2.4272 - kl_loss: 5.6941 - lr: 2.5000e-04\n",
      "Epoch 58/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5761 - reconstruction_loss: 2.4316 - kl_loss: 5.7155 - lr: 2.5000e-04\n",
      "Epoch 59/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5913 - reconstruction_loss: 2.4328 - kl_loss: 5.7199 - lr: 2.5000e-04\n",
      "Epoch 60/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5852 - reconstruction_loss: 2.4317 - kl_loss: 5.7270\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5850 - reconstruction_loss: 2.4244 - kl_loss: 5.7273 - lr: 2.5000e-04\n",
      "Epoch 61/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5714 - reconstruction_loss: 2.4138 - kl_loss: 5.7075 - lr: 1.2500e-04\n",
      "Epoch 62/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5552 - reconstruction_loss: 2.4182 - kl_loss: 5.7124 - lr: 1.2500e-04\n",
      "Epoch 63/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5559 - reconstruction_loss: 2.4109 - kl_loss: 5.7277 - lr: 1.2500e-04\n",
      "Epoch 64/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5622 - reconstruction_loss: 2.4071 - kl_loss: 5.7154 - lr: 1.2500e-04\n",
      "Epoch 65/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5581 - reconstruction_loss: 2.4010 - kl_loss: 5.7235 - lr: 1.2500e-04\n",
      "Epoch 66/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5582 - reconstruction_loss: 2.4094 - kl_loss: 5.7116 - lr: 1.2500e-04\n",
      "Epoch 67/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5487 - reconstruction_loss: 2.4040 - kl_loss: 5.7155 - lr: 1.2500e-04\n",
      "Epoch 68/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5609 - reconstruction_loss: 2.4030 - kl_loss: 5.7062 - lr: 1.2500e-04\n",
      "Epoch 69/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5647 - reconstruction_loss: 2.4113 - kl_loss: 5.6974 - lr: 1.2500e-04\n",
      "Epoch 70/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5577 - reconstruction_loss: 2.4059 - kl_loss: 5.7006 - lr: 1.2500e-04\n",
      "Epoch 71/300\n",
      "193/196 [============================>.] - ETA: 0s - loss: 3.5531 - reconstruction_loss: 2.4233 - kl_loss: 5.7196\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5533 - reconstruction_loss: 2.4154 - kl_loss: 5.7200 - lr: 1.2500e-04\n",
      "Epoch 72/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5607 - reconstruction_loss: 2.4056 - kl_loss: 5.7332 - lr: 6.2500e-05\n",
      "Epoch 73/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5415 - reconstruction_loss: 2.3947 - kl_loss: 5.7237 - lr: 6.2500e-05\n",
      "Epoch 74/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5440 - reconstruction_loss: 2.3986 - kl_loss: 5.7085 - lr: 6.2500e-05\n",
      "Epoch 75/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5443 - reconstruction_loss: 2.3990 - kl_loss: 5.6955 - lr: 6.2500e-05\n",
      "Epoch 76/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5408 - reconstruction_loss: 2.3965 - kl_loss: 5.7019 - lr: 6.2500e-05\n",
      "Epoch 77/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5305 - reconstruction_loss: 2.3926 - kl_loss: 5.7035 - lr: 6.2500e-05\n",
      "Epoch 78/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5322 - reconstruction_loss: 2.3928 - kl_loss: 5.7025 - lr: 6.2500e-05\n",
      "Epoch 79/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5386 - reconstruction_loss: 2.3879 - kl_loss: 5.7070 - lr: 6.2500e-05\n",
      "Epoch 80/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5280 - reconstruction_loss: 2.3825 - kl_loss: 5.6907 - lr: 6.2500e-05\n",
      "Epoch 81/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5314 - reconstruction_loss: 2.3832 - kl_loss: 5.7068 - lr: 6.2500e-05\n",
      "Epoch 82/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5449 - reconstruction_loss: 2.3826 - kl_loss: 5.6947 - lr: 6.2500e-05\n",
      "Epoch 83/300\n",
      "196/196 [==============================] - ETA: 0s - loss: 3.5287 - reconstruction_loss: 2.3851 - kl_loss: 5.7159\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5287 - reconstruction_loss: 2.3851 - kl_loss: 5.7159 - lr: 6.2500e-05\n",
      "Epoch 84/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5380 - reconstruction_loss: 2.3826 - kl_loss: 5.7022 - lr: 3.1250e-05\n",
      "Epoch 85/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5145 - reconstruction_loss: 2.3769 - kl_loss: 5.6969 - lr: 3.1250e-05\n",
      "Epoch 86/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5377 - reconstruction_loss: 2.3818 - kl_loss: 5.6976 - lr: 3.1250e-05\n",
      "Epoch 87/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5228 - reconstruction_loss: 2.3775 - kl_loss: 5.7054 - lr: 3.1250e-05\n",
      "Epoch 88/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5183 - reconstruction_loss: 2.3741 - kl_loss: 5.7085 - lr: 3.1250e-05\n",
      "Epoch 89/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5093 - reconstruction_loss: 2.3764 - kl_loss: 5.7035 - lr: 3.1250e-05\n",
      "Epoch 90/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5198 - reconstruction_loss: 2.3795 - kl_loss: 5.7042 - lr: 3.1250e-05\n",
      "Epoch 91/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.5318 - reconstruction_loss: 2.3825 - kl_loss: 5.7118\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5315 - reconstruction_loss: 2.3740 - kl_loss: 5.7116 - lr: 3.1250e-05\n",
      "Epoch 92/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5304 - reconstruction_loss: 2.3719 - kl_loss: 5.7009 - lr: 1.5625e-05\n",
      "Epoch 93/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4961 - reconstruction_loss: 2.3692 - kl_loss: 5.6957 - lr: 1.5625e-05\n",
      "Epoch 94/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5207 - reconstruction_loss: 2.3697 - kl_loss: 5.6944 - lr: 1.5625e-05\n",
      "Epoch 95/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5100 - reconstruction_loss: 2.3681 - kl_loss: 5.7020 - lr: 1.5625e-05\n",
      "Epoch 96/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5211 - reconstruction_loss: 2.3673 - kl_loss: 5.7012 - lr: 1.5625e-05\n",
      "Epoch 97/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5214 - reconstruction_loss: 2.3701 - kl_loss: 5.7078 - lr: 1.5625e-05\n",
      "Epoch 98/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5165 - reconstruction_loss: 2.3658 - kl_loss: 5.7169 - lr: 1.5625e-05\n",
      "Epoch 99/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5174 - reconstruction_loss: 2.3746 - kl_loss: 5.7113\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5173 - reconstruction_loss: 2.3663 - kl_loss: 5.7117 - lr: 1.5625e-05\n",
      "Epoch 100/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5239 - reconstruction_loss: 2.3648 - kl_loss: 5.7075 - lr: 7.8125e-06\n",
      "Epoch 101/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5078 - reconstruction_loss: 2.3645 - kl_loss: 5.7063 - lr: 7.8125e-06\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5000 - reconstruction_loss: 2.3642 - kl_loss: 5.7069 - lr: 7.8125e-06\n",
      "Epoch 103/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5021 - reconstruction_loss: 2.3637 - kl_loss: 5.7056 - lr: 7.8125e-06\n",
      "Epoch 104/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5214 - reconstruction_loss: 2.3657 - kl_loss: 5.7018 - lr: 7.8125e-06\n",
      "Epoch 105/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5061 - reconstruction_loss: 2.3644 - kl_loss: 5.6981 - lr: 7.8125e-06\n",
      "Epoch 106/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5192 - reconstruction_loss: 2.3653 - kl_loss: 5.7036 - lr: 7.8125e-06\n",
      "Epoch 107/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5011 - reconstruction_loss: 2.3656 - kl_loss: 5.7058 - lr: 7.8125e-06\n",
      "Epoch 108/300\n",
      "192/196 [============================>.] - ETA: 0s - loss: 3.5261 - reconstruction_loss: 2.3728 - kl_loss: 5.7046\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5257 - reconstruction_loss: 2.3631 - kl_loss: 5.7047 - lr: 7.8125e-06\n",
      "Epoch 109/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5070 - reconstruction_loss: 2.3654 - kl_loss: 5.7013 - lr: 3.9063e-06\n",
      "Epoch 110/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5150 - reconstruction_loss: 2.3632 - kl_loss: 5.7039 - lr: 3.9063e-06\n",
      "Epoch 111/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5183 - reconstruction_loss: 2.3621 - kl_loss: 5.7073 - lr: 3.9063e-06\n",
      "Epoch 112/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5231 - reconstruction_loss: 2.3615 - kl_loss: 5.7061 - lr: 3.9063e-06\n",
      "Epoch 113/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5105 - reconstruction_loss: 2.3623 - kl_loss: 5.7075 - lr: 3.9063e-06\n",
      "Epoch 114/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5034 - reconstruction_loss: 2.3627 - kl_loss: 5.7075 - lr: 3.9063e-06\n",
      "Epoch 115/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5140 - reconstruction_loss: 2.3607 - kl_loss: 5.7070 - lr: 3.9063e-06\n",
      "Epoch 116/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5081 - reconstruction_loss: 2.3619 - kl_loss: 5.7021 - lr: 3.9063e-06\n",
      "Epoch 117/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5120 - reconstruction_loss: 2.3613 - kl_loss: 5.7024 - lr: 3.9063e-06\n",
      "Epoch 118/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5296 - reconstruction_loss: 2.3634 - kl_loss: 5.7032 - lr: 3.9063e-06\n",
      "Epoch 119/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5183 - reconstruction_loss: 2.3616 - kl_loss: 5.7031 - lr: 3.9063e-06\n",
      "Epoch 120/300\n",
      "191/196 [============================>.] - ETA: 0s - loss: 3.4975 - reconstruction_loss: 2.3719 - kl_loss: 5.7065\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4979 - reconstruction_loss: 2.3624 - kl_loss: 5.7065 - lr: 3.9063e-06\n",
      "Epoch 121/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5170 - reconstruction_loss: 2.3621 - kl_loss: 5.7102 - lr: 1.9531e-06\n",
      "Epoch 122/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5120 - reconstruction_loss: 2.3582 - kl_loss: 5.7079 - lr: 1.9531e-06\n",
      "Epoch 123/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5086 - reconstruction_loss: 2.3610 - kl_loss: 5.7070 - lr: 1.9531e-06\n",
      "Epoch 124/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5104 - reconstruction_loss: 2.3592 - kl_loss: 5.7041 - lr: 1.9531e-06\n",
      "Epoch 125/300\n",
      "195/196 [============================>.] - ETA: 0s - loss: 3.5119 - reconstruction_loss: 2.3706 - kl_loss: 5.7019\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5118 - reconstruction_loss: 2.3622 - kl_loss: 5.7017 - lr: 1.9531e-06\n",
      "Epoch 126/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5024 - reconstruction_loss: 2.3606 - kl_loss: 5.7054 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5068 - reconstruction_loss: 2.3594 - kl_loss: 5.7042 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5120 - reconstruction_loss: 2.3605 - kl_loss: 5.7030 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.4984 - reconstruction_loss: 2.3596 - kl_loss: 5.7046 - lr: 1.0000e-06\n",
      "Epoch 130/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5149 - reconstruction_loss: 2.3599 - kl_loss: 5.7058 - lr: 1.0000e-06\n",
      "Epoch 131/300\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 3.5105 - reconstruction_loss: 2.3608 - kl_loss: 5.7057 - lr: 1.0000e-06\n",
      "Epoch 132/300\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 3.5213 - reconstruction_loss: 2.3589 - kl_loss: 5.7061 - lr: 1.0000e-06\n",
      "Epoch 132: early stopping\n",
      "0.004621938731196289\n",
      "INFO:tensorflow:Assets written to: vae_model\\assets\n"
     ]
    }
   ],
   "source": [
    "#Early stopping function\n",
    "early_stop = EarlyStopping(monitor='loss',\n",
    "                                    patience=10,\n",
    "                                    mode='min',\n",
    "                                    verbose=1)\n",
    "\n",
    "#Learning rate scheduler\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', \n",
    "                                     factor=0.5, \n",
    "                                     patience=3, \n",
    "                                     verbose=1,\n",
    "                                     mode='min',\n",
    "                                     min_lr=0.000001)\n",
    "\n",
    "#Training with optimal values\n",
    "beta = 0.2\n",
    "vae = VAE(encoder, decoder, input_shape, beta)\n",
    "vae.compile(optimizer=Adam(learning_rate = 0.001))\n",
    "vae.fit(X_train,\n",
    "        epochs=300, \n",
    "        batch_size=512,\n",
    "        callbacks=[reduce_lr, early_stop]\n",
    "       )\n",
    "\n",
    "z_mean, z_log_var, z = encoder(X_train) #Calculating z, mean and variance\n",
    "reconstruction = decoder(z) #Reconstructing data using decoder\n",
    "score = mean_squared_error(X_train, reconstruction) #Score is mean squared error of reconstruction vs original\n",
    "print(score)\n",
    "\n",
    "input_shape2 = (None, 8,)\n",
    "vae.compute_output_shape(input_shape=input_shape2)\n",
    "vae.save(\"vae_model\")\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "055f8671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36697534 0.4013448  0.09764888 0.85939858 0.41765937 0.74414825\n",
      " 0.21474288 0.25250095]\n",
      "[[0.48938784 0.50199187 0.4834351  0.5156611  0.5309471  0.46917096\n",
      "  0.74469763 0.745518  ]]\n"
     ]
    }
   ],
   "source": [
    "vae = load_model('vae_model')\n",
    "z_sample = np.array([[rng.normal(), rng.normal()]])\n",
    "x_decoded = vae.decoder.predict(z_sample)\n",
    "print(X_train[0])\n",
    "print(x_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69933f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Function that plots the latent space\n",
    "def plot_label_clusters(vae, data, labels):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = vae.encoder.predict(data)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels, norm=matplotlib.colors.LogNorm())\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()\n",
    "\n",
    "vae = load_model('vae_model')\n",
    "plot_label_clusters(vae, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "148a1e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87abaa3889a43bb84eb23f33d757d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "outputs = np.zeros((100000,9))\n",
    "\n",
    "#Sampling 100.000 times and generating data points\n",
    "for i in tqdm(range(100000)):\n",
    "    z_sample = np.array([[rng.normal(), rng.normal()]])\n",
    "    x_decoded = vae.decoder.predict(z_sample)\n",
    "    outputs[i,0] = i\n",
    "    for j in range(8):\n",
    "        true_val = (x_decoded[:,j] * (maxima[j] - minima[j])) + minima[j] #Undoing normalisation\n",
    "        outputs[i,j+1] = true_val\n",
    "\n",
    "#Creating .csv file with dataset\n",
    "header = ['ID', 'E1', 'E2', 'p_x1', 'p_x2', 'p_y1', 'p_y2', 'p_z1', 'p_z2']\n",
    "\n",
    "with open('bVAE_data.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for i in range(100000):\n",
    "        writer.writerow(outputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbd5cd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loading generated data file\n",
    "gen_data = np.genfromtxt('bVAE_data.csv', delimiter=',') #Reading datafile\n",
    "gen_data = np.delete(gen_data, 0, 0) #Removing header\n",
    "gen_data = np.delete(gen_data, 0, 1) #Removing ID column\n",
    "    \n",
    "\n",
    "\n",
    "##Plotting histograms   \n",
    "#Energy distribution\n",
    "plt.hist([gen_data[:,0], gen_data[:,1]], label=['E1', 'E2'], bins=25, color=['orange','b'])\n",
    "plt.title(\"Energy distribution\")\n",
    "plt.xlabel(\"Energy (GeV)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "P1, theta1, phi1 = cart2sph(gen_data[:,2],gen_data[:,4],gen_data[:,6])\n",
    "P2, theta2, phi2 = cart2sph(gen_data[:,3],gen_data[:,5],gen_data[:,7])\n",
    "\n",
    "#Theta distribution\n",
    "plt.hist(theta1, bins=100, label='theta1', alpha=0.5, color='b')\n",
    "plt.hist(theta2, bins=100, label='theta2', alpha=0.5, color='y')\n",
    "plt.title(\"Theta distribution\")\n",
    "plt.xlabel(\"Theta\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Phi distribution\n",
    "plt.hist(phi1, bins=100, label='phi1', alpha=0.5, color='b')\n",
    "plt.hist(phi2, bins=100, label='phi2', alpha=0.5, color='y')\n",
    "plt.title(\"Phi distribution\")\n",
    "plt.xlabel(\"Phi\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Relativistic dispersion relation\n",
    "px1_list = np.array(px1_list)\n",
    "py1_list = np.array(py1_list)\n",
    "pz1_list = np.array(pz1_list)\n",
    "px2_list = np.array(px2_list)\n",
    "py2_list = np.array(py2_list)\n",
    "pz2_list = np.array(pz2_list)\n",
    "E1_list = np.array(E1_list)\n",
    "E2_list = np.array(E2_list)\n",
    "\n",
    "P1_list = np.sqrt(gen_data[:,2]**2 + gen_data[:,4]**2 + gen_data[:,6]**2)\n",
    "P2_list = np.sqrt(gen_data[:,3]**2 + gen_data[:,5]**2 + gen_data[:,7]**2)\n",
    "\n",
    "plt.hist(gen_data[:,0]**2 - P1_list**2, bins=100, label='Particle 1', alpha=0.5, color='b')\n",
    "plt.hist(gen_data[:,1]**2 - P2_list**2, bins=100, label='Particle 2', alpha=0.5, color='y')\n",
    "plt.title(\"Relativistic Dispersion Relation\")\n",
    "plt.ylabel(\"M^2\")\n",
    "plt.xlabel(\"E^2 - P^2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
